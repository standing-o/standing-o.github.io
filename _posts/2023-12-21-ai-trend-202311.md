---
title: "2023 AI ê¸°ìˆ  íŠ¸ë Œë“œ #2 | AI Trends in 2023"
date: 2023-12-21 00:00:00 +/-TTTT
categories: [AI Trend]
tags: [ai-trend]
math: true
toc: true
author: seoyoung
img_path: /assets/img/for_post/
pin: false
image:
  path: 20231123-t.jpg
  alt: ""
description: ì¸ê³µì§€ëŠ¥ íŠ¸ë Œë“œ, ë”¥ëŸ¬ë‹ íŠ¸ë Œë“œ, 2023 ì¸ê³µì§€ëŠ¥ íŠ¸ë Œë“œ, 2023 ì¸ê³µì§€ëŠ¥ ê¸°ìˆ  íŠ¸ë Œë“œ, ìµœê·¼ ì¸ê³µì§€ëŠ¥ ë™í–¥, Recent AI Trend, AI Issue, AI News, Top AI Paper
---

> 2023ë…„ 11ì›”ì˜ AI ê¸°ìˆ  ë™í–¥ê³¼ ì£¼ìš” ë‰´ìŠ¤, ë…¼ë¬¸, ê¸€, ì‚¬ë¡€ ë“±ì„ ì†Œê°œí•©ë‹ˆë‹¤.
{: .prompt-info }

ìµœê·¼ AI ë¶„ì•¼ì—ì„œëŠ” AI ê·œì œ ê°•í™”ì™€ í•¨ê»˜ ê¸°ìˆ  ë°œì „ê³¼ êµ­ì œ í˜‘ë ¥ì´ ì£¼ìš” ê´€ì‹¬ì‚¬ë¡œ ë– ì˜¤ë¦…ë‹ˆë‹¤.

GPT-4 Turbo ì¶œì‹œì™€ ì„¸ê³„ì ì¸ ê¸°ì—…ë“¤ì˜ AI íˆ¬ìì™€ ê´€ë ¨í•œ ë‰´ìŠ¤ë„ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤.

&nbsp;
&nbsp;
&nbsp;

## ğŸ’¡ **News**
### ë°”ì´ë“  ëŒ€í†µë ¹, AI ê·œì œ ê°•í™”í•˜ëŠ” í–‰ì • ëª…ë ¹ ë°œí‘œ [^ref-n-1]
- **ë¯¸êµ­** ëŒ€í†µë ¹ ë°”ì´ë“ ì€ AI ê·œì œë¥¼ ìœ„í•œ í–‰ì • ëª…ë ¹ì„ ë°œí‘œí–ˆìŠµë‹ˆë‹¤. 
- ëŒ€í†µë ¹ì˜ ë²•ì  ê¶Œí•œì„ ë°”íƒ•ìœ¼ë¡œ AI ê¸°ì—…ê³¼ ê¸°ê´€ë“¤ì—ê²Œ íŠ¹ì • ëª¨ë¸ì˜ ë³´ê³ ì™€ í…ŒìŠ¤íŠ¸ë¥¼ ìš”êµ¬í•˜ë©° ì—°ë°© ê¸°ê´€ì— AI ê¸°ì¤€ì„ ì„¤ì •í•˜ë„ë¡ ì§€ì‹œí–ˆìŠµë‹ˆë‹¤. 

### ìº˜ë¦¬í¬ë‹ˆì•„, í¬ë£¨ì¦ˆ ë¬´ì¸ ììœ¨ì£¼í–‰ ì°¨ëŸ‰ ìš´í–‰ ì¤‘ë‹¨ ì¡°ì¹˜ [^ref-n-2]
- **ìº˜ë¦¬í¬ë‹ˆì•„**ëŠ” ìš´ì „ì ì—†ëŠ” í¬ë£¨ì¦ˆ ì°¨ëŸ‰ ìš´í–‰ í—ˆê°€ë¥¼ ì •ì§€í–ˆê³ , ì´ì— í¬ë£¨ì¦ˆëŠ” ë¯¸êµ­ ì „ì—­ì˜ ë¡œë³´íƒì‹œ ìš´ì˜ì„ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤. 

### OpenAIì˜ ì—…ê·¸ë ˆì´ë“œëœ GPT-4 Turbo [^ref-n-3]
- **GPT-4 Turbo**ëŠ” í•œ ë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í† í° ìˆ˜ë¥¼ ì´ì „ ìµœëŒ€ 32,000ê°œì—ì„œ 128,000ê°œë¡œ í™•ì¥í•©ë‹ˆë‹¤.

![fig1](20231221-1.webp){: width="500"}

### 28ê°œêµ­ AI ê·œì œ í˜‘ì˜ [^ref-n-4]
- ì¤‘êµ­, ë¯¸êµ­, ìœ ëŸ½ì—°í•©ì„ í¬í•¨í•œ 28ê°œêµ­ì´ AI ìœ„í—˜ ì™„í™”ë¥¼ ìœ„í•œ ì„ ì–¸ì— ì„œëª…í–ˆìŠµë‹ˆë‹¤.

### OECDì˜ ìƒˆë¡œìš´ AI ì •ì˜ [^ref-n-5]
- OECDëŠ” AI ì‹œìŠ¤í…œì— ëŒ€í•œ ìƒˆë¡œìš´ ì •ì˜ë¥¼ ì±„íƒí–ˆìŠµë‹ˆë‹¤. AI ê·œì œ ë° ì‚°ì—… í‘œì¤€ì˜ ì¼í™˜ìœ¼ë¡œ ì±„íƒë˜ê³  ìˆëŠ” ì¼ë¶€ í•µì‹¬ ì£¼ì œì™€ ì •ì˜ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤.

### í• ë¦¬ìš°ë“œ ë°°ìš° íŒŒì—…ê³¼ AI [^ref-n-6]
- **í• ë¦¬ìš°ë“œ ë°°ìš° íŒŒì—…**ì€ ë°°ìš°ì™€ ìŠ¤íŠœë””ì˜¤ê°€ ì˜í™” ì œì‘ì— ìƒì„± AIë¥¼ ì‚¬ìš©í•˜ê¸°ë¡œ í•©ì˜í•˜ë©´ì„œ ëë‚¬ìŠµë‹ˆë‹¤. 
- ì˜í™” ìŠ¤íŠœë””ì˜¤ëŠ” ìƒì„±ëœ ì—°ê¸°ë¥¼ ì‚¬ìš©í•˜ê¸° ì „ì— ë°°ìš°ì˜ ë™ì˜ë¥¼ êµ¬í•˜ê³  ë°°ìš°ì—ê²Œ ë³´ìƒí•´ì•¼ í•©ë‹ˆë‹¤. 

### ChatGPTì™€ DDOS [^ref-n-7]
- 11ì›” 8ì¼ ChatGPT ì¤‘ë‹¨ì€ DDoS ê³µê²©ìœ¼ë¡œ ì¸í•´ ë°œìƒí–ˆì„ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤ê³  OpenAIê°€ ë°í˜”ìŠµë‹ˆë‹¤.

### êµ¬ê¸€ì˜ Anthropic íˆ¬ì [^ref-n-8]
- êµ¬ê¸€ì´ AI ìŠ¤íƒ€íŠ¸ì—… **Anthropic**ì— 20ì–µ ë‹¬ëŸ¬ì˜ íˆ¬ìë¥¼ ì•½ì†í•œ ê²ƒìœ¼ë¡œ, ì•„ë§ˆì¡´ì˜ íˆ¬ìì— ì´ì–´ ê±°ë¬¼ë“¤ì´ ì¸ê³µì§€ëŠ¥ì„ ê°œë°œí•˜ëŠ” ìŠ¤íƒ€íŠ¸ì—…ì— ëŒ€í•œ íˆ¬ìë¥¼ ëŠ˜ë¦¬ê³  ìˆëŠ” ì¶”ì„¸ì…ë‹ˆë‹¤.

### OpenAI CEO ìƒ˜ ì•ŒíŠ¸ë¨¼(Sam Altman) í•´ê³  í›„ ë³µê·€ [^ref-n-9]
- **ìƒ˜ ì•ŒíŠ¸ë¨¼(Sam Altman)**ì˜ í•´ê³ ì™€ ë™ì‹œì— Greg Brockman(ì‹¤ì œ ì‚¬ì¥ ê²¸ ê³µë™ ì°½ì—…ì)ì´ ì´ì‚¬íšŒ ì˜ì¥ì—ì„œ ë¬¼ëŸ¬ë‚˜ê³  Mira Murati(ì‹¤ì œ CTO)ê°€ ì„ì‹œ CEOê°€ ë©ë‹ˆë‹¤.
- ì´í›„ Emmett Shearê°€ ì„ì‹œ CEOê°€ ë˜ì—ˆê³  ê²°êµ­ ìƒ˜ ì•ŒíŠ¸ë¨¼ì´ ë‹¤ì‹œ CEOë¡œ ë³µê·€í–ˆìŠµë‹ˆë‹¤.

![fig2](20231221-2.webp){: width="600"}

### AIë¥¼ í™œìš©í•œ UX ë””ìì¸ [^ref-n-10]
- **TLDraw**ëŠ” ìŠ¤ì¼€ì¹˜ë¥¼ ë°˜ë³µ ê°€ëŠ¥í•œ ì‹¤ì œ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ChatGPT ê¸°ë°˜ ë„êµ¬ë¥¼ ë„ì…í•˜ì—¬ ë””ìì¸ê³¼ êµ¬í˜„ì„ ì—°ê²°í•©ë‹ˆë‹¤. 

### DeepMindì˜ ìƒˆ ê¸°ìƒ ì˜ˆì¸¡ ë”¥ëŸ¬ë‹ ëª¨ë¸ GraphCast [^ref-n-11]
- 40ë…„ ë™ì•ˆì˜ ë‚ ì”¨ ë°ì´í„°ë¡œ í›ˆë ¨ëœ **GraphCast**ëŠ” ê¸°ìƒ ì´ë³€ì„ í¬í•¨í•œ ë‹¤ì–‘í•œ ëŒ€ê¸° ë° ì§€êµ¬ í‘œë©´ ë³€ìˆ˜ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

![fig3](20231221-3.webp){: width="600"}

&nbsp;
&nbsp;
&nbsp;

-------------------

## ğŸ“° **Papers**

### The Foundation Model Transparency Index [^ref-pa-1]
- ìŠ¤íƒ í¼ë“œ ì—°êµ¬ì†ŒëŠ” 10ê°œ ì¸ê¸° AI ëª¨ë¸ì„ íˆ¬ëª…ì„± ì¸¡ë©´ì—ì„œ í‰ê°€í–ˆìŠµë‹ˆë‹¤. 
- ì´ëŠ” AI ëª¨ë¸ì˜ íˆ¬ëª…ì„±ì´ ë¶€ì¡±í•œ ìƒí™©ì„ ë“œëŸ¬ë‚´ë©°, ì—°êµ¬ì™€ í¸í–¥ì„± ëŒ€ì‘ì„ ìœ„í•´ ë” ë§ì€ ì •ë³´ ê³µê°œê°€ í•„ìš”í•¨ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

### Improving Image Generation with Better Captions [^ref-pa-2]
- OpenAIì™€ ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ ì—°êµ¬ìë“¤ì´ ì´ë¯¸ì§€-ìº¡ì…˜ ë°ì´í„°ì…‹ì„ í†µí•´ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ëŠ”ë°, 
ì›¹ì—ì„œ ìŠ¤í¬ë©ëœ ìº¡ì…˜ë³´ë‹¤ ë” ìƒì„¸í•œ ìƒì„±ëœ ìº¡ì…˜ì„ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨í•˜ë©´ ë” í’ë¶€í•œ ì´ë¯¸ì§€-ë‹¨ì–´ ê´€ê³„ë¥¼ ë°°ìš¸ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ë°í˜”ìŠµë‹ˆë‹¤.

### Deep Learning for Day Forecasts from Sparse Observations [^ref-pa-3]
- Google Researchì™€ DeepMindëŠ” **MetNet-3** ì´ë¼ëŠ” ì‹ ê²½ë§ ê¸°ìƒ ì˜ˆì¸¡ ëª¨ë¸ì„ ì¶œì‹œí–ˆìŠµë‹ˆë‹¤.
- MetNet-3ì˜ ì‹¤ì‹œê°„ ì¼ê¸° ì˜ˆë³´ëŠ” ë‹¤ì–‘í•œ Google ì œí’ˆ ë‚´ì—ì„œ ì‘ë™í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ë¯¸êµ­ê³¼ ìœ ëŸ½ ì¼ë¶€ ì§€ì—­ì˜ ê°•ìˆ˜ëŸ‰ì— ëŒ€í•œ ì •í™•í•œ ë‚ ì”¨ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

### Evaluating Large Language Models: A Comprehensive Survey [^ref-pa-4]
- LLMì— ëŒ€í•˜ì—¬ ì—¬ëŸ¬ ì¸¡ë©´ì— ëŒ€í•œ í‰ê°€ ë°©ë²• ë° ê¸°ì¤€ê³¼ ì„±ëŠ¥ í‰ê°€ë¥¼ ë‹¤ë£¨ë©°, LLM í‰ê°€ì— ëŒ€í•œ í¬ê´„ì ì¸ í”Œë«í¼ êµ¬ì¶•ì— ëŒ€í•´ ë…¼ì˜í•©ë‹ˆë‹¤. 

### Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks [^ref-pa-5]
- ë‹¤ì–‘í•œ ì»´í“¨í„° ë¹„ì „ ì‘ì—…ì„ ìœ„í•œ ëŒ€ê·œëª¨ ë²¤ì¹˜ë§ˆí‚¹ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. 

### YaRN: Efficient Context Window Extension of Large Language Models [^ref-pa-6]
- **YaRN**ëŠ” íš¨ìœ¨ì ì¸ ë°©ë²•ìœ¼ë¡œ transformer ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸ì˜ context window ì„ ì´ì „ë³´ë‹¤ í›¨ì”¬ ì ì€ í† í°ê³¼ í•™ìŠµ ë‹¨ê³„ë¡œ í™•ì¥ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### A Unified Framework to Enforce, Discover, and Promote Symmetry in Machine Learning [^ref-pa-7]
- ë¬¼ë¦¬í•™ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ëŒ€ì¹­ì„±ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•˜ë©°, ëŒ€ì¹­ì„±ì„ ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì— í†µí•©í•˜ëŠ” ì„¸ ê°€ì§€ ë°©ë²•ì„ ì œì‹œí•˜ê³  ì´ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ í†µí•©í•˜ëŠ” í‹€ì„ ì œì•ˆí•©ë‹ˆë‹¤.

### FP8-LM: Training FP8 Large Language Models [^ref-pa-8]
- ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ íš¨ìœ¨ì ì¸ í•™ìŠµì„ ìœ„í•´ ìƒˆë¡œìš´ **FP8-LM** í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•˜ì—¬ ëª¨ë¸ ì •í™•ë„ë¥¼ ìœ ì§€í•˜ë©´ì„œë„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ í¬ê²Œ ì¤„ì´ê³  í•™ìŠµ ì†ë„ë¥¼ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.

![fig4](20231221-4.png){: width="600"}
_Transformer layer with FP8 tensor and sequence parallelism._

### StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners [^ref-pa-9]
- Stable Diffusion ì´ë¯¸ì§€ ìƒì„±ê¸°ë¡œ ìƒì„±ëœ ì´ë¯¸ì§€ì— ëŒ€í•´ ë¹„ì „ ë³€í™˜ê¸°ë¥¼ í›ˆë ¨ì‹œí‚¤ëŠ” ìì²´ ê°ë… ë°©ë²•ì¸ **StableRep**ì„ ë„ì…í–ˆìŠµë‹ˆë‹¤.

### Simplifying Transformer Blocks [^ref-pa-10]
- Transformer ë¸”ë¡ ë‹¨ìˆœí™”ë¥¼ í†µí•´ í›ˆë ¨ ì†ë„ ì†ì‹¤ ì—†ì´ ë§ì€ ë¸”ë¡ì˜ êµ¬ì„±ìš”ì†Œë¥¼ ì œê±°í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ë°í˜”ìŠµë‹ˆë‹¤.

### Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models [^ref-pa-11]
- Transformer ëª¨ë¸ì´ ì‚¬ì „ í›ˆë ¨ëœ ë°ì´í„°ì—ì„œ ìƒˆë¡œìš´ ì‘ì—…ì„ ë‚´ì¬ëœ í•™ìŠµ(In-Context Learning)ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ì‚´í´ë´…ë‹ˆë‹¤.

### Alternating Updates for Efficient Transformers [^ref-pa-12]
- ê³„ì‚° ë¹„ìš©ì„ ëŠ˜ë¦¬ì§€ ì•Šê³  Transformer ëª¨ë¸ì˜ í™•ì¥ ë° ìš©ëŸ‰ ì¦ê°€ë¥¼ í™œìš©í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.

### Metaì˜ Emu Video ì™€ Emu Edit [^ref-pa-13]
- Diffusion ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ê°œë°œëœ ì´ë¯¸ì§€ í¸ì§‘ ë° í…ìŠ¤íŠ¸-ë¹„ë””ì˜¤ ìƒì„±ì„ ìœ„í•œ ìƒˆë¡œìš´ ëª¨ë¸ì„ ì œì‹œí•©ë‹ˆë‹¤.

### The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4 [^ref-pa-14]
- GPT-4ì˜ ê³¼í•™ì  ì—­ëŸ‰ì— ì´ˆì ì„ ë§ì¶”ì–´, ì•½ë¬¼ ë°œê²¬, ìƒë¬¼í•™, í™”í•™ ê³„ì‚°, ë¬¼ì§ˆ ë””ìì¸, ê·¸ë¦¬ê³  í¸ë¯¸ë¶„ ë°©ì •ì‹ ë“± ë‹¤ì–‘í•œ ê³¼í•™ ë¶„ì•¼ì—ì„œì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
- GPT-4ê°€ ë³µì¡í•œ ê³¼í•™ì  ë¬¸ì œ í•´ê²°ê³¼ ì§€ì‹ í†µí•©ì„ ìœ„í•´ ìœ ë§í•œ ì ì¬ë ¥ì„ ë³´ì—¬ì¤€ë‹¤ëŠ” ê²°ë¡ ì„ ë„ì¶œí–ˆìŠµë‹ˆë‹¤.

### Fine-tuning Language Models for Factuality [^ref-pa-15]
- ë¼ë²¨ë§ ì—†ì´ LLM ì„ ë³´ë‹¤ ì‚¬ì‹¤ì ìœ¼ë¡œ fine-tuningí•˜ì—¬ hallucinationì„ ì¤„ì´ê³ , ëª¨ë¸ì˜ ì‚¬ì‹¤ì„±ì„ ê°œì„ í•˜ëŠ” ë°©ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤. 
- Llama-2 ëª¨ë¸ì„ ì´ëŸ¬í•œ ë°©ë²•ìœ¼ë¡œ fine-tuningí•˜ë©´ ìƒì„±ëœ ë¬¸ì¥ ì¤‘ ì˜¬ë°”ë¥¸ ë¹„ìœ¨ì´ ìƒë‹¹íˆ ì¦ê°€í•¨ì„ ì‹¤í—˜ì ìœ¼ë¡œ ì…ì¦í•˜ì˜€ìŠµë‹ˆë‹¤.

### JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models [^ref-pa-16]
- **JARVIS-1**ì€ Minecraftì—ì„œ ë‹¤ì–‘í•œ ê³¼ì œë¥¼ ì¸ê°„ê³¼ ìœ ì‚¬í•œ ì œì–´ì™€ ê´€ì°°ì„ í†µí•´ ì™„ìˆ˜í•©ë‹ˆë‹¤.
- ì´ëŠ” ì‹œê°ì  ê´€ì¸¡ê³¼ í…ìŠ¤íŠ¸ ëª…ë ¹ì„ ê³„íšìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë‹¤ì¤‘ ëª¨ë‹¬ ì–¸ì–´ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ê°œë°œë˜ì—ˆìœ¼ë©°, ë‹¨ê¸° ë° ì¥ê¸°ì  ê³¼ì œì—ì„œ ìš°ìˆ˜í•œ ì„±ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.

![fig5](20231221-5.png){: width="600"}
_How does JARV IS-1 unlock the technology tree of the Minecraft universe._

### Technical Report: Large Language Models can Strategically Deceive their Users when Put Under Pressure [^ref-pa-17]
- GPT-4ê°€ ì „ëµì ìœ¼ë¡œ ì‚¬ìš©ìë¥¼ ì†ì´ëŠ” í–‰ë™ì„ ë³´ì´ëŠ” ìƒí™©ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ì—¬ LLMì˜ ì‹¤ì œ ìƒí™©ì—ì„œ ë¹„ì •ë ¬ëœ í–‰ë™ì„ ë³´ì—¬ì¤€ ì²« ì‚¬ë¡€ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.

### Text2Performer: Text-Driven Human Video Generation [^ref-pa-18]
- **Text2Performer**ëŠ” í…ìŠ¤íŠ¸ì˜ ëª…í™•í•œ ë™ì‘ìœ¼ë¡œ ìƒìƒí•œ íœ´ë¨¼ ë¹„ë””ì˜¤ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ìƒì„±í•  ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ ê¸°ë°˜ íœ´ë¨¼ ë¹„ë””ì˜¤ ìƒì„± ê¸°ìˆ ì…ë‹ˆë‹¤. 


&nbsp;
&nbsp;
&nbsp;

----------------

## ğŸ§  **Deep Learning, LLM**
### LLM ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê°€ì¥ ì¢‹ì€ ë„êµ¬ëŠ”? [^ref-dl-1]
- RAGì™€ ë¯¸ì„¸ ì¡°ì •ì€ LLM ê¸°ë°˜ ì‘ìš© í”„ë¡œê·¸ë¨ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ìœ ì‚¬í•œ ë‘ ê°€ì§€ ê¸°ìˆ ì´ë©° ì´ë“¤ì˜ ì°¨ì´ì  ë° ì¥ë‹¨ì ì„ ì†Œê°œí•©ë‹ˆë‹¤.

### PEFT: Parameter-Efficient Fine-Tuning [^ref-dl-2]
- ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” LLMì„ íš¨ìœ¨ì ìœ¼ë¡œ íŠœë‹í•˜ì—¬ ì»´í“¨íŒ… ë° ì €ì¥ ê³µê°„ì„ ì ˆì•½í•˜ë©´ì„œ ì „ì²´ fine-tuningê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ëŠ” PEFT ê¸°ë²•ì„ ì œê³µí•©ë‹ˆë‹¤.

### LLM ë°°í¬ ë¹„ìš© ë¶„ì„ [^ref-dl-3]
- LLMì„ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ë°°í¬í•˜ëŠ” 3ê°€ì§€ ë°©ë²•ê³¼ ê·¸ ë¹„ìš©ì„ ì‚´í´ë´…ë‹ˆë‹¤.

### ë”¥ëŸ¬ë‹ ëª¨ë¸ ì‹œê°í™” í•˜ê¸° [^ref-dl-4]
- ë”¥ëŸ¬ë‹ ëª¨ë¸ ì‹œê°í™”ì— ëŒ€í•œ ì‹¤ì œ ì‚¬ë¡€ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° íŠœí† ë¦¬ì–¼ì„ ì œê³µí•©ë‹ˆë‹¤.

&nbsp;
&nbsp;
&nbsp;

----------------

## âš™ï¸ **MLOps & Data**
### AI ì œí’ˆì„ ë‹¤ë¥´ê²Œ ê°œë°œí•˜ê¸° [^ref-ml-1]
- AI ì œí’ˆì„ ë§Œë“¤ ë•Œ, ë‹¤ë¥¸ ì‚¬ëŒë“¤ì´ í•˜ê³  ìˆëŠ” ê²ƒê³¼ëŠ” ë‹¤ë¥´ê²Œ ì ‘ê·¼í•´ì•¼ í•œë‹¤ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤. 
- AIë¥¼ ìµœì†Œí™”í•˜ë©´ì„œë„ ê³ ìœ í•œ ëª¨ë¸ì„ ê°œë°œí•˜ëŠ” ê²ƒì´ ì£¼ëœ ìš”ì ì…ë‹ˆë‹¤.

### Data í”Œë«í¼ê³¼ ML í”Œë«í¼ [^ref-ml-2]
- ë°ì´í„° í”Œë«í¼ì— MLê¸°ëŠ¥ì„ ì¶”ê°€í•˜ëŠ” ì ì§„ì ì¸ ë°©ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤.

### ë°ì´í„° ì—”ì§€ë‹ˆì–´ì˜ ë‘ê°€ì§€ ì›í˜• [^ref-ml-3]
- ë°ì´í„° ì—”ì§€ë‹ˆì–´ì˜ ë‘ê°€ì§€ ì›í˜•ì„ ì†Œê°œí•˜ë©°, í•˜ë‚˜ëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì œ í•´ê²°ì— ì¤‘ì ì„ ë‘ê³  ë°ì´í„° ì†”ë£¨ì…˜ì„ ë§Œë“œëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ì  ì—”ì§€ë‹ˆì–´, ë‹¤ë¥¸ í•˜ë‚˜ëŠ” í™•ì¥ ê°€ëŠ¥í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•˜ëŠ” ê¸°ìˆ ì  ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤.

### DoorDashì—ì„œì˜ MLOps ë³€í™˜ [^ref-ml-4]
- **DoorDash**ê°€ ML ì›Œí¬í”Œë¡œ ìë™í™”ë¥¼ ìœ„í•œ ê°„ì†Œí™”ëœ í™˜ê²½ì„ êµ¬ì¶•í•˜ì—¬ ML ê°œë°œ ì†ë„ë¥¼ ì–´ë–»ê²Œ ê°€ì†í™”í–ˆëŠ”ì§€ ì„¤ëª…í•©ë‹ˆë‹¤. 



&nbsp;
&nbsp;
&nbsp;

----------------

## ğŸ’» **Programming**
### íŒŒì´ì¬ìœ¼ë¡œ 17ë§Œë°° ë¹ ë¥´ê²Œ ë°ì´í„° ë¶„ì„í•˜ê¸° [^ref-pr-1]
- ë°ì´í„° ë¶„ì„ ì‘ì—…ì˜ ì†ë„ë¥¼ í¬ê²Œ ë†’ì´ê¸° ìœ„í•´ Python ì½”ë“œë¥¼ ìµœì í™”í•˜ëŠ” ì‚¬ë¡€ ì—°êµ¬ ì…ë‹ˆë‹¤.


&nbsp;
&nbsp;
&nbsp;

----------------
## References
[^ref-n-1]: [Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence](https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence)

[^ref-n-2]: [California sidelines GM Cruise's driverless cars, cites safety risk](https://www.reuters.com/business/autos-transportation/california-suspends-gm-cruises-driverless-autonomous-vehicle-permits-2023-10-24)

[^ref-n-3]: [New models and developer products announced at DevDay](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)

[^ref-n-4]: [The Bletchley Declaration by Countries Attending the AI Safety Summit, 1-2 November 2023](https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023)

[^ref-n-5]: [OECD AI Principles overview](https://oecd.ai/en/ai-principles)

[^ref-n-6]: [TV/THEATRICAL CONTRACTS 2023](https://deadline.com/wp-content/uploads/2023/11/SAG-AFTRA-TV-Theatrical-Summary-Agreement.pdf)

[^ref-n-7]: [Periodic outages across ChatGPT and API](https://status.openai.com/incidents/21vl32gvx3hb)

[^ref-n-8]: [Google Commits $2 Billion in Funding to AI Startup Anthropic](https://www.wsj.com/tech/ai/google-commits-2-billion-in-funding-to-ai-startup-anthropic-db4d4c50)

[^ref-n-9]: [Sam Altman Is Reinstated as OpenAIâ€™s Chief Executive](https://www.nytimes.com/2023/11/22/technology/openai-sam-altman-returns.html)

[^ref-n-10]: [make real, the story so far](https://tldraw.substack.com/p/make-real-the-story-so-far)

[^ref-n-11]: [GraphCast: AI model for faster and more accurate global weather forecasting](https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/)

[^ref-pa-1]: [The Foundation Model Transparency Index](https://arxiv.org/abs/2310.12941)

[^ref-pa-2]: [Improving Image Generation with Better Captions](https://cdn.openai.com/papers/dall-e-3.pdf)

[^ref-pa-3]: [Deep Learning for Day Forecasts from Sparse Observations](https://arxiv.org/abs/2306.06079)

[^ref-pa-4]: [Evaluating Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2310.19736)

[^ref-pa-5]: [Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks](https://arxiv.org/abs/2310.19909)

[^ref-pa-6]: [YaRN: Efficient Context Window Extension of Large Language Models](https://arxiv.org/abs/2309.00071)

[^ref-pa-7]: [A Unified Framework to Enforce, Discover, and Promote Symmetry in Machine Learning](https://arxiv.org/abs/2311.00212)

[^ref-pa-8]: [FP8-LM: Training FP8 Large Language Models](https://arxiv.org/abs/2310.18313)

[^ref-pa-9]: [StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners](https://arxiv.org/abs/2306.00984)

[^ref-pa-10]: [Simplifying Transformer Blocks](https://arxiv.org/abs/2311.01906)

[^ref-pa-11]: [Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models](https://arxiv.org/abs/2311.00871)

[^ref-pa-12]: [Alternating Updates for Efficient Transformers](https://arxiv.org/abs/2301.13310)

[^ref-pa-13]: [Introducing Emu Video and Emu Edit, our latest generative AI research milestones](https://ai.meta.com/blog/emu-text-to-video-generation-image-editing-research)

[^ref-pa-14]: [The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4](https://arxiv.org/abs/2311.07361)

[^ref-pa-15]: [Fine-tuning Language Models for Factuality](https://arxiv.org/abs/2311.08401)

[^ref-pa-16]: [JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models](https://arxiv.org/abs/2311.05997)

[^ref-pa-17]: [Technical Report: Large Language Models can Strategically Deceive their Users when Put Under Pressure](https://arxiv.org/abs/2311.07590)

[^ref-pa-18]: [Text2Performer: Text-Driven Human Video Generation](https://arxiv.org/abs/2304.08483)

[^ref-dl-1]: [RAG vs Finetuning â€” Which Is the Best Tool to Boost Your LLM Application?](https://towardsdatascience.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application-94654b1eaba7)

[^ref-dl-2]: [PEFT: Parameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware](https://huggingface.co/blog/peft)

[^ref-dl-3]: [Cost Analysis of deploying LLMs: A comparative Study between Cloud Managed, Self-Hosted and 3rd Party LLMs](https://medium.com/artefact-engineering-and-data-science/llms-deployment-a-practical-cost-analysis-e0c1b8eb08ca)

[^ref-dl-4]: [How to Visualize Deep Learning Models](https://neptune.ai/blog/deep-learning-visualization)

[^ref-ml-1]: [Donâ€™t Build AI Products The Way Everyone Else Is Doing It](https://www.builder.io/blog/build-ai)

[^ref-ml-2]: [From Data Platform to ML Platform](https://towardsdatascience.com/from-data-platform-to-ml-platform-4a8192edab5d)

[^ref-ml-3]: [Two Archetypes of Data Engineers](https://luminousmen.com/post/two-archetypes-of-data-engineers)

[^ref-ml-4]: [Transforming MLOps at DoorDash with Machine Learning Workbench](https://doordash.engineering/2023/11/28/transforming-mlops-at-doordash-with-machine-learning-workbench)


[^ref-pr-1]: [Analyzing Data 170,000x Faster with Python](https://sidsite.com/posts/python-corrset-optimization/)
