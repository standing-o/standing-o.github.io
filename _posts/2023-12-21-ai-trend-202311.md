---
title: "2023 AI 기술 트렌드 #2 | AI Trends in 2023"
date: 2023-12-21 00:00:00 +/-TTTT
categories: [AI Trend]
tags: [ai-trend]
math: true
toc: true
author: seoyoung
img_path: /assets/img/for_post/
pin: false
image:
  path: 20231123-t.jpg
  alt: ""
description: 인공지능 트렌드, 딥러닝 트렌드, 2023 인공지능 트렌드, 2023 인공지능 기술 트렌드, 최근 인공지능 동향, Recent AI Trend, AI Issue, AI News, Top AI Paper
---

> 2023년 11월의 AI 기술 동향과 주요 뉴스, 논문, 글, 사례 등을 소개합니다.
{: .prompt-info }

최근 AI 분야에서는 AI 규제 강화와 함께 기술 발전과 국제 협력이 주요 관심사로 떠오릅니다.

GPT-4 Turbo 출시와 세계적인 기업들의 AI 투자와 관련한 뉴스도 주목받고 있습니다.

&nbsp;
&nbsp;
&nbsp;

## 💡 **News**
### 바이든 대통령, AI 규제 강화하는 행정 명령 발표 [^ref-n-1]
- **미국** 대통령 바이든은 AI 규제를 위한 행정 명령을 발표했습니다. 
- 대통령의 법적 권한을 바탕으로 AI 기업과 기관들에게 특정 모델의 보고와 테스트를 요구하며 연방 기관에 AI 기준을 설정하도록 지시했습니다. 

### 캘리포니아, 크루즈 무인 자율주행 차량 운행 중단 조치 [^ref-n-2]
- **캘리포니아**는 운전자 없는 크루즈 차량 운행 허가를 정지했고, 이에 크루즈는 미국 전역의 로보택시 운영을 중단했습니다. 

### OpenAI의 업그레이드된 GPT-4 Turbo [^ref-n-3]
- **GPT-4 Turbo**는 한 번에 처리할 수 있는 토큰 수를 이전 최대 32,000개에서 128,000개로 확장합니다.

![fig1](20231221-1.webp){: width="500"}

### 28개국 AI 규제 협의 [^ref-n-4]
- 중국, 미국, 유럽연합을 포함한 28개국이 AI 위험 완화를 위한 선언에 서명했습니다.

### OECD의 새로운 AI 정의 [^ref-n-5]
- OECD는 AI 시스템에 대한 새로운 정의를 채택했습니다. AI 규제 및 산업 표준의 일환으로 채택되고 있는 일부 핵심 주제와 정의에 대해 자세히 설명합니다.

### 할리우드 배우 파업과 AI [^ref-n-6]
- **할리우드 배우 파업**은 배우와 스튜디오가 영화 제작에 생성 AI를 사용하기로 합의하면서 끝났습니다. 
- 영화 스튜디오는 생성된 연기를 사용하기 전에 배우의 동의를 구하고 배우에게 보상해야 합니다. 

### ChatGPT와 DDOS [^ref-n-7]
- 11월 8일 ChatGPT 중단은 DDoS 공격으로 인해 발생했을 가능성이 높다고 OpenAI가 밝혔습니다.

### 구글의 Anthropic 투자 [^ref-n-8]
- 구글이 AI 스타트업 **Anthropic**에 20억 달러의 투자를 약속한 것으로, 아마존의 투자에 이어 거물들이 인공지능을 개발하는 스타트업에 대한 투자를 늘리고 있는 추세입니다.

### OpenAI CEO 샘 알트먼(Sam Altman) 해고 후 복귀 [^ref-n-9]
- **샘 알트먼(Sam Altman)**의 해고와 동시에 Greg Brockman(실제 사장 겸 공동 창업자)이 이사회 의장에서 물러나고 Mira Murati(실제 CTO)가 임시 CEO가 됩니다.
- 이후 Emmett Shear가 임시 CEO가 되었고 결국 샘 알트먼이 다시 CEO로 복귀했습니다.

![fig2](20231221-2.webp){: width="600"}

### AI를 활용한 UX 디자인 [^ref-n-10]
- **TLDraw**는 스케치를 반복 가능한 실제 웹 애플리케이션으로 변환하는 ChatGPT 기반 도구를 도입하여 디자인과 구현을 연결합니다. 

### DeepMind의 새 기상 예측 딥러닝 모델 GraphCast [^ref-n-11]
- 40년 동안의 날씨 데이터로 훈련된 **GraphCast**는 기상 이변을 포함한 다양한 대기 및 지구 표면 변수를 예측합니다.

![fig3](20231221-3.webp){: width="600"}

&nbsp;
&nbsp;
&nbsp;

-------------------

## 📰 **Papers**

### The Foundation Model Transparency Index [^ref-pa-1]
- 스탠퍼드 연구소는 10개 인기 AI 모델을 투명성 측면에서 평가했습니다. 
- 이는 AI 모델의 투명성이 부족한 상황을 드러내며, 연구와 편향성 대응을 위해 더 많은 정보 공개가 필요함을 시사합니다.

### Improving Image Generation with Better Captions [^ref-pa-2]
- OpenAI와 마이크로소프트 연구자들이 이미지-캡션 데이터셋을 통해 모델을 훈련시키는데, 
웹에서 스크랩된 캡션보다 더 상세한 생성된 캡션을 사용하여 훈련하면 더 풍부한 이미지-단어 관계를 배울 수 있다는 것을 밝혔습니다.

### Deep Learning for Day Forecasts from Sparse Observations [^ref-pa-3]
- Google Research와 DeepMind는 **MetNet-3** 이라는 신경망 기상 예측 모델을 출시했습니다.
- MetNet-3의 실시간 일기 예보는 다양한 Google 제품 내에서 작동하여 사용자에게 미국과 유럽 일부 지역의 강수량에 대한 정확한 날씨 정보를 제공합니다.

### Evaluating Large Language Models: A Comprehensive Survey [^ref-pa-4]
- LLM에 대하여 여러 측면에 대한 평가 방법 및 기준과 성능 평가를 다루며, LLM 평가에 대한 포괄적인 플랫폼 구축에 대해 논의합니다. 

### Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks [^ref-pa-5]
- 다양한 컴퓨터 비전 작업을 위한 대규모 벤치마킹 프레임워크입니다. 

### YaRN: Efficient Context Window Extension of Large Language Models [^ref-pa-6]
- **YaRN**는 효율적인 방법으로 transformer 기반 언어 모델의 context window 을 이전보다 훨씬 적은 토큰과 학습 단계로 확장시킬 수 있습니다.

### A Unified Framework to Enforce, Discover, and Promote Symmetry in Machine Learning [^ref-pa-7]
- 물리학과 머신러닝에서 대칭성의 중요성을 강조하며, 대칭성을 기계 학습 모델에 통합하는 세 가지 방법을 제시하고 이를 수학적으로 통합하는 틀을 제안합니다.

### FP8-LM: Training FP8 Large Language Models [^ref-pa-8]
- 대규모 언어 모델의 효율적인 학습을 위해 새로운 **FP8-LM** 프레임워크를 제안하여 모델 정확도를 유지하면서도 메모리 사용량을 크게 줄이고 학습 속도를 향상시켰습니다.

![fig4](20231221-4.png){: width="600"}
_Transformer layer with FP8 tensor and sequence parallelism._

### StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners [^ref-pa-9]
- Stable Diffusion 이미지 생성기로 생성된 이미지에 대해 비전 변환기를 훈련시키는 자체 감독 방법인 **StableRep**을 도입했습니다.

### Simplifying Transformer Blocks [^ref-pa-10]
- Transformer 블록 단순화를 통해 훈련 속도 손실 없이 많은 블록의 구성요소를 제거할 수 있다는 것을 밝혔습니다.

### Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models [^ref-pa-11]
- Transformer 모델이 사전 훈련된 데이터에서 새로운 작업을 내재된 학습(In-Context Learning)으로 처리할 수 있는 능력을 살펴봅니다.

### Alternating Updates for Efficient Transformers [^ref-pa-12]
- 계산 비용을 늘리지 않고 Transformer 모델의 확장 및 용량 증가를 활용할 수 있는 방법을 제안합니다.

### Meta의 Emu Video 와 Emu Edit [^ref-pa-13]
- Diffusion 모델을 기반으로 개발된 이미지 편집 및 텍스트-비디오 생성을 위한 새로운 모델을 제시합니다.

### The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4 [^ref-pa-14]
- GPT-4의 과학적 역량에 초점을 맞추어, 약물 발견, 생물학, 화학 계산, 물질 디자인, 그리고 편미분 방정식 등 다양한 과학 분야에서의 성능을 평가합니다.
- GPT-4가 복잡한 과학적 문제 해결과 지식 통합을 위해 유망한 잠재력을 보여준다는 결론을 도출했습니다.

### Fine-tuning Language Models for Factuality [^ref-pa-15]
- 라벨링 없이 LLM 을 보다 사실적으로 fine-tuning하여 hallucination을 줄이고, 모델의 사실성을 개선하는 방법을 소개합니다. 
- Llama-2 모델을 이러한 방법으로 fine-tuning하면 생성된 문장 중 올바른 비율이 상당히 증가함을 실험적으로 입증하였습니다.

### JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models [^ref-pa-16]
- **JARVIS-1**은 Minecraft에서 다양한 과제를 인간과 유사한 제어와 관찰을 통해 완수합니다.
- 이는 시각적 관측과 텍스트 명령을 계획으로 변환하는 다중 모달 언어 모델을 기반으로 개발되었으며, 단기 및 장기적 과제에서 우수한 성과를 보여줍니다.

![fig5](20231221-5.png){: width="600"}
_How does JARV IS-1 unlock the technology tree of the Minecraft universe._

### Technical Report: Large Language Models can Strategically Deceive their Users when Put Under Pressure [^ref-pa-17]
- GPT-4가 전략적으로 사용자를 속이는 행동을 보이는 상황을 시뮬레이션하여 LLM의 실제 상황에서 비정렬된 행동을 보여준 첫 사례를 제시합니다.

### Text2Performer: Text-Driven Human Video Generation [^ref-pa-18]
- **Text2Performer**는 텍스트의 명확한 동작으로 생생한 휴먼 비디오를 효율적으로 생성할 수 있는 텍스트 기반 휴먼 비디오 생성 기술입니다. 


&nbsp;
&nbsp;
&nbsp;

----------------

## 🧠 **Deep Learning, LLM**
### LLM 성능을 향상시키는 가장 좋은 도구는? [^ref-dl-1]
- RAG와 미세 조정은 LLM 기반 응용 프로그램의 성능을 향상시키기 위한 유사한 두 가지 기술이며 이들의 차이점 및 장단점을 소개합니다.

### PEFT: Parameter-Efficient Fine-Tuning [^ref-dl-2]
- 이 라이브러리는 LLM을 효율적으로 튜닝하여 컴퓨팅 및 저장 공간을 절약하면서 전체 fine-tuning과 유사한 성능을 달성하는 PEFT 기법을 제공합니다.

### LLM 배포 비용 분석 [^ref-dl-3]
- LLM을 애플리케이션에 배포하는 3가지 방법과 그 비용을 살펴봅니다.

### 딥러닝 모델 시각화 하기 [^ref-dl-4]
- 딥러닝 모델 시각화에 대한 실제 사례와 라이브러리 및 튜토리얼을 제공합니다.

&nbsp;
&nbsp;
&nbsp;

----------------

## ⚙️ **MLOps & Data**
### AI 제품을 다르게 개발하기 [^ref-ml-1]
- AI 제품을 만들 때, 다른 사람들이 하고 있는 것과는 다르게 접근해야 한다는 내용입니다. 
- AI를 최소화하면서도 고유한 모델을 개발하는 것이 주된 요점입니다.

### Data 플랫폼과 ML 플랫폼 [^ref-ml-2]
- 데이터 플랫폼에 ML기능을 추가하는 점진적인 방법을 소개합니다.

### 데이터 엔지니어의 두가지 원형 [^ref-ml-3]
- 데이터 엔지니어의 두가지 원형을 소개하며, 하나는 비즈니스 문제 해결에 중점을 두고 데이터 솔루션을 만드는 비즈니스적 엔지니어, 다른 하나는 확장 가능한 데이터 파이프라인을 구축하는 기술적 엔지니어입니다.

### DoorDash에서의 MLOps 변환 [^ref-ml-4]
- **DoorDash**가 ML 워크플로 자동화를 위한 간소화된 환경을 구축하여 ML 개발 속도를 어떻게 가속화했는지 설명합니다. 



&nbsp;
&nbsp;
&nbsp;

----------------

## 💻 **Programming**
### 파이썬으로 17만배 빠르게 데이터 분석하기 [^ref-pr-1]
- 데이터 분석 작업의 속도를 크게 높이기 위해 Python 코드를 최적화하는 사례 연구 입니다.


&nbsp;
&nbsp;
&nbsp;

----------------
## References
[^ref-n-1]: [Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence](https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence)

[^ref-n-2]: [California sidelines GM Cruise's driverless cars, cites safety risk](https://www.reuters.com/business/autos-transportation/california-suspends-gm-cruises-driverless-autonomous-vehicle-permits-2023-10-24)

[^ref-n-3]: [New models and developer products announced at DevDay](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)

[^ref-n-4]: [The Bletchley Declaration by Countries Attending the AI Safety Summit, 1-2 November 2023](https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023)

[^ref-n-5]: [OECD AI Principles overview](https://oecd.ai/en/ai-principles)

[^ref-n-6]: [TV/THEATRICAL CONTRACTS 2023](https://deadline.com/wp-content/uploads/2023/11/SAG-AFTRA-TV-Theatrical-Summary-Agreement.pdf)

[^ref-n-7]: [Periodic outages across ChatGPT and API](https://status.openai.com/incidents/21vl32gvx3hb)

[^ref-n-8]: [Google Commits $2 Billion in Funding to AI Startup Anthropic](https://www.wsj.com/tech/ai/google-commits-2-billion-in-funding-to-ai-startup-anthropic-db4d4c50)

[^ref-n-9]: [Sam Altman Is Reinstated as OpenAI’s Chief Executive](https://www.nytimes.com/2023/11/22/technology/openai-sam-altman-returns.html)

[^ref-n-10]: [make real, the story so far](https://tldraw.substack.com/p/make-real-the-story-so-far)

[^ref-n-11]: [GraphCast: AI model for faster and more accurate global weather forecasting](https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/)

[^ref-pa-1]: [The Foundation Model Transparency Index](https://arxiv.org/abs/2310.12941)

[^ref-pa-2]: [Improving Image Generation with Better Captions](https://cdn.openai.com/papers/dall-e-3.pdf)

[^ref-pa-3]: [Deep Learning for Day Forecasts from Sparse Observations](https://arxiv.org/abs/2306.06079)

[^ref-pa-4]: [Evaluating Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2310.19736)

[^ref-pa-5]: [Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks](https://arxiv.org/abs/2310.19909)

[^ref-pa-6]: [YaRN: Efficient Context Window Extension of Large Language Models](https://arxiv.org/abs/2309.00071)

[^ref-pa-7]: [A Unified Framework to Enforce, Discover, and Promote Symmetry in Machine Learning](https://arxiv.org/abs/2311.00212)

[^ref-pa-8]: [FP8-LM: Training FP8 Large Language Models](https://arxiv.org/abs/2310.18313)

[^ref-pa-9]: [StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners](https://arxiv.org/abs/2306.00984)

[^ref-pa-10]: [Simplifying Transformer Blocks](https://arxiv.org/abs/2311.01906)

[^ref-pa-11]: [Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models](https://arxiv.org/abs/2311.00871)

[^ref-pa-12]: [Alternating Updates for Efficient Transformers](https://arxiv.org/abs/2301.13310)

[^ref-pa-13]: [Introducing Emu Video and Emu Edit, our latest generative AI research milestones](https://ai.meta.com/blog/emu-text-to-video-generation-image-editing-research)

[^ref-pa-14]: [The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4](https://arxiv.org/abs/2311.07361)

[^ref-pa-15]: [Fine-tuning Language Models for Factuality](https://arxiv.org/abs/2311.08401)

[^ref-pa-16]: [JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models](https://arxiv.org/abs/2311.05997)

[^ref-pa-17]: [Technical Report: Large Language Models can Strategically Deceive their Users when Put Under Pressure](https://arxiv.org/abs/2311.07590)

[^ref-pa-18]: [Text2Performer: Text-Driven Human Video Generation](https://arxiv.org/abs/2304.08483)

[^ref-dl-1]: [RAG vs Finetuning — Which Is the Best Tool to Boost Your LLM Application?](https://towardsdatascience.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application-94654b1eaba7)

[^ref-dl-2]: [PEFT: Parameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware](https://huggingface.co/blog/peft)

[^ref-dl-3]: [Cost Analysis of deploying LLMs: A comparative Study between Cloud Managed, Self-Hosted and 3rd Party LLMs](https://medium.com/artefact-engineering-and-data-science/llms-deployment-a-practical-cost-analysis-e0c1b8eb08ca)

[^ref-dl-4]: [How to Visualize Deep Learning Models](https://neptune.ai/blog/deep-learning-visualization)

[^ref-ml-1]: [Don’t Build AI Products The Way Everyone Else Is Doing It](https://www.builder.io/blog/build-ai)

[^ref-ml-2]: [From Data Platform to ML Platform](https://towardsdatascience.com/from-data-platform-to-ml-platform-4a8192edab5d)

[^ref-ml-3]: [Two Archetypes of Data Engineers](https://luminousmen.com/post/two-archetypes-of-data-engineers)

[^ref-ml-4]: [Transforming MLOps at DoorDash with Machine Learning Workbench](https://doordash.engineering/2023/11/28/transforming-mlops-at-doordash-with-machine-learning-workbench)


[^ref-pr-1]: [Analyzing Data 170,000x Faster with Python](https://sidsite.com/posts/python-corrset-optimization/)
