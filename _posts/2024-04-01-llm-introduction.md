---
title: "LLM 및 Gen AI 프로젝트 입문"
date: 2024-04-01 00:00:00 +/-TTTT
categories: [AI Theory, NLP and LLM]
tags: [llm, generative-ai, prompt-engineering, attention, transformer]
math: true
toc: true
author: seoyoung
img_path: /assets/img/for_post/
pin: false
image:
  path: 20240401-t.jpg
  alt: ""
description: LLM, LLM 딥러닝, LLM 이란, LLM 인공지능 뜻, LLM 생성형 ai, Generative AI, Generative AI LLM, Prompt Engineering
---

> LLM과 Transformer의 구조, Prompt Engineering과 Inference 파라미터 설정에 대한 개념을 소개합니다.
{: .prompt-info }

LLM은 대규모 데이터와 컴퓨팅 파워를 이용해 학습된 언어 모델로, 자연어 이해와 유사한 작업 수행이 가능합니다. 

Prompt와 Completion을 통해 모델을 조작하고 다양한 작업을 수행할 수 있으며, Prompt Engineering과 Inference Parameter 설정을 통해 모델의 출력을 조절할 수 있습니다.


&nbsp;
&nbsp;
&nbsp;



## **Large Language Model, LLM 이란?**
- **LLM(Large Language Model)** 은 수십조가 넘는 단어 데이터로 수개월동안 대규모 컴퓨팅 파워를 통해 학습된 AI 언어 모델입니다.
- LLM은 자연어나 인간이 작성한 지시를 이해하고, 인간이 하는 것과 유사하게 작업을 수행할 수 있습니다.
- 수십억개의 파라미터를 가진 **Foundation 모델**은 언어뿐만 아니라 다양한 도메인에서 새로운 특성을 보여주고 있으며, 여러 분야에서 복잡한 작업을 분해하고 추론하는데 활용되고 있습니다.
- 모델의 파라미터가 많을수록 필요한 메모리도 많아지며, 모델이 수행 가능한 작업도 더욱 정교해집니다.
- LLM을 사용하여 정보 검색과 같은 작고 집약적인 작업을 수행할 수 있습니다.
  - 모델에 프롬프트를 기반으로 글을 쓰도록 요청하거나, 대화를 요약할 수 있도록 대화 일부를 프롬프트에 제공하고 모델은 이 데이터와 자연어에 대한 이해력을 바탕으로 요약을 생성합니다.
  - 다양한 번역 작업에 모델을 활용할 수 있습니다.
  - 그 외 개체명 인식(Named Entity Recognition), 단어 분류 등에도 활용 가능합니다.
    - 모델에게 뉴스 기사에서 식별된 모든 사람과 장소를 식별하도록 요청합니다.

- 모델의 파라미터에 인코딩된 지식을 이해하면 위 작업을 잘 수행하고 요청된 정보를 반환할 수 있습니다.
- 또한, LLM을 외부 데이터 소스에 연결하거나 외부 API를 활용하여 **증강(Augmenting)** 할 수 있습니다.


![fig1](20240401-1.png){: width="600"}

&nbsp;
&nbsp;
&nbsp;

## **LLM 규모 <sup>Scale</sup> 의 중요성**
- Foundation 모델의 규모가 수억 개 ~ 수백억 개의 파라미터로 증가함에 따라 모델이 가지는 주관적인 언어 이해력도 증가합니다.
- 모델의 파라미터에 저장된 언어 이해력은 사용자가 제공하는 작업을 처리하고 추론하는데 활용됩니다.
- 작은 모델은 특정한 작업에서 잘 수행될 수 있도록 세밀하게 조정될 수 있습니다.
- 지난 몇 년 동안 LLM이 보여주는 빠른 능력 향상은 규모를 바탕으로 설계된 아키텍쳐 덕분입니다.


&nbsp;
&nbsp;
&nbsp;




## **트랜스포머 <sup>Transformer</sup>**
- 문장 내 모든 단어의 관련성과 문맥을 학습할 수 있는 능력을 가진 모델입니다.
- 입력에 상관없이 모델이 각 단어에 대한 다른 모든 단어와의 관련성을 학습할 수 있도록 가중치를 적용합니다.

### **Transformer 이전의 텍스트 생성 <sup>Text Generation</sup>**
- **RNN(Recurrent Neural Network)** 구조를 바탕으로 텍스트에서 이전 단어들을 더 많이 참고하도록 설계할 때, 모델이 사용하는 리소스를 크게 확장해야 했습니다.
  - 모델을 확장해도 여전히 충분한 입력을 제공할 수 없었기에 좋은 예측을 할 수 없었습니다.
  - 다음 단어를 성공적으로 예측하려면 모델이 이전 몇 단어보다 더 많은 정보를 보아야 하며, 모델은 문장 전체를 이해할 필요성이 대두됐습니다.
  - 하지만 **언어는 복잡**하다는 이유로 이를 극복하기는 힘들었고, 이때 **트랜스포머(Transformer)** 가 등장했습니다.
    - 한 단어는 여러 의미를 가질 수 있고, 문장 구조 내의 단어 및 문법은 문법적으로 모호할 수 있습니다.

- 트랜스포머는 멀티 코어 GPU를 효율적으로 사용하여 확장될 수 있으며, 입력 데이터를 병렬로 처리할 수 있어 훨씬 더 큰 규모의 학습 데이터를 사용할 수 있습니다.
  - 또한, 처리 중인 단어의 의미에 **주의(Attention)** 를 기울일 수 있다는 장점이 있습니다.

&nbsp;
&nbsp;
&nbsp;


### **Transformer 구조**
#### 1. 단어 토큰화 <sup>Tokenization</sup>
- 텍스트를 개별 단어나 토큰으로 나누는 과정이며, LLM이 개별 단어의 의미와 텍스트 내에서의 관계를 분석할 수 있도록 해줍니다.
- 단어를 LLM이 다룰 수 있는 숫자 형태로 변환합니다.
  - 각 숫자는 모델이 작업할 수 있는 모든 가능한 단어의 어휘 사전에서의 위치를 나타냅니다.
- 토큰화 방법으로는 두 완전한 단어와 일치하는 토큰 ID를 활용하는 방법과 단어의 일부를 나타내기 위해 토큰 ID를 활용하는 방법이 있습니다.



#### 2. Embedding Layer 전달
- Embedding Layer는 학습가능한 벡터 공간으로, 각 토큰이 벡터로 표현되고 그 공간 내에서 고유한 위치를 차지합니다.
- 어휘 사전의 각 토큰 ID는 다차원 벡터에 대응되며, 이러한 벡터들은 입력 시퀀스 내 개별 토큰의 의미와 문맥을 인코딩 하도록 학습됩니다.
  - ex. Word2Vec

![fig3](20240401-3.png){: width="600"}


#### 3. Positional Encoding 추가
- 모델은 입력 토큰을 병렬로 처리합니다.
- 단어 순서에 관한 정보를 보존하고, 문장에서의 단어 위치와 중요성을 유지합니다.

> [Positional Encoding 이란?](https://standing-o.github.io/posts/positional-encoding/)
{: .prompt-tip }

![fig4](20240401-4.png){: width="600"}

#### 4. Multi-Head Self-Attention
- 전체 입력을 고려하여 학습하는 능력으로 언어 인코딩 능력을 크게 향상시킵니다.
- 입력 시퀀스 내 토큰 간의 관계를 분석합니다.
  - 모델은 단어 간의 문맥적 종속성을 더 잘 파악하기 위해 입력 시퀀스의 다양한 부분에 주의를 기울일 수 있습니다.
  - 해당 레이어에 저장된 Self-attention 가중치는 해당 입력 시퀀스의 각 단어가 시퀀스 내 다른 모든 단어에 대한 중요성을 반영합니다.
- Multi-Head Self-Attention 가중치 또는 헤드는 서로 독립적으로 병렬로 학습됩니다. (12~100)
  - 직관적으로 각 Self-Attention 헤드는 언어의 다른 측면을 학습합니다.
  - 각 헤드의 가중치는 무작위로 초기화됩니다.


![fig5](20240401-5.png){: width="600"}


#### 5. Fully-Connected Feed Forward Network
- 해당 레이어의 출력은 토큰화 사전의 각 토큰에 대한 확률 점수에 비례하는 Logit 벡터입니다.
- 이러한 Logit을 최종 Softmax 레이어에 전달하여, 각 단어에 대한 확률 점수로 정규화합니다.
  - 어휘 사전의 모든 단어에 대한 확률을 포함하므로, 수천 개의 점수가 있을 것입니다.
- 하나의 토큰만 나머지보다 높은 점수를 가질 것 입니다.
  - 해당 토큰이 가장 가능성이 높은 예측된 토큰입니다.
  - 이 벡터를 통해 최종 선택을 하는 방식이 다양하게 존재합니다.

![fig6](20240401-6.png){: width="600"}


&nbsp;
&nbsp;
&nbsp;


### **Transformer 유형**
- **Encoder** ㅣ Prompt를 문맥적 이해와 함께 인코딩하여 각 입력 토큰마다 하나의 벡터를 생성합니다.
- **Decoder** ㅣ 입력 토큰을 받아 새 토큰을 생성합니다.

#### 유형
- **인코더 전용 모델(Encoder Only Model)**
  - Sequence-to-sequence 모델로 작동하지만, 추가적인 수정 없이 입력 시퀀스와 출력 시퀀스가 동일한 길이입니다.
  - 아키텍처에 추가적인 레이어를 추가하여 인코더 전용 모델을 감정 분석과 같은 분류 작업을 수행하도록 학습 가능합니다. 
  - ex. BERT
- **인코더-디코더 모델(Encoder-Decoder Model)**
  - 번역과 같은 Sequence-to-sequence 작업에서 잘 수행되며, 입력 시퀀스와 출력 시퀀스의 길이가 다를 수 있습니다.
  - 모델 Scale을 늘려서 학습하여 일반적인 텍스트 생성작업을 수행할 수 있습니다. 
  - ex. BART
- **디코더 전용 모델(Decoder Only Model)**
  - Scale이 커짐에 따라 그 기능이 향상되었습니다. 대부분의 작업에 일반화 가능합니다. 
  - ex. GPT, BLOOM Jurassic, LLaMA

  
&nbsp;
&nbsp;
&nbsp;


### **Transformer를 활용한 텍스트 생성 <sup>Text Generation</sup>**

#### 번역 <sup>Translation</sup>: Sequence-to-sequence Task
1. 네트워크를 훈련하는 데 사용된 동일한 Tokenizer를 사용하여 입력 단어를 토큰화합니다.
2. 이러한 토큰은 네트워크의 인코더 부분에 입력으로 추가되고, 임베딩 레이어를 통과한 다음, Multi-Head Self-Attention 레이어로 전달됩니다.
3. Multi-Head Self-Attention 레이어의 출력은 Fully-Connected Feed Forward Network를 통해 인코더의 출력에 전달됩니다.
  - 인코더를 떠나는 데이터는 입력 시퀀스의 구조와 의미에 대한 심층적인 표현입니다.
  - 이 Representation은 디코더의 셀프 어텐션 메커니즘에 영향을 주기 위해 디코더의 중간에 삽입됩니다.
4. 디코더의 입력에는 시작 시퀀스 토큰이 추가됩니다.
  - 디코더가 다음 토큰을 예측하도록 유도합니다. 인코더로부터 제공되는 문맥적 이해를 기반으로 이 작업이 수행됩니다.
5. 디코더의 셀프 어텐션 레이어의 출력은 디코더의 피드 포워드 네트워크와 최종 Softmax 출력 레이어를 통과합니다.
  - 이 시점에서 우리는 첫 번째 토큰을 가지게 됩니다. 이러한 반복 과정을 통해 다음 토큰의 생성을 유도하기 위해 출력 토큰을 다시 입력으로 전달합니다.
  - 모델이 종료 시퀀스 토큰을 예측할 때까지 이를 계속합니다.


![fig7](20240401-7.png){: width="800"}



&nbsp;
&nbsp;
&nbsp;


## **프롬프트 엔지니어링 <sup>Prompt Engineering</sup>**
### **Prompt와 Completion**
- LLM에게 전달되는 텍스트를 **프롬프트(Prompt)** 라고 합니다.
  - 프롬프트에 사용가능한 공간 또는 메모리를 **컨텍스트 창(Context Window)** 라고 하며, 보통 수천 단어 정도의 공간을 가지고 있지만 모델마다 상이합니다.
- 모델의 출력물을 **컴플리션(Completion)** 라고 하며, 모델을 사용하여 텍스트를 생성하는 행위를 **추론(Inference)** 라고 합니다.
  - 사용자가 모델에게 질문을 한다면, 프롬프트가 모델에 전달되고 모델은 다음 단어를 예측합니다.
  - 사용자의 프롬프트에 질문이 포함되어 있으므로, 모델은 답변을 생성합니다.
  - 컴플리션은 원래 프롬프트에 포함된 텍스트를 따르고, 그 뒤에 생성된 텍스트가 포함됩니다.

![fig2](20240401-2.png){: width="600"}

&nbsp;
&nbsp;
&nbsp;


### **컨텍스트 내 학습 <sup>In-context Learning, ICL</sup>**

![fig8](20240401-8.png){: width="600"}


- 컨텍스트 창 내에서 예제 제공
- 프롬프트에 예제나 추가 데이터를 포함시켜, 요청된 작업에 대해 LLM이 더 많이 학습하도록 돕습니다.

#### Zero-shot Inference
- 프롬프트 내에 데이터를 포함하지 않으며, **큰** LLM에 유리합니다.

#### One-shot Inference
- 프롬프트 내에 한번의 예제를 제공합니다. 
  - ChatGPT-2와 같은 **작은** 모델은 작업의 세부 사항을 이해하지 못하고 프롬프트를 식별하지 못합니다.

#### Few-shot Inference
- 프롬프트 내에 여러 예제를 제공합니다.
  - 서로 다른 출력 클래스를 가진 예제를 혼합하여 모델이 수행해야 할 작업을 이해하는 데 도움이 됩니다.
  - 학습 데이터가 매우 제한적인 경우 유용합니다.


#### Prompt Engineering
- 프롬프트를 개발하고 개선하는 작업입니다.
  - 원하는 방식으로 모델이 작동하도록, 언어나 문장을 여러 번 수정해야 할 수 있습니다.
- 컨텍스트 창은 모델에 전달할 수 있는 컨텍스트 내 학습의 양에 제한이 있기 때문에 중요합니다.
  - 일반적으로 다섯 가지 예제를 포함하여도 모델의 성능이 좋지 않다면 모델을 미세 조정해야 합니다.
- **미세 조정**은 새로운 데이터를 사용하여 모델을 추가적으로 훈련시켜 원하는 작업을 수행할 수 있도록 하는 것입니다.
- 점점 더 큰 규모의 모델이 훈련되면서 모델이 여러 작업을 수행하는 능력과 그 작업을 얼마나 잘 수행하는지는 모델의 규모에 강하게 의존한다는 것이 명확해졌습니다.

&nbsp;
&nbsp;
&nbsp;

## **Generative Configuration**
- 모델이 다음 단어 생성에 대한 최종 결정을 내리는 방식입니다.
- 트랜스포머의 Softmax에서의 출력은 모델이 사용하는 전체 단어 사전에 대한 확률 분포입니다.
  - 사용자는 단어와 확률 점수를 선택할 수 있습니다.

#### Greedy Decoding
  - 다음 단어 예측의 가장 간단한 형태로, 모델은 항상 가장 높은 확률을 가진 단어를 선택합니다.
  - 짧은 생성에는 잘 작동하지만, 반복된 단어나 단어 시퀀스에 취약합니다.

#### Random Sampling
  - 일정한 변동성을 도입하는 가장 쉬운 방법입니다.
  - 무작위 샘플링에서는 항상 가장 확률이 높은 단어를 선택하는 대신, 모델은 확률 분포를 사용하여 선택을 가중치를 부여하여 무작위로 출력 단어를 선택합니다.
  - 단어가 반복될 가능성을 줄입니다.
  - 설정에 따라 출력이 너무 창의적이어서 생성이 특정 주제로 혹은 이해할 수 없는 단어로 이어질 수 있습니다.


![fig9](20240401-9.png){: width="600"}

&nbsp;
&nbsp;
&nbsp;


### **Inference 파라미터**
#### `Max new tokens`
- 완성된 토큰의 최대 수
- 모델이 생성할 토큰의 수를 제한하는 데 사용됩니다.
  - ex. 200인 예제의 경우 컴플리션 길이가 짧음
  - 모델이 종료 토큰을 예측하는 등 다른 중단 조건에 도달했습니다.


#### `Sample top K`
- 모델에게 확률이 가장 높은 상위 k개의 토큰 중에서 선택하도록 지시합니다.
- 모델이 일정한 무작위성을 가지면서도 확률이 매우 낮은 완성 단어를 선택하는 것을 방지합니다.
  - 텍스트 생성이 더 합리적으로 들리고 의미가 있는 것으로 만듭니다.
  - 상위 k개에서는 모델이 무작위로 선택할 토큰 수를 지정합니다.

  

![fig10](20240401-10.png){: width="600"}


#### `Sample top P`
- 결합된 확률이 p를 초과하지 않는 예측에 대한 무작위 샘플링을 제한합니다.
- 모델은 이러한 토큰 중에서 선택하기 위해 무작위 확률 가중치 방법을 사용합니다.
- 상위 p에서는 모델이 선택할 총 확률을 지정합니다.


![fig11](20240401-11.png){: width="600"}

#### `Temperature`
- Temperature 값은 모델의 최종 Softmax 레이어 내에서 적용되는 스케일링 요소로, 다음 토큰의 확률 분포의 모양에 영향을 줍니다.
  - 실제로 모델이 생성할 예측을 변경합니다.
- Temperature가 높을수록 (`>1`), 무작위성이 더 높아집니다. (더 창의적)
  - 모델이 더 넓고 평평한 확률을 계산합니다.
  - 이는 모델이 더 높은 수준의 무작위성과 출력의 더 큰 변동성을 가지게 하며, 시원한 온도 설정과 비교하여 결과적으로 텍스트를 생성합니다.
- Temperature가 낮을수록 (`<1`), 무작위성이 감소합니다.
  - Softmax 레이어에서 얻은 결과 확률 분포가 더 강하게 Peak되어 있습니다.
  - 모델은 무작위 샘플링을 사용하여 분포에서 선택하며, 결과적인 텍스트는 덜 무작위적이며 모델이 훈련 중에 학습한 가장 가능성 있는 단어 순서를 더 따릅니다.
- Temperature (`= 1`)
  - Softmax 함수를 수정하지 않은 확률 분포를 사용합니다.


![fig12](20240401-12.png){: width="600"}


&nbsp;
&nbsp;
&nbsp;


## **LLM Project 수행 과정**

#### 1. 사용 사례 정의
- 가능한 한 정확하고 좁은 범위로 정의합니다.
- LLMs는 다양한 작업을 수행할 수 있지만, 그 능력은 모델의 크기와 구조에 강하게 의존합니다.
- ex. Long-form Text Generation, High Degree of Capability, Specific (Entity Recognition)


#### 2. 기존 모델 선택 또는 자체 사전 훈련
- 처음부터 자체 모델을 훈련할지 또는 기존 기본 모델과 작업할지 선택합니다.


#### 3. 프롬프트 엔지니어링, 미세 조정, 인간 피드백과 조정
- 성능을 평가하고 필요에 따라 추가 훈련을 수행합니다.
- 애플리케이션 개발의 이 적응 및 조정 단계는 매우 반복적일 수 있습니다.


#### 4. 모델 강화 및 LLM 기반 애플리케이션 구축
- 배포를 위해 모델을 최적화합니다.
- 애플리케이션이 잘 작동하기위한 추가 인프라를 고려합니다.


&nbsp;
&nbsp;
&nbsp;


---------------------------

## Reference
1. [Generative AI with Large Language Models, Coursera](https://www.coursera.org/learn/generative-ai-with-llms)
