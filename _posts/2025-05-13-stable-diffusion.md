---
title: "Stable Diffusion ì›ë¦¬ë¡œ ì´í•´í•˜ê¸° (1)"
date: 2025-05-13 00:00:00 +/-TTTT
categories: [ì¸ê³µì§€ëŠ¥ | AI, AI ì´ë¡ ]
tags: [deep-learning, generative-ai, llm, neural-network, auto-encoder, vae, stable-diffusion]
math: true
toc: true
author: seoyoung
img_path: /assets/img/for_post/
pin: false
description: ğŸ¨ ìƒì„± ëª¨ë¸ì˜ ê¸°ë³¸ ê°œë…ê³¼ ë‹¤ì–‘í•œ í•™ìŠµ ë°©ë²•ì„ ê³µë¶€í•˜ê³ , ë°ì´í„° ìƒì„± ì›ë¦¬ë¥¼ ì•Œì•„ë´…ì‹œë‹¤.
---


--------------------
> **<u>KEYWORDS</u>**         
> Stable Diffusion, Stable Diffusion ì›ë¦¬, Stable Diffusion AI, Stable Diffusion Model, Stable Diffusion ìˆ˜í•™
{: .prompt-info }
--------------------

&nbsp;
&nbsp;
&nbsp;



## **ìƒì„± ëª¨ë¸ <sup>Generative Models</sup>**

### **Introduction**

- ìƒì„±ëª¨ë¸ì´ë€ ëª©í‘œ ë„ë©”ì¸ì˜ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ëª¨ë¸ì„ ëœ»í•©ë‹ˆë‹¤.
- Nê°œì˜ í•™ìŠµ ë°ì´í„° $$D = \{ \mathbf{x}^{(1)}, \dots, \mathbf{x}^{(N)} \}$$ê°€ ë¯¸ì§€ì˜ í™•ë¥ ë¶„í¬ $$p(\mathbf{x})$$ë¡œ ë¶€í„° ì„œë¡œ ë…ë¦½ì ìœ¼ë¡œ ì¶”ì¶œëœ ê²ƒì´ë¼ê³  ê°€ì •í•  ë•Œ, ìƒì„± ëª¨ë¸ì€ í™•ë¥ ë¶„í¬ $$q_{\theta}(\mathbf{x})$$ë¥¼ ê°€ì§€ë©° í•´ë‹¹ ë¶„í¬ì— ë”°ë¼ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
  - $$\mathbf{x} \sim q_{\theta}(\mathbf{x})$$
  - $$\theta$$ëŠ” ì‹ ê²½ë§ì˜ ë§¤ê°œë³€ìˆ˜ì™€ ê°™ì€ í™•ë¥ ë¶„í¬ì˜ íŠ¹ì§•ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
  - **ëª©í‘œ** ã…£ ëª©í‘œ í™•ë¥  ë¶„í¬ $$p(\mathbf{x})$$ì™€ ê°€ëŠ¥í•œ ê°€ê¹Œìš´ í™•ë¥ ë¶„í¬ $$q_{\theta}(\mathbf{x})$$ë¥¼ ê°€ì§€ëŠ” ìƒì„± ëª¨ë¸ì„ ì–»ëŠ” ê²ƒ.
    - ë‘ ë¶„í¬ê°€ ì–¼ë§ˆë‚˜ ê°€ê¹Œìš´ì§€ë¥¼ í™•ì¸í•˜ëŠ” ë‹®ìŒ ì§€í‘œë¡œ **KL-Divergence**ì™€ **Wasserstein Distance**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

- í…ìŠ¤íŠ¸ì™€ ê°™ì€ íƒ€ ë°ì´í„°ì— ëŒ€ì‘í•˜ëŠ” ì´ë¯¸ì§€ $$\mathbf{x}$$ë¥¼ ìƒì„±í•˜ëŠ” ë¬¸ì œëŠ” Joint Probability $$p(\mathbf{x}, \mathbf{c})$$ ë˜ëŠ” Conditional Probability $$p(\mathbf{x} \vert \mathbf{c})$$ë¥¼ ì´ìš©í•˜ëŠ” ìƒì„±ëª¨ë¸ë¡œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


&nbsp;
&nbsp;
&nbsp;


- ë°ì´í„° $$x \in X$$ì— ëŒ€í•œ ìƒì„±ëª¨ë¸ì˜ í™•ë¥  ë¶„í¬ $$q_{\theta}(\mathbf{x})$$ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

$$
q_\theta(\mathbf{x}) = \gamma_{\theta}(\mathbf{x})/Z(\theta), \\
Z(\theta) = \int_{\mathbf{x}' \in X} \gamma_{\theta}(\mathbf{x}' d \mathbf{x}').
$$

- - where $$\gamma_{\theta}(\mathbf{x}) \geq 0$$ is an unnormalized probability density function(PDF) and $$Z(\theta) > 0$$ is a partition function(normalization constant).
  - Partition function $$Z(\theta)$$ëŠ” ë°ì´í„° ê³µê°„ì˜ ëª¨ë“  ì •ë³´ë¥¼ ê°€ì§€ê¸° ë•Œë¬¸ì— ì´ë¥¼ í™œìš©í•˜ë©´ ë°ì´í„° ì „ì²´ì˜ ì—¬ëŸ¬ í†µê³„ëŸ‰ì„ ê³„ì‚°í•  ìˆ˜ ìˆìœ¼ë©°, $$\int q_\theta(\mathbf{x}) d \mathbf{x}= 1$$ ì¦‰ í™•ë¥ ë°€ë„ê°€ ë˜ë„ë¡ í•©ë‹ˆë‹¤.


&nbsp;
&nbsp;
&nbsp;


- **<u>Energy-based model(EBM)</u>**

  - Unnormalized PDF $$\gamma_{\theta}(\mathbf{x}) = \exp(-f_{\theta}(\mathbf{x}))$$

  - i.e.,
    $$
    q_\theta(\mathbf{x}) = \exp(-f_{\theta}(\mathbf{x}))/Z(\theta), \\
    Z(\theta) = \int_{\mathbf{x}' \in X} \exp(-f_{\theta}(\mathbf{x}')) d\mathbf{x}'.
    $$

  - ì—ë„ˆì§€ $$f_{\theta}(\mathbf{x})$$ê°€ ì‘ìœ¼ë©´, $$\mathbf{x}$$ ë°ì´í„°ëŠ” ì¶œí˜„í•˜ê¸° ì‰¬ìš´ ë°ì´í„°ë¡œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  - ì´ëŸ¬í•œ ì—ë„ˆì§€ í•¨ìˆ˜ëŠ” í™•ë¥ ë¶„í¬ë¡œì„œì˜ ì œì•½ì´ í¬ê²Œ ì—†ì–´ ëª¨ë¸ë§ì´ ììœ ë¡­ì§€ë§Œ, Partition functionì„ ê³„ì‚°í•´ì•¼ í•˜ë¯€ë¡œ ê³ ì°¨ì› ì…ë ¥ ë°ì´í„°ë¥¼ ë‹¤ë£° ê²½ìš° ì–´ë ¤ì›€ì´ ìˆìŠµë‹ˆë‹¤.

&nbsp;
&nbsp;
&nbsp;


### **Training**

#### 0. KL Divergence

- **í™•ë¥  ë¶„í¬ $$p(\mathbf{x})$$ë¡œ ë¶€í„° $$q(\mathbf{x})$$ë¡œì˜ KL Divergence**
  $$
  D_{KL}(p \| q) := \int_x p(x) \log \frac{p(x)}{q(x)} dx.
  $$

  - If $$p(x) = q(x),$$ then $$D_{KL}(p \| q) = 0$$.
  - else,  $$D_{KL}(p \| q) > 0$$.

- ë‘ í™•ë¥ ë¶„í¬ê°€ ë‹¤ë¥¼ìˆ˜ë¡ KL DivergenceëŠ” í° ì–‘ì˜ ê°’ì„ ê°€ì§‘ë‹ˆë‹¤.


&nbsp;
&nbsp;
&nbsp;


#### 1. Likelihood-based Model

- **<u>Likelihood-based Model</u>**

  - ë°ì´í„° $$\mathbf{x}$$ì˜ ìƒì„±í™•ë¥  ë˜ëŠ” ìš°ë„ $$q_\theta(\mathbf{x})$$ë¥¼ ëª…ì‹œì  í™•ë¥ ë¶„í¬ë¡œ ì •ì˜í•˜ê³ , ê·¸ ë¶„í¬ì˜ **ë¡œê·¸ ìš°ë„(Log-likelihood)ë¥¼ ìµœëŒ€í™”**í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë§¤ê°œë³€ìˆ˜ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤.

  - Nê°œì˜ í•™ìŠµ ë°ì´í„° $$D = \{ \mathbf{x}^{(1)}, \dots, \mathbf{x}^{(N)} \}$$ëŠ” ì„œë¡œ ë…ë¦½ì ìœ¼ë¡œ ì¶”ì¶œë˜ì—ˆê¸°ì—, Dì˜ ìš°ë„ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê° ë°ì´í„° ìš°ë„ì˜ ê³±ìœ¼ë¡œ ì •ì˜ë©ë‹ˆë‹¤.
    $$
    q_\theta(D) = \Pi_i q_\theta(\mathbf{x}^{(i)}).
    $$

  - ë¡œê·¸ ìš°ë„ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
    $$
    xL(\theta) = \frac{1}{N} \log q_\theta(D) = \frac{1}{N} \sum_i \log q_\theta(\mathbf{x}^{(i)}).
    $$

  - VAE, Auto-regressive Model, Energy-based Modelê³¼ ê°™ì€ ìƒì„± ëª¨ë¸ì´ Likelihood-based Model ì…ë‹ˆë‹¤.


&nbsp;
&nbsp;
&nbsp;

- **<u>Maximum Likelihood Estimation(MLE)</u>**

  - $$\theta^{*}_{ML}$$ë¥¼ í†µí•´ ë§¤ê°œë³€ìˆ˜ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤.
    $$
    \theta^{*}_{ML} := arg\max_\theta L(\theta)
    $$

  - **ex. MLE of Energy-based Model**

    - ì•„ë˜ì˜ ì²«ë²ˆì§¸ Termì„ í†µí•´ í•™ìŠµ ë°ì´í„° ìœ„ì¹˜ì˜ ì—ë„ˆì§€ë¥¼ ì¤„ì´ê³  (1), ë‘ë²ˆì§¸ Termì„ í†µí•´ ê·¸ ì™¸ ëª¨ë“  ìœ„ì¹˜ì˜ ì—ë„ˆì§€ë¥¼ ë†’ì´ëŠ” (2) Parameterë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

    $$
    L(\theta) = \frac{1}{N} \sum^N _{i=1} \log q_\theta (\mathbf{x}^{(i)})
    $$
  
    $$
    = -\frac{1}{N} \sum^N _{i=1} \big[ f_\theta(\mathbf{x}^{(i)}) \big] - \log Z(\theta)
    $$
    
    $$
    = -\frac{1}{N} \sum^N _{i=1} \big[ f_\theta(\mathbf{x}^{(i)}) \big] - \log \int_{\mathbf{x}' \in X} \exp(-f_\theta(\mathbf{x}')) d \mathbf{x}'.
    $$

    - í•˜ì§€ë§Œ, ê³ ì°¨ì› ë°ì´í„° $$\mathbf{x}$$ì— ëŒ€í•˜ì—¬ (1)ë²ˆ ì‘ì—…ì„ ìˆ˜í–‰í•œë‹¤ë©´, í•™ìŠµ ë°ì´í„° ì™¸ì—ë„ ì—ë„ˆì§€ê°€ ë‚®ì•„ì§€ëŠ” ìœ„ì¹˜ê°€ ë¬´ìˆ˜íˆ ë§ì´ ë°œìƒí•˜ê²Œ ë˜ë©°, ì´ëŠ” (2)ë²ˆ ì‘ì—… ë˜í•œ ì–´ë µê²Œ ë§Œë“­ë‹ˆë‹¤.

      - ì¦‰, ë¬´ìˆ˜íˆ ë§ì€ ìœ„ì¹˜ì— ì‹¤ì œë¡œ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ê°€ ìƒì„±ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    - í•™ìŠµ ë°ì´í„°ë‚˜ ì •ë‹µ ë°ì´í„°ì˜ ìš°ë„ë¥¼ í™œìš©í•˜ë©´ í•™ìŠµ ì§„í–‰ ì¤‘ ëª¨ë¸ì„ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

    - **Log-likelihoodì˜ ê¸°ìš¸ê¸°**ëŠ” í•™ìŠµ ë°ì´í„°ì™€ ëª¨ë¸ ë¶„í¬ ê°„ ì—ë„ˆì§€ ê¸°ìš¸ê¸° ì°¨ì´ë¡œ í‘œí˜„ë  ìˆ˜ ìˆëŠ”ë°, Energy-based Modelì€ ëª¨ë¸ ë¶„í¬ì—ì„œì˜ ê¸°ëŒ“ê°’ ê³„ì‚°ì´ ì–´ë ¤ì›Œ ì§ì ‘ ì¶”ì •í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.
      
      $$
      \frac{\partial L(\theta)}{\partial \theta} = -\frac{1}{N} \sum^N _{i=1} \big[ \frac{\partial f_\theta(\mathbf{x}^{(i)})}{\partial \theta} \big] - \frac{\partial}{\partial \theta} \log Z(\theta) \\
      $$

      $$
      = -\frac{1}{N} \sum^N _{i=1} \big[ \frac{\partial f_\theta(\mathbf{x}^{(i)})}{\partial \theta} \big] + \mathbb{E}_{\mathbf{x} \sim q_\theta(\mathbf{x})} \big[ \frac{\partial f_\theta (\mathbf{x})}{\partial \theta} \big].
      $$


      - Markov Chain Monte Carlo(MCMC) ìƒ˜í”Œë§ìœ¼ë¡œ Partition Function ê³„ì‚° ì—†ì´ ê·¼ì‚¬í•  ìˆ˜ ìˆì§€ë§Œ, ê³„ì‚° ë¹„ìš©ì´ í¬ê³  ë¶„ì‚°ì´ í¬ë‹¤ëŠ” í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤.


&nbsp;
&nbsp;
&nbsp;


- **<u>KL Divergenceì™€ MLE</u>**
  - **MLEëŠ” $$D_{KL}(p \| q)$$ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë¬¸ì œ**ì…ë‹ˆë‹¤.
  - $$p(\mathbf{x})>0$$ì¼ ë•Œ, ë¶„ëª¨ì¸ $$q(\mathbf{x})$$ê°€ ì‘ë‹¤ë©´ í° Penaltyê°€ ìƒê¸¸ ìˆ˜ ìˆì–´, ëª¨ë¸ì€ ê°€ëŠ¥í•œ ëª¨ë“  Modeë¥¼ í¬í•¨í•˜ë„ë¡ í•™ìŠµí•©ë‹ˆë‹¤.
  - í‹€ë¦° ë°ì´í„°ì— Penaltyë¥¼ ì£¼ê¸° ì–´ë µìŠµë‹ˆë‹¤.


&nbsp;
&nbsp;
&nbsp;


#### 2. Implicit Generative Model

-  ëª…ì‹œì ì¸(Explicit) í™•ë¥  ë¶„í¬ $$q_\theta(\mathbf{x}))$$ë¥¼ ì •ì˜í•˜ì§€ ì•Šê³ , ìƒ˜í”Œì„ ì§ì ‘ ìƒì„±í•˜ëŠ” ìƒì„± í•¨ìˆ˜ë§Œì„ í†µí•´ ë°ì´í„°ë¥¼ ëª¨ë¸ë§ í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.
   - í‘œë³¸ ì¶”ì¶œ ê³¼ì •ìœ¼ë¡œ í™•ë¥  ë¶„í¬ê°€ Implicití•˜ê²Œ í‘œí˜„ë˜ëŠ” ëª¨ë¸ë¡œ Likelihoodê°€ Explicití•˜ê²Œ êµ¬í•´ì§€ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤.
   - ex. ì •ê·œ ë¶„í¬ë¡œë¶€í„° ì¶”ì¶œëœ ì ì¬ë³€ìˆ˜ë¥¼ ì‹ ê²½ë§ê³¼ ê°™ì€ ê²°ì •ë¡ ì  í•¨ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ ì–»ì–´ì§„ ë¶„í¬ë¡œ í™•ë¥  ë¶„í¬ë¥¼ í‘œí˜„í•  ê²½ìš°, ì´ë¥¼ ê²°ì •ë¡ ì  í•¨ìˆ˜ì— ì˜í•œ Push-forward ë¶„í¬ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤ (GAN).
-  í•™ìŠµì´ ë¶ˆì•ˆì •í•´ì§€ê¸° ì‰¬ìš°ë©°, í•™ìŠµ ì§„í–‰ ì¤‘ ëª¨ë¸ì„ í‰ê°€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.
-  Partition Functionì„ Explicití•˜ê²Œ ê³„ì‚°í•  í•„ìš”ê°€ ì—†ì–´, ë†’ì€ í‘œí˜„ë ¥ì„ ê°€ì§€ëŠ” ëª¨ë¸ ìƒì„±ì— í™œìš©í•˜ê¸° ìš©ì´í•©ë‹ˆë‹¤.

&nbsp;
&nbsp;
&nbsp;

- **<u>KL Divergenceì™€ Implicit Generative Model</u>**

  - ì´ëŠ” **Inverse KL Divergence $$D_{KL}(q \| p)$$ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë¬¸ì œ**ì…ë‹ˆë‹¤.
  - ì•„ë˜ì˜ Jensen-Shannon Divergenceë¥¼ ìµœì†Œí™”í•˜ëŠ” ë¬¸ì œë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  $$
  D_{JS}(p \| q) = \frac{1}{2} D_{KL}(p \| \frac{1}{2}(p+q)) + \frac{1}{2} D_{KL}(q \| \frac{1}{2}(p+q)).
  $$

  - $$q(\mathbf{x})>0$$ì¼ ë•Œ, $$p(\mathbf{x})$$ê°€ ì‘ìœ¼ë©´ í° Penaltyê°€ ìƒê¸¸ ìˆ˜ ìˆì–´, ì¼ë¶€ì˜ í° Modeë¥¼ íŒŒì•…í•˜ë„ë¡ í•™ìŠµí•©ë‹ˆë‹¤.
  - Modeê°€ ì¼ë¶€ë¶„ì— ì§‘ì¤‘ë˜ê±°ë‚˜ ë†“ì¹˜ëŠ” Mode Collapseê°€ ì¼ì–´ë‚˜ê¸° ì‰½ìŠµë‹ˆë‹¤. 
    - ex. GAN


&nbsp;
&nbsp;
&nbsp;


### **Score**

- **Score**
  $$
  s(\mathbf{x}) := \nabla_{\mathbf{x}} \log p(\mathbf{x}) : \mathbb{R}^d \rightarrow \mathbb{R}^d, \\
  = \frac{\nabla_{\mathbf{x}} p(\mathbf{x})}{p(\mathbf{x})}.
  $$


  - $$\log p(\mathbf{x})$$ì—ì„œì˜ $$\mathbf{x}$$ì— ëŒ€í•œ ê¸°ìš¸ê¸°ë¥¼ ëœ»í•©ë‹ˆë‹¤.
  - $$\mathbf{x}$$ì™€ ê°™ì€ ì°¨ì›ì„ ê°€ì§€ëŠ” ë²¡í„°ì…ë‹ˆë‹¤.
  - ì„ì˜ì˜ ì…ë ¥ì— ëŒ€í•˜ì—¬ ë¯¸ë¶„ ê°€ëŠ¥í•œ í™•ë¥  ë¶„í¬ì…ë‹ˆë‹¤.
    - ì…ë ¥ ê³µê°„ì—ì„œì˜ ë²¡í„°ì¥ì„ ë‚˜íƒ€ë‚´ë©°, ê° ì ì˜ ë²¡í„°ëŠ” ë‹¹ì—°í•˜ê²Œë„ ê·¸ ìœ„ì¹˜ì—ì„œ ë¡œê·¸ ìš°ë„ê°€ ê°€ì¥ ê¸‰ê²©íˆ ì»¤ì§€ëŠ” ë°©í–¥ê³¼ ê·¸ í¬ê¸°ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

- Since $$\nabla_\mathbf{x} \log Z(\theta) = 0$$,
  $$
  \nabla_{\mathbf{x}} \log q_\theta(\mathbf{x}) = - \nabla_\mathbf{x} f_\theta(\mathbf{x}) - \nabla_\mathbf{x} \log Z(\theta) \\= - \nabla_\mathbf{x} f_\theta(\mathbf{x})
  $$

  - ScoreëŠ” ì—ë„ˆì§€ í•¨ìˆ˜ì˜ ì…ë ¥ì— ëŒ€í•œ ìŒì˜ ê¸°ìš¸ê¸°ì™€ ê°™ìŠµë‹ˆë‹¤.
  - Scoreë¥¼ ì‚¬ìš©í•˜ë©´ í˜„ì¬ ìœ„ì¹˜ì—ì„œ ì–´ëŠ ë°©í–¥ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í•˜ì—¬ì•¼ í™•ë¥ ì´ ë†’ì€ ì˜ì—­ì— ë„ë‹¬í•  ìˆ˜ ìˆëŠ”ì§€ ì•Œ ìˆ˜ ìˆì–´, ê³ ì°¨ì› ê³µê°„ì—ì„œ í™•ë¥ ì´ ë†’ì€ ì˜ì—­ì„ íš¨ìœ¨ì ìœ¼ë¡œ íƒìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


&nbsp;
&nbsp;
&nbsp;


#### Langevin Monte Carlo

- Scoreë¥¼ ì‚¬ìš©í•˜ëŠ” MCMC ë°©ë²•ì´ë©°, í•´ë‹¹ ê³¼ì •ì„ ë°˜ë³µí•˜ë©´ $$p(\mathbf{x})$$ë¡œ ë¶€í„° ìµœì¢… í‘œë³¸ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  - **í™•ë¥  ë¶„í¬ì˜ ì ìˆ˜ê°€ ê³„ì‚°ë˜ë©´, Langevin Monte Carlo ë°©ë²•ì„ í™œìš©í•˜ì—¬ í•´ë‹¹ í™•ë¥  ë¶„í¬ë¡œë¶€í„° í‘œë³¸ì„ ì¶”ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.**

- ì„ì˜ì˜ Prior Distribution $$\pi(\mathbf{x})$$ë¡œ ë¶€í„° ë°ì´í„°ë¥¼ $$\mathbf{x}_0 \sim \pi(\mathbf{x})$$ë¡œ ì¶”ì¶œí•˜ì—¬ ê° ìœ„ì¹˜ì—ì„œì˜ Scoreì— ë”°ë¼ ì „ì´í•©ë‹ˆë‹¤.

  - ì´ë•Œ, ì •ê·œë¶„í¬ë¡œë¶€í„° ì¶”ì¶œëœ ë…¸ì´ì¦ˆ $$\sqrt{2 \alpha} \mathbf{u}_{i+1}$$ë¥¼ ì¶”ê°€í•˜ë©°, í•´ë‹¹ ì´ë™ì„ Kë²ˆ ë°˜ë³µí•©ë‹ˆë‹¤.

  $$
  \mathbf{x}_{i+1} = \mathbf{x}_i + \alpha \nabla_\mathbf{x} \log p(\mathbf{x}_i) + \sqrt{2 \alpha} \mathbf{u}_{i+1}.
  $$

  - ë§Œì•½ $$\alpha \rightarrow 0 \,\, \text{and} \,\, K \rightarrow \inf$$ ì´ë©´, $$\mathbf{x}$$ëŠ” $$p(\mathbf{x})$$ë¡œ ë¶€í„°ì˜ í‘œë³¸ì— ìˆ˜ë ´í•©ë‹ˆë‹¤.



- ë°ì´í„°ëŠ” Scoreì— ë”°ë¼ ë°ì´í„°ì˜ ìš°ë„ê°€ í° ì˜ì—­ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì „ì´í•˜ì§€ë§Œ, ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ë©´ ê·¹ëŒ“ê°’ìœ¼ë¡œë¶€í„° íƒˆì¶œí•  ìˆ˜ ìˆì–´ í™•ë¥  ë¶„í¬ ì „ì²´ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  - ê³ ì°¨ì› ê³µê°„ì—ì„œ í™•ë¥ ì´ ë†’ì€ ì˜ì—­ì„ íš¨ìœ¨ì ìœ¼ë¡œ íƒìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

&nbsp;
&nbsp;
&nbsp;

#### Score Matching

- **Score-based Model(SBM)**
  - í™•ë¥ ë¶„í¬ë¥¼ ì§ì ‘ í•™ìŠµí•˜ì§€ ì•Šê³ , í™•ë¥  ë¶„í¬ì˜ Scoreë¥¼ í•™ìŠµí•˜ì—¬ í•´ë‹¹ Scoreë¥¼ í™œìš©í•˜ì—¬ ìƒì„± ëª¨ë¸ì„ êµ¬í˜„í•˜ëŠ” ëª¨ë¸ì„ ëœ»í•©ë‹ˆë‹¤.
- í™•ë¥  ë¶„í¬ì˜ ê²½ìš° ì¼ë°˜ í•¨ìˆ˜ì™€ ë‹¤ë¥´ê²Œ ê·¸ ì´í•©ì´ 1ì´ë¼ëŠ” ì œì•½ì´ ìˆì–´, ëª¨ë“  ì…ë ¥ì—ì„œì˜ Scoreë§Œ ì¼ì¹˜í•˜ë©´ ë™ì¼í•œ í™•ë¥  ë¶„í¬ì„ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **Scoreë¥¼ ì–´ë–»ê²Œ í•™ìŠµí•˜ëŠ”ì§€**ì— ëŒ€í•œ ë°©ë²•ë¡ ë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.



- **1. Explicit Score Matching(ESM)**
  - ëª¨ë¸ $$s_\theta(\mathbf{x}):\mathbb{R}^d \rightarrow \mathbb{R}^d$$ì— ëŒ€í•˜ì—¬, í•™ìŠµ ëŒ€ìƒì˜ Scoreì™€ ëª¨ë¸ ì¶œë ¥ê°„ì˜ ì œê³± ì˜¤ì°¨ê°€ ìµœì†Œí™”ë˜ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ êµ¬í•´ë³´ê² ìŠµë‹ˆë‹¤.

    - ëª©í‘œ ë¶„í¬ì¸ $$p(\mathbf{x})$$ì— ëŒ€í•œ Expectationì„ ê³„ì‚°í•œë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

    $$
    J_{ESM_p}(\theta) = \frac{1}{2} \mathbb{E}_{p(\mathbf{x})} \big[ \| \nabla_\mathbf{x} \log p(\mathbf{x}) - s_\theta(\mathbf{x}) \| ^2 \big].
    $$

  - $$J_{ESM_p}(\theta)$$ëŠ” ì§ì ‘ í•¨ìˆ˜ë¥¼ ì¶”ì •í•˜ëŠ” ë°©ë²•ì´ì§€ë§Œ, ëŒ€ë¶€ë¶„ì˜ í™•ë¥  ë¶„í¬ëŠ” Score $$\nabla_\mathbf{x} \log p(\mathbf{x})$$ë¥¼ ì•Œ ìˆ˜ ì—†ê¸°ì— ê·¸ëŒ€ë¡œ ì ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.


&nbsp;
&nbsp;
&nbsp;


- **2. Implicit Score Matching(ISM)**
  - Score $$\nabla_\mathbf{x} \log p(\mathbf{x})$$ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  í•™ìŠµ ëª©í‘œë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
    
    $$
    J_{ISM_p} (\theta) = \mathbb{E}_{p(\mathbf{x})} \big[ \frac{1}{2} \| s_\theta(\mathbf{x}) \|^2 + tr(\nabla_\mathbf{x} s_\theta(\mathbf{x})) \big], \\
    $$    

    $$
    tr(\nabla_\mathbf{x} s_\theta(\mathbf{x})) = \sum^d_{i=1} \frac{\partial s_\theta (\mathbf{x})_i}{\partial x_i}
    $$
    
    $$
    = - \sum^d _{i=1} \frac{\partial^2 f_\theta(\mathbf{x})}{\partial x^2 _i}
    $$

    - where $$s_\theta(\mathbf{x})$$ is estimated score by model.
    - ì´ëŠ” Explicit Score Matchingì„ í™œìš©í•˜ì—¬ í•™ìŠµí•œ ê²°ê³¼ì™€ ì¼ì¹˜í•˜ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤.

    - $$\mathbb{E}_{p(\mathbf{x})}$$ì—ì„œ ì‹¤ì œë¡œ $$p(\mathbf{x})$$ëŠ” ì•Œ ìˆ˜ ì—†ê¸° ë•Œë¬¸ì—, í•™ìŠµ ë°ì´í„° $$D$$ì— ëŒ€í•œ í‰ê· ê°’ìœ¼ë¡œ ê¸°ëŒ“ê°’ì„ ì¹˜í™˜í•˜ì—¬ ëª©ì í•¨ìˆ˜ë¥¼ ì¬ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
      
      $$
      J_{ISM_{discrete}}(\theta) = \frac{1}{N} \sum^N _{i=1} \big[ \frac{1}{2} \| s_\theta(\mathbf{x}^{(i)}) \|^2 + tr(\nabla_\mathbf{x} s_\theta (\mathbf{x}^{(i)})) \big].
      $$

      - ì²«ë²ˆì§¸ Termì€ í•™ìŠµ ë°ì´í„° ìœ„ì¹˜ì˜ Scoreì˜ ì ˆëŒ“ê°’ì„ ìµœì†Œí™”í•˜ëŠ” ê²ƒì´ë©°, í•™ìŠµë°ì´í„°ì˜ ìœ„ì¹˜ $$\mathbf{x}^{(i)}$$ê°€ Log Likelihood $$\log q(\mathbf{x}; \theta)$$ì˜ Critical Pointê°€ ë˜ë„ë¡ í•©ë‹ˆë‹¤.
      - ë‘ë²ˆì§¸ Termì€ ê° ì„±ë¶„ì˜ 2ì°¨ ë¯¸ë¶„ì˜ í•©ì„ ìŒìˆ˜ë¡œ í•œë‹¤ëŠ” ê²ƒì´ë©°, ì²«ë²ˆì§¸ Termì˜ Critical Pointê°€ ë˜ë„ë¡ í•˜ëŠ” ì¡°ê±´ê³¼ í•¨ê»˜ í•œë‹¤ë©´ í•™ìŠµ ë°ì´í„°ì˜ ìœ„ì¹˜ê°€ ì—ë„ˆì§€ í•¨ìˆ˜ì˜ ê·¹ê°’ì´ ë˜ë„ë¡ í•œë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.

    - **ë‹¨ì **
      - $$\mathbb{E}_{p(d)} \big[ tr(\nabla_\mathbf{x} s_\theta (\mathbf{\mathbf{x}})) \big]$$ì„ ê³„ì‚°í•˜ë ¤ë©´, $$s_\theta(\mathbf{x})$$ì˜ ê° ì„±ë¶„ë§ˆë‹¤ Error Back-propagationì„ ì ìš©í•´ì•¼ í•˜ì§€ë§Œ ì´ëŠ” ì…ë ¥ì´ ê³ ì°¨ì›ì¼ ê²½ìš° ê±°ì˜ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.
      - 2ì°¨ ë¯¸ë¶„ì´ $$-\infty$$ê°€ ë˜ëŠ” ëª¨ë¸ì€ í•™ìŠµë˜ê¸° ì‰½ê³  Over-fittingì´ ì¼ì–´ë‚˜ê¸° ì‰½ìŠµë‹ˆë‹¤.
      - ì´ ë‹¨ì ë“¤ì„ ì•„ë˜ Denoising Score Matching ê¸°ë²•ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


&nbsp;
&nbsp;
&nbsp;


- **3. Denoising Score Matching(DSM)**
  - $$\tilde{\mathbf{x}}$$ ã…£ $$\mathbf{x}$$ì— $$\epsilon \sim \mathcal{N}(0, \sigma^2 I)$$ ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•œ ë³€ìˆ˜
    
    $$
    \tilde{\mathbf{x}} = \mathbf{x} + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2 I)
    $$
    - $$\sigma$$ ã…£ ë…¸ì´ì¦ˆì˜ Scale
  - ìœ„ ê³¼ì •ì€ í‰ê· ì´ $$\mathbf{x}$$, ë¶„ì‚°ì´ $$\sigma^2 I$$ì¸ ì •ê·œë¶„í¬ë¡œë¶€í„° í‘œë³¸ì„ ì–»ëŠ” ê³¼ì •ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  - **Perturbated Distribution $$p_\sigma(\tilde{\mathbf{x}})$$**
  
    $$
    p_\sigma (\tilde{\mathbf{x}}, \mathbf{x}) = \mathcal{N}(\tilde{\mathbf{x}}; \mathbf{x}, \sigma^2 I)
    $$
  
    $$
    = \frac{1}{(2 \pi)^{d/2} \sigma^d} \exp(-\frac{1}{2 \sigma^2} \| \tilde{\mathbf{x}} - \mathbf{x}\|^2)
    $$

    $$
    p_\sigma(\tilde{\mathbf{x}}) = \int _{\mathbf{x} \in \mathbb{R}^d} p_\sigma (\tilde{\mathbf{x}} \vert \mathbf{x}) p(\mathbf{x}) d \mathbf{x}.
    $$

  - Explicit Score Matching with Perturbated Distribution
    $$
    J_{ESM_{p_\sigma}} (\theta) = \frac{1}{2} \mathbb{E}_{p_{\theta} (\tilde{\mathbf{x}})} \big[ \| \nabla_\tilde{\mathbf{x}} \log p_\sigma (\tilde{\mathbf{x}}) - \mathbf{s}_\theta (\tilde{\mathbf{x}}, \sigma) \|^2 \big]
    $$

  - Implicit Score Matching with Perturbated Distribution
    $$
    J_{ISM_{p_\sigma}} (\theta) =  \mathbb{E}_{p_{\theta} (\tilde{\mathbf{x}})} \big[ \frac{1}{2} \| \mathbf{s}_\theta (\tilde{\mathbf{x}}, \sigma) \|^2 + tr(\nabla _\tilde{\mathbf{x}} \mathbf{s}_\theta (\tilde{\mathbf{x}}, \sigma)) \big]
    $$

    - If $$\sigma > 0$$,
      $$
      J_{ESM_{p_\sigma}} (\theta) = J_{ISM_{p_\sigma}} (\theta) + C_1
      $$
    - ì´ë¥¼ í†µí•´ Perturbated Distributionì˜ Scoreë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆê³ , Over-fittingì„ ì¤„ì¼ ìˆ˜ ìˆì§€ë§Œ ê³„ì‚°ëŸ‰ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆœ ì—†ìŠµë‹ˆë‹¤.

  - Denoising Score Matchingì€ ì§ì ‘ Scoreë¥¼ ëª©í‘œë¡œ í•˜ì—¬ í•™ìŠµí•˜ëŠ” ê²ƒì´ ì•„ë‹Œ, Perturbationì´ ë°œìƒí•˜ì˜€ì„ ë•Œì˜ ì¡°ê±´ë¶€ í™•ë¥ ì— ëŒ€í•œ Scoreë¥¼ ëª©í‘œë¡œ í•™ìŠµí•©ë‹ˆë‹¤.

    $$
    J_{DSM_{p_\theta}} (\theta) = \frac{1}{2} \mathbb{E}_{p_\theta (\tilde{\mathbf{x}} \vert \mathbf{x})} \big[ \|  \nabla_{\tilde{\mathbf{x}}} \log p_\sigma (\tilde{\mathbf{x}} \vert \mathbf{x}) - \mathbf{s}_\theta(\tilde{\mathbf{x}}, \sigma)\| \big]
    $$

    - ì´ëŠ” Original Distributionê³¼ Perturbated Distributionì˜ ë™ì‹œ í™•ë¥ ë¡œ Expectationì„ ê³„ì‚°í•˜ê³  ìˆìœ¼ë©°, ê·¸ ëª©í‘œê°€ $$ \nabla_{\tilde{\mathbf{x}}} \log p_\sigma (\tilde{\mathbf{x}}) $$ (Perturbated Distributionì˜ Score)ê°€ ì•„ë‹ˆë¼ $$ \nabla_\tilde{\mathbf{x}} \log p_\sigma(\tilde{\mathbf{x}} \vert \mathbf{x})$$ (ì¡°ê±´ë¶€ í™•ë¥ ì˜ Score) ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
  
    $$
    \nabla _\tilde{\mathbf{x}} \log p_\sigma(\tilde{\mathbf{x}} \vert \mathbf{x}) = \nabla_\tilde{\mathbf{x}} \log \big( \frac{1}{(2 \pi)^{d/2} \sigma^d} \exp (-\frac{1}{2 \sigma^2} \| \tilde{\mathbf{x}} - \mathbf{x} \|^2) \big)
    $$

    $$
    = \nabla_\tilde{\mathbf{x}} \log \frac{1}{(2 \pi)^{d/2} \sigma^d} + \nabla_\tilde{\mathbf{x}} \big( -\frac{1}{2 \sigma^2} \| \tilde{\mathbf{x}} - \mathbf{x} \|^2 \big)
    $$

    $$
    = 0 - \frac{1}{\sigma^2} (\tilde{\mathbf{x}} - \mathbf{x}) = -\frac{1}{\sigma^2} \epsilon
    $$

    - Perturbated Sampleì˜ ì¡°ê±´ë¶€ í™•ë¥  ë¶„í¬ì˜ ScoreëŠ” Perturbationì„ Denoisingí•  ë°©í–¥ìœ¼ë¡œ Scalingí•œ ê°’ì´ ë©ë‹ˆë‹¤.

&nbsp;
&nbsp;
&nbsp;

  - ê²°ë¡ ì ìœ¼ë¡œ, **Denoising Score Matching**ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
  
  $$
  J_{DSM_{p_\theta}} (\theta) = \frac{1}{2} \mathbb{E}_{\epsilon \sim \mathcal{N}(0, sigma^2 I), \mathbf{x} \sim p(\mathbf{x})} \big[ \| -\frac{1}{\sigma^2} \epsilon - \mathbf{s}_\theta (\mathbf{x} + \epsilon, \sigma) \|^2 \big]
  $$
  
  - $$J_{DSM_{p_{\sigma}}} (\theta)$$ëŠ” ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•œ ë°ì´í„°ë¡œë¶€í„° ì¶”ê°€í•œ ë…¸ì´ì¦ˆë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œë¥¼ í‘¸ëŠ” ê²ƒìœ¼ë¡œ, ë°ì´í„° ë¶„í¬ì˜ Scoreë¥¼ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - Perturbationì„ ì¶”ê°€í•˜ì—¬ Overfittingì„ ë§‰ì„ ìˆ˜ ìˆìœ¼ë©° ê³„ì‚°ëŸ‰ì€ ì…ë ¥ ì°¨ì› $$d$$ì— ì„ í˜•ì ì…ë‹ˆë‹¤.



--------------
## Reference
[^ref1]: 
