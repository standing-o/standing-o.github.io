---
title: "Stable Diffusion ì›ë¦¬ë¡œ ì´í•´í•˜ê¸° (1)"
date: 2025-05-13 00:00:00 +/-TTTT
categories: [ì¸ê³µì§€ëŠ¥ | AI, AI ì´ë¡ ]
tags: [deep-learning, generative-ai, llm, neural-network, auto-encoder, vae, stable-diffusion]
math: true
toc: true
author: seoyoung
img_path: /assets/img/for_post/
pin: false
description: ğŸ¨ ìƒì„± ëª¨ë¸ì˜ ê¸°ë³¸ ê°œë…ê³¼ ë‹¤ì–‘í•œ í•™ìŠµ ë°©ë²•ì„ ê³µë¶€í•˜ê³ , ë°ì´í„° ìƒì„± ì›ë¦¬ë¥¼ ì•Œì•„ë´…ì‹œë‹¤.
---


--------------------
> **<u>KEYWORDS</u>**         
> Stable Diffusion, Stable Diffusion ì›ë¦¬, Stable Diffusion AI, Stable Diffusion Model, Stable Diffusion ìˆ˜í•™
{: .prompt-info }
--------------------

&nbsp;
&nbsp;
&nbsp;



## **ìƒì„± ëª¨ë¸ <sup>Generative Models</sup>**

### **Introduction**

- ìƒì„±ëª¨ë¸ì´ë€ ëª©í‘œ ë„ë©”ì¸ì˜ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ëª¨ë¸ì„ ëœ»í•©ë‹ˆë‹¤.
  - ì¦‰, ì‹¤ì œ ë°ì´í„°ì™€ ìœ ì‚¬í•œ ìƒˆ ë°ì´í„°ë¥¼ ë§Œë“¤ì–´ë‚´ëŠ” ê²ƒì´ ëª©ì ì…ë‹ˆë‹¤.
- Nê°œì˜ í•™ìŠµ ë°ì´í„° $$D = \{ \mathbf{x}^{(1)}, \dots, \mathbf{x}^{(N)} \}$$ê°€ ë¯¸ì§€ì˜ í™•ë¥ ë¶„í¬ $$p(\mathbf{x})$$ë¡œ ë¶€í„° ì„œë¡œ ë…ë¦½ì ìœ¼ë¡œ ì¶”ì¶œëœ ê²ƒì´ë¼ê³  ê°€ì •í•  ë•Œ, ìƒì„± ëª¨ë¸ì€ $$q_{\theta}(\mathbf{x})$$ë¼ëŠ” í™•ë¥ ë¶„í¬ë¥¼ ì •ì˜í•˜ê³ , ì´ì— ë”°ë¼ ë°ì´í„°ë¥¼ ìƒ˜í”Œë§í•©ë‹ˆë‹¤.
  - $$\mathbf{x} \sim q_{\theta}(\mathbf{x})$$
  - $$\theta$$ëŠ” ì‹ ê²½ë§ì˜ ë§¤ê°œë³€ìˆ˜ì™€ ê°™ì€, ëª¨ë¸ì˜ í™•ë¥ ë¶„í¬ë¥¼ ê²°ì •ì§“ëŠ” ìš”ì†Œë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
  - **ëª©í‘œ** ã…£ ì‹¤ì œ ë°ì´í„°ì˜ í™•ë¥  ë¶„í¬ $$p(\mathbf{x})$$ì™€ ìµœëŒ€í•œ ìœ ì‚¬í•œ ë¶„í¬ $$q_{\theta}(\mathbf{x})$$ë¥¼ ê°–ëŠ” ìƒì„± ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
    - ì´ë•Œ ë‘ ë¶„í¬ì˜ ìœ ì‚¬ë„ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ **KL-Divergence**ì™€ **Wasserstein Distance**ì™€ ê°™ì€ ì§€í‘œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

- ex. í…ìŠ¤íŠ¸ì™€ ê°™ì€ ì¡°ê±´ ì •ë³´ $$\mathbf{c}$$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ì´ì— ëŒ€ì‘í•˜ëŠ” ì´ë¯¸ì§€ $$\mathbf{x}$$ë¥¼ ìƒì„±í•˜ëŠ” ë¬¸ì œëŠ” Joint Probability $$p(\mathbf{x}, \mathbf{c})$$ ë˜ëŠ” Conditional Probability $$p(\mathbf{x} \vert \mathbf{c})$$ë¥¼ ëª¨ë¸ë§í•˜ëŠ” ìƒì„± ëª¨ë¸ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


&nbsp;
&nbsp;
&nbsp;


- ë°ì´í„° $$\mathbf{x} \in X$$ì— ëŒ€í•œ ìƒì„± ëª¨ë¸ì´ ì •ì˜í•˜ëŠ” í™•ë¥  ë¶„í¬ $$q_{\theta}(\mathbf{x})$$ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

  $$
  q_\theta(\mathbf{x}) = \gamma_{\theta}(\mathbf{x})/Z(\theta),
  $$
  
  $$
  Z(\theta) = \int_{\mathbf{x}' \in X} \gamma_{\theta}(\mathbf{x}') d \mathbf{x}'.
  $$

  - where $$\gamma_{\theta}(\mathbf{x}) \geq 0$$ is an unnormalized probability density function(PDF) and $$Z(\theta) > 0$$ is a partition function(normalization constant).
    - Partition Function $$Z(\theta)$$ëŠ” ì „ì²´ ë°ì´í„° ê³µê°„ì„ ì ë¶„í•¨ìœ¼ë¡œì¨ ê³„ì‚°ë˜ë©°, ëª¨ë¸ì˜ í™•ë¥  ë¶„í¬ê°€ ì •ê·œí™”ë˜ë„ë¡ í•©ë‹ˆë‹¤.
    - i.e., ì „ì²´ ë°ì´í„° ê³µê°„ì— ëŒ€í•œ ëª¨ë“  ì •ë³´ë¥¼ í¬í•¨í•˜ë¯€ë¡œ, ë°ì´í„° ì „ì²´ì˜ ë‹¤ì–‘í•œ í†µê³„ëŸ‰ì„ ê³„ì‚°í•  ìˆ˜ ìˆìœ¼ë©°, $$\int q_\theta(\mathbf{x}) d \mathbf{x}= 1$$ì´ ë˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.


&nbsp;
&nbsp;
&nbsp;


- **<u>Energy-based model(EBM)</u>**

  - Unnormalized PDF $$\gamma_{\theta}(\mathbf{x}) = \exp(-f_{\theta}(\mathbf{x}))$$

  - i.e.,
    $$
    q_\theta(\mathbf{x}) = \exp(-f_{\theta}(\mathbf{x}))/Z(\theta),
    $$
    
    $$
    Z(\theta) = \int_{\mathbf{x}' \in X} \exp(-f_{\theta}(\mathbf{x}')) d\mathbf{x}'.
    $$

  - ì—ë„ˆì§€ í•¨ìˆ˜ $$f_{\theta}(\mathbf{x})$$ëŠ” ë°ì´í„° í¬ì¸íŠ¸ $$\mathbf{x}$$ì˜ ì—ë„ˆì§€ë¥¼ ì •ì˜í•˜ë©°, ì´ ê°’ì´ ì‘ì„ìˆ˜ë¡ í•´ë‹¹ ë°ì´í„°ê°€ ë” ìì£¼ ë“±ì¥í•  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤ê³  ê°„ì£¼í•©ë‹ˆë‹¤.
  - ì´ëŸ¬í•œ Energy-based ëª¨ë¸ì˜ ì¥ì ì€ í™•ë¥ ë¶„í¬ë¡œì„œ ì—„ê²©í•œ ì œì•½ ì—†ì´ ììœ ë¡­ê²Œ ëª¨ë¸ë§ í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤. 
    - ê·¸ëŸ¬ë‚˜ Normalization Constant $$Z(\theta)$$ì˜ ê³„ì‚°ì´ í•„ìˆ˜ì ì´ë©°, ì´ëŠ” ê³ ì°¨ì› ê³µê°„ì—ì„œ ê³„ì‚° ë¹„ìš©ì´ í½ë‹ˆë‹¤.

&nbsp;
&nbsp;
&nbsp;


### **Training**

#### 0. KL Divergence

- **í™•ë¥  ë¶„í¬ $$p(\mathbf{x})$$ë¡œ ë¶€í„° $$q(\mathbf{x})$$ë¡œì˜ KL DivergenceëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë©ë‹ˆë‹¤.**
  $$
  D_{KL}(p \| q) := \int_x p(x) \log \frac{p(x)}{q(x)} dx.
  $$

  - If $$p(x) = q(x),$$ then $$D_{KL}(p \| q) = 0$$.
  - else,  $$D_{KL}(p \| q) > 0$$.

- KL DivergenceëŠ” ë‘ ë¶„í¬ê°€ ì„œë¡œ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ì§€ë¥¼ ì¸¡ì •í•˜ë©°, ì°¨ì´ê°€ í´ìˆ˜ë¡ ê·¸ ê°’ì€ í° ì–‘ì˜ ê°’ì„ ê°€ì§‘ë‹ˆë‹¤.


&nbsp;
&nbsp;
&nbsp;


#### 1. Likelihood-based Model

- **<u>Likelihood-based Model</u>**
  - Likelihood-based ëª¨ë¸ì€ ë°ì´í„° $$\mathbf{x}$$ì˜ Likelihood $$q_\theta(\mathbf{x})$$ë¥¼ ëª…ì‹œì (Explicit)ìœ¼ë¡œ ì •ì˜í•˜ê³ , ê·¸ í™•ë¥  ë¶„í¬ì˜ Log Likelihoodë¥¼ ìµœëŒ€í™”í•˜ì—¬ íŒŒë¼ë¯¸í„° $$\theta$$ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤.

  - Nê°œì˜ í•™ìŠµ ë°ì´í„° $$D = \{ \mathbf{x}^{(1)}, \dots, \mathbf{x}^{(N)} \}$$ëŠ” ì„œë¡œ ë…ë¦½ì ìœ¼ë¡œ ì¶”ì¶œë˜ì—ˆê¸°ì—, $$D$$ì˜ LikelihoodëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê° ë°ì´í„° Likelihoodì˜ ê³±ìœ¼ë¡œ ì •ì˜ë©ë‹ˆë‹¤.
    
    $$
    q_\theta(D) = \Pi_i q_\theta(\mathbf{x}^{(i)}).
    $$

  - Log LikelihoodëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
    
    $$
    L(\theta) = \frac{1}{N} \log q_\theta(D)
    $$
    
    $$
    = \frac{1}{N} \sum_i \log q_\theta(\mathbf{x}^{(i)}).
    $$

  - ëŒ€í‘œì ì¸ Likelihood-based ìƒì„±ëª¨ë¸ë¡œëŠ” VAE, Auto-regressive Model, Energy-based ëª¨ë¸ì´ ìˆìŠµë‹ˆë‹¤.


&nbsp;
&nbsp;
&nbsp;

- **<u>Maximum Likelihood Estimation(MLE)</u>**

  - MLEì—ì„œëŠ” ì•„ë˜ì™€ ê°™ì´ Log Likelhoodë¥¼ ìµœëŒ€í™”í•˜ëŠ” íŒŒë¼ë¯¸í„° $$\theta^{*}_{ML}$$ë¥¼ êµ¬í•©ë‹ˆë‹¤.
    $$
    \theta^{*}_{ML} := arg\max_\theta L(\theta)
    $$

  - **ex. MLE of Energy-based Model**

    - ì²«ë²ˆì§¸ Termì€ í•™ìŠµ ë°ì´í„° $$\mathbf{x}^{(i)}$$ì˜ ì—ë„ˆì§€ë¥¼ ë‚®ì¶”ëŠ” í•­ì´ë©°, ë‘ë²ˆì§¸ Termì€ ì „ì²´ ë°ì´í„° ê³µê°„ì—ì„œì˜ ì—ë„ˆì§€ í•©ì„ ë†’ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ì‘ìš©í•©ë‹ˆë‹¤.
      - í•˜ì§€ë§Œ ê³ ì°¨ì› ë°ì´í„° $$\mathbf{x}$$ì˜ ê²½ìš°, í•™ìŠµ ë°ì´í„° ì£¼ë³€ì—ì„œë§Œ ì—ë„ˆì§€ë¥¼ ë‚®ì¶”ë©´, ì‹¤ì œë¡œ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë‹¤ë¥¸ ìœ„ì¹˜ì—ì„œë„ ì—ë„ˆì§€ê°€ ë‚®ì•„ì§€ëŠ” ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤.
      - ì´ë¡œ ì¸í•´, ì „ì²´ ì—ë„ˆì§€ë¥¼ ë†’ì´ëŠ” ì‘ì—…ì´ ì–´ë ¤ì›Œì§‘ë‹ˆë‹¤.
      - i.e., í•™ìŠµ ë°ì´í„° ì™¸ì˜ ìœ„ì¹˜ì—ì„œë„ ì˜ëª»ëœ ë°ì´í„°ê°€ ìƒì„±ë  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ëª¨ë¸ì˜ í’ˆì§ˆì„ ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    $$
    L(\theta) = \frac{1}{N} \sum^N _{i=1} \log q_\theta (\mathbf{x}^{(i)})
    $$
  
    $$
    = -\frac{1}{N} \sum^N _{i=1} \big[ f_\theta(\mathbf{x}^{(i)}) \big] - \log Z(\theta)
    $$
    
    $$
    = -\frac{1}{N} \sum^N _{i=1} \big[ f_\theta(\mathbf{x}^{(i)}) \big] - \log \int_{\mathbf{x}' \in X} \exp(-f_\theta(\mathbf{x}')) d \mathbf{x}'.
    $$

    - í•™ìŠµ ì¤‘ì—ëŠ” í•™ìŠµ ë°ì´í„° í˜¹ì€ ê²€ì¦ ë°ì´í„°ì˜ Likelihoodë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    - **Log-likelihoodì˜ ê¸°ìš¸ê¸°**ëŠ” ì•„ë˜ì™€ ê°™ì´ í‘œí˜„ë˜ë©°, ì—ë„ˆì§€ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.
      - í•˜ì§€ë§Œ, ì•„ë˜ ê¸°ëŒ“ê°’ì€ ëª¨ë¸ ë¶„í¬ $$q_\theta(\mathbf{x})$$ì— ëŒ€í•œ ìƒ˜í”Œë§ì´ í•„ìš”í•˜ë©°, ì´ëŠ” ë§¤ìš° ì–´ë µê³  ê³„ì‚°ë¹„ìš©ì´ í½ë‹ˆë‹¤.
      - ì´ë¥¼ ê·¼ì‚¬í•˜ê¸° ìœ„í•´ Markov Chain Monte Carlo(MCMC) ê¸°ë°˜ì˜ ìƒ˜í”Œë§ ê¸°ë²•ì´ ì‚¬ìš©ë˜ê¸°ë„ í•˜ì§€ë§Œ, ìˆ˜ë ´ ì†ë„ê°€ ëŠë¦¬ê³  ë¶„ì‚°ì´ í° ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤.
      
      $$
      \frac{\partial L(\theta)}{\partial \theta} = -\frac{1}{N} \sum^N _{i=1} \big[ \frac{\partial f_\theta(\mathbf{x}^{(i)})}{\partial \theta} \big] - \frac{\partial}{\partial \theta} \log Z(\theta) \\
      $$

      $$
      = -\frac{1}{N} \sum^N _{i=1} \big[ \frac{\partial f_\theta(\mathbf{x}^{(i)})}{\partial \theta} \big] + \mathbb{E}_{\mathbf{x} \sim q_\theta(\mathbf{x})} \big[ \frac{\partial f_\theta (\mathbf{x})}{\partial \theta} \big].
      $$


&nbsp;
&nbsp;
&nbsp;


- **<u>KL Divergenceì™€ MLE</u>**
  - **MLEëŠ” $$D_{KL}(p \| q)$$ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë¬¸ì œ**ë¡œ í•´ì„ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  - ì´ë•Œ $$p(\mathbf{x})>0$$ì¸ ìœ„ì¹˜ì—ì„œ ë¶„ëª¨ì¸ $$q(\mathbf{x})$$ê°€ ì‘ì„ ê²½ìš°, KL Divergenceê°€ í° Penaltyë¥¼ ìœ ë°œí•˜ë¯€ë¡œ ëª¨ë¸ì€ ëª¨ë“  Modeë¥¼ í¬ê´„í•˜ë„ë¡ í•™ìŠµë©ë‹ˆë‹¤.
    - ë°˜ëŒ€ë¡œ ì˜ëª»ëœ ìƒ˜í”Œì— ëŒ€í•œ PenaltyëŠ” ìƒëŒ€ì ìœ¼ë¡œ ì‘ê¸° ë•Œë¬¸ì—, í‹€ë¦° ë°ì´í„°ë¥¼ ëª…í™•íˆ ë°°ì œí•˜ëŠ” ë°ëŠ” í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤.


&nbsp;
&nbsp;
&nbsp;


#### 2. Implicit Generative Model

-  ëª…ì‹œì ì¸(Explicit) í™•ë¥  ë¶„í¬ $$q_\theta(\mathbf{x})$$ë¥¼ ì •ì˜í•˜ì§€ ì•Šê³ , ë°ì´í„°ë¥¼ ì§ì ‘ ìƒì„±í•˜ëŠ” ìƒì„± í•¨ìˆ˜ë§Œì„ í†µí•´ ë°ì´í„°ë¥¼ ëª¨ë¸ë§ í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.
  - i.e., ìƒ˜í”Œì„ ìƒì„±í•˜ëŠ” ê³¼ì •ì€ ì¡´ì¬í•˜ì§€ë§Œ ê·¸ì— ëŒ€í•œ ëª…ì‹œì ì¸ Likelihood í•¨ìˆ˜ëŠ” ì •ì˜ë˜ì§€ ì•Šìœ¼ë©°, ëª¨ë¸ì´ ë‚˜íƒ€ë‚´ëŠ” ë¶„í¬ëŠ” ì•”ë¬µì (Implicit)ìœ¼ë¡œ í‘œí˜„ë©ë‹ˆë‹¤.
   - ex. ì •ê·œ ë¶„í¬ë¡œë¶€í„° ì¶”ì¶œëœ ì ì¬ë³€ìˆ˜ $$\mathbf{z}$$ë¥¼ ì‹ ê²½ë§ê³¼ ê°™ì€ ê²°ì •ë¡ ì  í•¨ìˆ˜ $$g_\theta(\mathbf{z})$$ì— í†µê³¼ì‹œì¼œ $$\mathbf{x}$$ë¥¼ ìƒì„±í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.
     - ì´ì™€ ê°™ì€ ë¶„í¬ë¥¼ Push-forward Distributionì´ë¼ í•˜ë©°, GANì´ ëŒ€í‘œì ì¸ ì‚¬ë¡€ì…ë‹ˆë‹¤.
- í•™ìŠµì´ ë¶ˆì•ˆì •í•´ì§€ê¸° ì‰¬ìš°ë©°, í•™ìŠµ ì¤‘ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ìˆ˜ì¹˜ì ìœ¼ë¡œ í‰ê°€í•˜ê¸° ì–´ë µë‹¤ëŠ” ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤.
  - ê·¸ëŸ¬ë‚˜ Partition Functionì„ Explicití•˜ê²Œ ê³„ì‚°í•  í•„ìš”ê°€ ì—†ì–´, ê³ ì°¨ì› ë°ì´í„°ì— ëŒ€í•´ ë”ìš± ìœ ì—°í•˜ê³  í‘œí˜„ë ¥ì´ ë†’ì€ ëª¨ë¸ì„ ì„¤ê³„í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ë„ ìˆìŠµë‹ˆë‹¤.

&nbsp;
&nbsp;
&nbsp;

- **<u>KL Divergenceì™€ Implicit Generative Model</u>**

  - Implicit ëª¨ë¸ì€ ì¼ë°˜ì ìœ¼ë¡œ **Inverse KL Divergence $$D_{KL}(q \| p)$$ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë¬¸ì œ**ë¡œ ì´í•´ë©ë‹ˆë‹¤.
  - ì´ëŠ” ì¢…ì¢… **Jensen-Shannon Divergence**ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë¬¸ì œë¡œ ëŒ€ì²´ë˜ë©°, GANì€ ì´ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ìµœì í™”í•©ë‹ˆë‹¤.

  $$
  D_{JS}(p \| q) = \frac{1}{2} D_{KL}(p \| \frac{1}{2}(p+q)) + \frac{1}{2} D_{KL}(q \| \frac{1}{2}(p+q)).
  $$

  - Inverse KLì˜ ê²½ìš° $$q(\mathbf{x})>0$$ì¸ ìœ„ì¹˜ì—ì„œ, $$p(\mathbf{x})$$ê°€ ì‘ìœ¼ë©´ í° Penaltyê°€ ë°œìƒí•˜ë¯€ë¡œ, ëª¨ë¸ì€ íŠ¹ì •í•œ Modeì— ì§‘ì¤‘í•˜ì—¬ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤.
  - ì´ë¡œ ì¸í•´ ì¼ë¶€ Modeë§Œ ë³µì›í•˜ê³  ë‹¤ë¥¸ Modeë¥¼ ë†“ì¹˜ëŠ” **Mode Collapse** í˜„ìƒì´ ë°œìƒí•˜ê¸° ì‰½ìŠµë‹ˆë‹¤.


&nbsp;
&nbsp;
&nbsp;


### **Score**

- **Score**

  $$
  s(\mathbf{x}) := \nabla_{\mathbf{x}} \log p(\mathbf{x}) : \mathbb{R}^d \rightarrow \mathbb{R}^d
  $$
  
  $$
  \nabla_{\mathbf{x}} \log p(\mathbf{x}) = \frac{\nabla_{\mathbf{x}} p(\mathbf{x})}{p(\mathbf{x})}.
  $$


  - $$\log p(\mathbf{x})$$ì—ì„œì˜ $$\mathbf{x}$$ì— ëŒ€í•œ ê¸°ìš¸ê¸°ë¥¼ ëœ»í•©ë‹ˆë‹¤.
  - $$\mathbf{x}$$ì™€ ë™ì¼í•œ ì°¨ì›ì„ ê°€ì§€ëŠ” ë²¡í„°ì…ë‹ˆë‹¤.
  - í™•ë¥  ë¶„í¬ëŠ” ì„ì˜ì˜ ì…ë ¥ì— ëŒ€í•´ ë¯¸ë¶„ ê°€ëŠ¥í•˜ë‹¤ê³  ê°€ì •ë˜ë©°,
    - ScoreëŠ” ì…ë ¥ ê³µê°„ì—ì„œì˜ ë²¡í„°ì¥ì„ í˜•ì„±í•˜ê³ , ê° ìœ„ì¹˜ì—ì„œ í•´ë‹¹ ì ì˜ Log Likelihoodê°€ ê°€ì¥ ê¸‰ê²©í•˜ê²Œ ì¦ê°€í•˜ëŠ” ë°©í–¥ê³¼ ê·¸ í¬ê¸°ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

- Since $$\nabla_\mathbf{x} \log Z(\theta) = 0$$,
  
  $$
  \nabla_{\mathbf{x}} \log q_\theta(\mathbf{x}) = - \nabla_\mathbf{x} f_\theta(\mathbf{x}) - \nabla_\mathbf{x} \log Z(\theta)
  $$
  
  $$
  = - \nabla_\mathbf{x} f_\theta(\mathbf{x})
  $$

  - i.e., ScoreëŠ” ì—ë„ˆì§€ í•¨ìˆ˜ì˜ ì…ë ¥ì— ëŒ€í•œ ìŒì˜ ê¸°ìš¸ê¸°ì™€ ê°™ìŠµë‹ˆë‹¤.
  - Scoreë¥¼ ì‚¬ìš©í•˜ë©´ í˜„ì¬ ìœ„ì¹˜ì—ì„œ ì–´ëŠ ë°©í–¥ìœ¼ë¡œ ì´ë™í•´ì•¼ í™•ë¥ ì´ ë†’ì€ ì˜ì—­ì— ë„ë‹¬í•  ìˆ˜ ìˆëŠ”ì§€ íŒŒì•…í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ê³ ì°¨ì› ê³µê°„ì—ì„œ í™•ë¥ ì´ ë†’ì€ ì˜ì—­ì„ íš¨ìœ¨ì ìœ¼ë¡œ íƒìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


&nbsp;
&nbsp;
&nbsp;


#### Langevin Monte Carlo

- Scoreë¥¼ í™œìš©í•œ MCMC ê¸°ë²•ìœ¼ë¡œ, ë°˜ë³µ ìˆ˜í–‰ ì‹œ $$p(\mathbf{x})$$ë¡œ ë¶€í„° ìµœì¢… í‘œë³¸ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  - **í™•ë¥  ë¶„í¬ì˜ Scoreê°€ ê³„ì‚°ë˜ë©´, Langevin Monte Carlo ë°©ë²•ì„ í†µí•´ í•´ë‹¹ í™•ë¥  ë¶„í¬ë¡œë¶€í„°ì˜ ìƒ˜í”Œë§ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.**

- ì„ì˜ì˜ Prior Distribution $$\pi(\mathbf{x})$$ë¡œ ë¶€í„° ë°ì´í„° $$\mathbf{x}_0 \sim \pi(\mathbf{x})$$ë¥¼ ì¶”ì¶œí•˜ì—¬ ê° ìœ„ì¹˜ì—ì„œì˜ Scoreì— ë”°ë¼ ì „ì´í•©ë‹ˆë‹¤.

  - ì´ë•Œ, ì •ê·œë¶„í¬ë¡œë¶€í„° ì¶”ì¶œëœ ë…¸ì´ì¦ˆ $$\sqrt{2 \alpha} \mathbf{u}_{i+1}$$ë¥¼ ì¶”ê°€í•˜ë©°, ì´ ê³¼ì •ì„ $$K$$ë²ˆ ë°˜ë³µí•©ë‹ˆë‹¤.

  $$
  \mathbf{x}_{i+1} = \mathbf{x}_i + \alpha \nabla_\mathbf{x} \log p(\mathbf{x}_i) + \sqrt{2 \alpha} \mathbf{u}_{i+1}.
  $$

  - ë§Œì•½ $$\alpha \rightarrow 0 \,\, \text{and} \,\, K \rightarrow \inf$$ ì´ë©´, $$\mathbf{x}$$ëŠ” $$p(\mathbf{x})$$ë¡œ ë¶€í„°ì˜ ìƒ˜í”Œì— ìˆ˜ë ´í•©ë‹ˆë‹¤.



- Scoreì— ë”°ë¼ Likelihoodê°€ ë†’ì€ ë°©í–¥ìœ¼ë¡œ ë°ì´í„°ê°€ ì „ì´ë˜ë©°, ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ë©´ Local Extremumì— ê°‡íˆëŠ” í˜„ìƒì„ ë°©ì§€í•˜ì—¬ ë¶„í¬ ì „ë°˜ì„ íƒìƒ‰í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.
  - ê³ ì°¨ì› ê³µê°„ì—ì„œ í™•ë¥ ì´ ë†’ì€ ì˜ì—­ì„ íš¨ìœ¨ì ìœ¼ë¡œ íƒìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

&nbsp;
&nbsp;
&nbsp;


- **Score-based Model(SBM)**
  - í™•ë¥  ë¶„í¬ ìì²´ë¥¼ í•™ìŠµí•˜ì§€ ì•Šê³ , í•´ë‹¹ ë¶„í¬ì˜ Scoreë¥¼ í•™ìŠµí•˜ì—¬ ìƒì„± ëª¨ë¸ì„ êµ¬í˜„í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.
- í™•ë¥  ë¶„í¬ì˜ ê²½ìš° ì¼ë°˜ í•¨ìˆ˜ì™€ ë‹¤ë¥´ê²Œ ê·¸ ì´í•©ì´ 1ì´ë¼ëŠ” ì œì•½ì´ ìˆì–´, ëª¨ë“  ì…ë ¥ì—ì„œì˜ Scoreë§Œ ì¼ì¹˜í•˜ë©´ ë™ì¼í•œ í™•ë¥  ë¶„í¬ë¡œ ê°„ì£¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **Scoreë¥¼ ì–´ë–»ê²Œ í•™ìŠµí•  ê²ƒì¸ê°€**ì— ëŒ€í•œ ë°©ë²•ë¡ ë“¤ì€ ì•„ë˜ì™€ ê°™ì´ ì„¸ ê°€ì§€ë¡œ êµ¬ë¶„ë©ë‹ˆë‹¤.

&nbsp;
&nbsp;
&nbsp;

#### Score Matching 1 ã…£ Explicit Score Matching(ESM)
- ëª¨ë¸ $$s_\theta(\mathbf{x}):\mathbb{R}^d \rightarrow \mathbb{R}^d$$ì— ëŒ€í•˜ì—¬, ì‹¤ì œ Scoreì™€ ëª¨ë¸ ì¶œë ¥ ê°„ì˜ ì œê³± ì˜¤ì°¨ë¥¼ ìµœì†Œí™”ë˜ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.

  - ëª©í‘œ ë¶„í¬ì¸ $$p(\mathbf{x})$$ì— ëŒ€í•œ Expectationì„ ê³„ì‚°í•œë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

  $$
  J_{ESM_p}(\theta) = \frac{1}{2} \mathbb{E}_{p(\mathbf{x})} \big[ \| \nabla_\mathbf{x} \log p(\mathbf{x}) - s_\theta(\mathbf{x}) \| ^2 \big].
  $$

- ê·¸ëŸ¬ë‚˜ ëŒ€ë¶€ë¶„ì˜ ê²½ìš° $$\nabla_\mathbf{x} \log p(\mathbf{x})$$ë¥¼ ì•Œ ìˆ˜ ì—†ê¸° ë–„ë¬¸ì—, ìœ„ ëª©ì  í•¨ìˆ˜ëŠ” ì§ì ‘ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.


&nbsp;
&nbsp;
&nbsp;


#### Score Matching 2 ã…£ Implicit Score Matching(ISM)
- Score $$\nabla_\mathbf{x} \log p(\mathbf{x})$$ë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì§€ ì•Šê³ ë„ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ëŒ€ì²´ ëª©ì  í•¨ìˆ˜ì…ë‹ˆë‹¤.
    
  $$
  J_{ISM_p} (\theta) = \mathbb{E}_{p(\mathbf{x})} \big[ \frac{1}{2} \| s_\theta(\mathbf{x}) \|^2 + tr(\nabla_\mathbf{x} s_\theta(\mathbf{x})) \big], \\
  $$    

  $$
  tr(\nabla_\mathbf{x} s_\theta(\mathbf{x})) = \sum^d_{i=1} \frac{\partial s_\theta (\mathbf{x})_i}{\partial x_i}
  $$
    
  $$
  = - \sum^d _{i=1} \frac{\partial^2 f_\theta(\mathbf{x})}{\partial x^2 _i},
  $$

  - where $$s_\theta(\mathbf{x})$$ is estimated score by model.
  - ì´ëŠ” Explicit Score Matchingê³¼ ë™ì¼í•œ ê²°ê³¼ë¥¼ ë„ì¶œí•˜ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤.

- $$p(\mathbf{x})$$ê°€ ëª…ì‹œì ìœ¼ë¡œ ì£¼ì–´ì§€ì§€ ì•Šìœ¼ë¯€ë¡œ, ì´ë¥¼ í•™ìŠµ ë°ì´í„° ì§‘í•© $$D$$ë¥¼ ì´ìš©í•œ í‰ê· ìœ¼ë¡œ ê·¼ì‚¬í•˜ì—¬ ì•„ë˜ì™€ ê°™ì´ ì¬ì •ì˜í•©ë‹ˆë‹¤.
      
  $$
  J_{ISM_{discrete}}(\theta) = \frac{1}{N} \sum^N _{i=1} \big[ \frac{1}{2} \| s_\theta(\mathbf{x}^{(i)}) \|^2 + tr(\nabla_\mathbf{x} s_\theta (\mathbf{x}^{(i)})) \big].
  $$

  - ì²«ë²ˆì§¸ Termì€ í•™ìŠµ ë°ì´í„° ìœ„ì¹˜ì˜ Scoreì˜ í¬ê¸°ë¥¼ ìµœì†Œí™”í•˜ì—¬, í•´ë‹¹ ì§€ì ë“¤ì´ $$\log q(\mathbf{x}; \theta)$$ì˜ Critical Pointê°€ ë˜ë„ë¡ í•©ë‹ˆë‹¤ (1).
  - ë‘ë²ˆì§¸ Termì€ ê° ì„±ë¶„ì˜ 2ì°¨ ë¯¸ë¶„ì˜ í•©ì´ ìŒìˆ˜ë¡œ ìœ ì§€ë˜ë„ë¡ ìœ ë„í•˜ë©°, ì´ëŠ” (1) ì¡°ê±´ê³¼ í•¨ê»˜ ì—ë„ˆì§€ í•¨ìˆ˜ì˜ Extremumì— í•´ë‹¹ ìœ„ì¹˜ê°€ ì˜¤ë„ë¡ ë§Œë“­ë‹ˆë‹¤.

- **ë‹¨ì **
  - $$s_\theta(\mathbf{x})$$ì˜ ê° ì„±ë¶„ì— ëŒ€í•´ 2ì°¨ ë¯¸ë¶„ì´ ìš”êµ¬ë˜ë¯€ë¡œ, ì…ë ¥ì´ ê³ ì°¨ì›ì¼ ê²½ìš° ê³„ì‚°ëŸ‰ì´ í½ë‹ˆë‹¤.
  - 2ì°¨ ë¯¸ë¶„ì´ $$-\infty$$ê°€ ë˜ëŠ” ëª¨ë¸ì€ í•™ìŠµë˜ê¸° ì‰½ê³  Over-fittingì´ ì¼ì–´ë‚˜ê¸° ì‰½ìŠµë‹ˆë‹¤.
  - ì´ ë‹¨ì ë“¤ì„ ì•„ë˜ **Denoising Score Matching** ê¸°ë²•ìœ¼ë¡œ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


&nbsp;
&nbsp;
&nbsp;


#### Score Matching 3 ã…£ Denoising Score Matching(DSM)
- ì›ë³¸ ë°ì´í„° $$\mathbf{x}$$ì— ë…¸ì´ì¦ˆ $$\epsilon \sim \mathcal{N}(0, \sigma^2 I)$$ë¥¼ ì¶”ê°€í•˜ì—¬ Perturbed Sample $$\tilde{\mathbf{x}}$$ì„ ì•„ë˜ì™€ ê°™ì´ êµ¬ì„±í•©ë‹ˆë‹¤.
    
  $$
  \tilde{\mathbf{x}} = \mathbf{x} + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2 I)
  $$
  - $$\sigma$$ ã…£ ë…¸ì´ì¦ˆì˜ Scale
- ì´ëŠ” í‰ê· ì´ $$\mathbf{x}$$, ë¶„ì‚°ì´ $$\sigma^2 I$$ì¸ ì •ê·œë¶„í¬ì—ì„œ ìƒ˜í”Œì„ ìƒì„±í•˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.

- **Perturbed Distribution**
  
  $$
  p_\sigma (\tilde{\mathbf{x}}, \mathbf{x}) = \mathcal{N}(\tilde{\mathbf{x}}; \mathbf{x}, \sigma^2 I)
  $$
  
  $$
  = \frac{1}{(2 \pi)^{d/2} \sigma^d} \exp(-\frac{1}{2 \sigma^2} \| \tilde{\mathbf{x}} - \mathbf{x}\|^2)
  $$

  $$
  p_\sigma(\tilde{\mathbf{x}}) = \int _{\mathbf{x} \in \mathbb{R}^d} p_\sigma (\tilde{\mathbf{x}} \vert \mathbf{x}) p(\mathbf{x}) d \mathbf{x}.
  $$

- Perturbated Distributionì— ëŒ€í•´ Explicit ë° Implicit Score Matchingì„ ì ìš©í•  ìˆ˜ ìˆìœ¼ë©°,

  $$
  J_{ESM_{p_\sigma}} (\theta) = \frac{1}{2} \mathbb{E}_{p_{\theta} (\tilde{\mathbf{x}})} \big[ \| \nabla_\tilde{\mathbf{x}} \log p_\sigma (\tilde{\mathbf{x}}) - \mathbf{s}_\theta (\tilde{\mathbf{x}}, \sigma) \|^2 \big].
  $$

  $$
  J_{ISM_{p_\sigma}} (\theta) =  \mathbb{E}_{p_{\theta} (\tilde{\mathbf{x}})} \big[ \frac{1}{2} \| \mathbf{s}_\theta (\tilde{\mathbf{x}}, \sigma) \|^2 + tr(\nabla _\tilde{\mathbf{x}} \mathbf{s}_\theta (\tilde{\mathbf{x}}, \sigma)) \big].
  $$

  - If $$\sigma > 0$$,
    $$
    J_{ESM_{p_\sigma}} (\theta) = J_{ISM_{p_\sigma}} (\theta) + C_1
    $$
  - ì´ë¥¼ í†µí•´ Perturbed Distributionì˜ Scoreë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆê³ , Over-fittingì„ ì¤„ì¼ ìˆ˜ ìˆì§€ë§Œ ê³„ì‚°ëŸ‰ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆœ ì—†ìŠµë‹ˆë‹¤.

- ê³„ì‚°ëŸ‰ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, Denoising Score Matching ê¸°ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    
  $$
  J_{DSM_{p_\theta}} (\theta) = \frac{1}{2} \mathbb{E}_{p_\theta (\tilde{\mathbf{x}} \vert \mathbf{x})} \big[ \|  \nabla_{\tilde{\mathbf{x}}} \log p_\sigma (\tilde{\mathbf{x}} \vert \mathbf{x}) - \mathbf{s}_\theta(\tilde{\mathbf{x}}, \sigma)\| \big]
  $$
    
  - ì—¬ê¸°ì„œ í•™ìŠµì˜ ëª©ì ì€ $$\nabla_{\tilde{\mathbf{x}}} \log p_\sigma (\tilde{\mathbf{x}} \vert \mathbf{x})$$ë¥¼ ëª¨ë¸ì´ ì˜ ê·¼ì‚¬í•˜ë„ë¡ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
    - ì¦‰, Perturbed Sampleì˜ ì¡°ê±´ë¶€ í™•ë¥  ë¶„í¬ì˜ ScoreëŠ” ì›ë³¸ ë°ì´í„° ë°©í–¥ìœ¼ë¡œì˜ Denoising ë°©í–¥ì´ë©°, ê·¸ í¬ê¸°ëŠ” ë…¸ì´ì¦ˆ ìˆ˜ì¤€ì— ë°˜ë¹„ë¡€í•©ë‹ˆë‹¤.

  $$
  \nabla _\tilde{\mathbf{x}} \log p_\sigma(\tilde{\mathbf{x}} \vert \mathbf{x}) = \nabla_\tilde{\mathbf{x}} \log \big( \frac{1}{(2 \pi)^{d/2} \sigma^d} \exp (-\frac{1}{2 \sigma^2} \| \tilde{\mathbf{x}} - \mathbf{x} \|^2) \big)
  $$

  $$
  = \nabla_\tilde{\mathbf{x}} \log \frac{1}{(2 \pi)^{d/2} \sigma^d} + \nabla_\tilde{\mathbf{x}} \big( -\frac{1}{2 \sigma^2} \| \tilde{\mathbf{x}} - \mathbf{x} \|^2 \big)
  $$

  $$
  = 0 - \frac{1}{\sigma^2} (\tilde{\mathbf{x}} - \mathbf{x}) = -\frac{1}{\sigma^2} \epsilon
  $$

  - Perturbated Sampleì˜ ì¡°ê±´ë¶€ í™•ë¥  ë¶„í¬ì˜ ScoreëŠ” Perturbationì„ Denoisingí•  ë°©í–¥ìœ¼ë¡œ Scalingí•œ ê°’ì´ ë©ë‹ˆë‹¤.

&nbsp;
&nbsp;
&nbsp;

- ìµœì¢…ì ìœ¼ë¡œ, **Denoising Score Matching**ì˜ í•™ìŠµ ëª©ì  í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
  
  $$
  J_{DSM_{p_\theta}} (\theta) = \frac{1}{2} \mathbb{E}_{\epsilon \sim \mathcal{N}(0, \sigma^2 I), \mathbf{x} \sim p(\mathbf{x})} \big[ \| -\frac{1}{\sigma^2} \epsilon - \mathbf{s}_\theta (\mathbf{x} + \epsilon, \sigma) \|^2 \big].
  $$
  
  - $$J_{DSM_{p_{\sigma}}} (\theta)$$ëŠ” ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•œ ë°ì´í„°ë¡œë¶€í„° ì¶”ê°€í•œ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ëŠ” ë°©í–¥ì„ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµí•˜ëŠ” ê²ƒì´ë©°, ë°ì´í„° ë¶„í¬ì˜ Scoreë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì¶”ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - Perturbationì„ í†µí•´ Overfittingì„ ë°©ì§€í•˜ê³ , ê³„ì‚° ë¹„ìš©ì€ ì…ë ¥ ì°¨ì›ì— ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€í•©ë‹ˆë‹¤.

&nbsp;
&nbsp;
&nbsp;


--------------
## References
1. ì˜¤ì¹´ë…¸í•˜ë¼ ë‹¤ì´ìŠ¤ì¼€. (2024). í™•ì‚° ëª¨ë¸ì˜ ìˆ˜í•™ (ì†ë¯¼ê·œ ì˜®ê¹€).

2. Vincent, Pascal. "A connection between score matching and denoising autoencoders." Neural computation 23.7 (2011): 1661-1674.

3. Kingma, Durk P., and Yann Cun. "Regularized estimation of image statistics by score matching." Advances in neural information processing systems 23 (2010).
