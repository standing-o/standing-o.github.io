---
title: "[Dacon] ìŒì„± ë¶„ë¥˜ ê²½ì§„ëŒ€íšŒ"
date: 2022-06-23 17:00:00 +/-TTTT
categories: [Competition, Dacon]
tags: [ë°ì´ì½˜, dacon, ìŒì„±ë¶„ë¥˜, speech, classification, data-augmentation, feature-extraction]
---

------------------

- ë³¸ í¬ìŠ¤íŒ…ì€ ìŒì„± ë°ì´í„°ì— ëŒ€í•œ data augmentationê³¼ feature extraction ë“±ì˜ ë‚´ìš©ì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.  
- ì½”ë“œì‹¤í–‰ì€ Google Colabì˜ CPU, Standard RAM í™˜ê²½ì—ì„œ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.  
â” [ë°ì´ì½˜ì—ì„œ ì½ê¸°](https://dacon.io/competitions/official/235905/codeshare/5209)

  

## **0. Import Packages**


```python
from google.colab import drive
drive.mount('/content/drive')
```

<pre>
Mounted at /content/drive
</pre>
```python
import numpy as np
import pandas as pd
import random as rn
import os

from scipy.io import wavfile
import librosa

import matplotlib.pyplot as plt
import seaborn as sns
import IPython.display as ipd
import librosa.display

import warnings
warnings.filterwarnings("ignore")
%matplotlib inline
```


```python
# reproducibility

def all_seed(seed_num):
    np.random.seed(seed_num)
    rn.seed(seed_num)
    os.environ['PYTHONHASHSEED']=str(seed_num)
    # tf.random.set_seed(seed_num)

seed_num = 42
all_seed(seed_num)
```

## **1. Load and explore dataset**



```python
train = pd.read_csv('/content/drive/MyDrive/Speech_classification/train.csv')
test = pd.read_csv('/content/drive/MyDrive/Speech_classification/test.csv')
```

- ğŸ“ í•œ ìŒì„±ì˜ waveplotì„ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.



```python
a_filename = '/content/drive/MyDrive/Speech_classification/dataset/train/001.wav'
samples, sample_rate = librosa.load(a_filename)

plt.figure(figsize=(10, 7))

# plt.plot(np.linspace(0, sample_rate/len(samples), len(samples)), samples)
librosa.display.waveplot(samples, sr=40000)

plt.xlabel('time', fontsize = 14)
plt.ylabel('amplitude', fontsize = 14)
plt.title('001.wav | Length : ' + str(len(samples)))

plt.show()
```

![waveplot](/assets/img/for_post/20220623-1.png)

```python
print(sample_rate)
print(samples)
```

<pre>
22050
[0.00013066 0.00016804 0.00014106 ... 0.00017342 0.00017514 0.        ]
</pre>

ğŸ“ í•œ ìŒì„±ì˜ spectrogramì„ ìƒì„±í•˜ê² ìŠµë‹ˆë‹¤.  
â†ª Short term Fourier transform (STFT)ì˜ magnitudeë¥¼ db ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜í•˜ì—¬ spectrogramì„ ìƒì„±í•©ë‹ˆë‹¤.



```python
samples, sample_rate = librosa.load(a_filename)
X = librosa.stft(samples)  # data -> short term FT
Xdb = librosa.amplitude_to_db(abs(X))

plt.figure(figsize=(12, 3))
plt.title('001.wav spectrogram | Length : ' + str(len(samples)))
librosa.display.specshow(Xdb, sr = sample_rate, x_axis='time', y_axis='hz')   
plt.colorbar()
plt.show()
```

![spectrogram](/assets/img/for_post/20220623-2.png)

ğŸ“ `train.csv`ì—ëŠ” `train` í´ë”ì˜ ìŒì„± íŒŒì¼ ì´ë¦„ê³¼ ë¼ë²¨ ì»¬ëŸ¼ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. `label` ì»¬ëŸ¼ì€ 0~9 ì •ìˆ˜ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.




```python
train.head()
```

<pre>
  file_name  label
0   001.wav      9
1   002.wav      0
2   004.wav      1
3   005.wav      8
4   006.wav      0
</pre>

```python
print(train['label'].unique())
```

<pre>
[9 0 1 8 7 4 5 2 6 3]
</pre>

ğŸ“ ë°ì´í„°ê°€ í´ë˜ìŠ¤ ê· í˜•ì„ ì´ë£¨ê³  ìˆìŠµë‹ˆë‹¤.



```python
plt.figure(figsize=(12, 8))
sns.countplot(train['label'])

plt.title("The number of recordings for each label")
plt.ylabel("Count", fontsize = 14)
plt.xlabel("Label", fontsize = 14)
plt.show()
```

![count](/assets/img/for_post/20220623-3.png)

```python
file_name = train['file_name']
train_path = '/content/drive/MyDrive/Speech_classification/dataset/train/'
```

ğŸ“ ë°ì´í„°ë“¤ì˜ ê¸¸ì´ê°€ ëª¨ë‘ ë‹¤ë¦…ë‹ˆë‹¤.



```python
all_shape = []
for f in file_name:
  data, sample_rate = librosa.load(train_path + f, sr = 20000)
  all_shape.append(data.shape)
```


```python
print(all_shape[:5])
print("Max :", np.max(all_shape, axis = 0))
print("Min :", np.min(all_shape, axis = 0))
```

<pre>
[(12740,), (13126,), (12910,), (9753,), (17572,)]
Max : [19466]
Min : [7139]
</pre>

## **2. Data augmentation**

ğŸ“ ì›ë˜ì˜ ìŒì„±ë°ì´í„°ì— ìƒˆë¡œìš´ perturbation ë“¤ì„ ì¶”ê°€í•˜ì—¬ ìƒˆë¡œìš´ ìŒì„±ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. (ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ í–¥ìƒì„ ìœ„í•¨)  
â†ª Noise ì¶”ê°€, time stretching, pitch ë³€í™˜




```python
# noise ì¶”ê°€
def noise(sample):
    noise_amp = 0.01*np.random.uniform()*np.amax(sample)
    sample = sample + noise_amp*np.random.normal(size = sample.shape[0])
    return sample

# time stretching
def stretch(sample, rate = 0.8):
    stretch_sample = librosa.effects.time_stretch(sample, rate)
    return stretch_sample

# pitch ë³€í™˜
def pitch(sample, sampling_rate, pitch_factor = 0.8):
    pitch_sample = librosa.effects.pitch_shift(sample, sampling_rate, pitch_factor)
    return pitch_sample
```

## **3. Feature Extraction**

ğŸ“ ëª¨ë¸ë§ì— ì‚¬ìš©í•˜ë©´ ë„ì›€ì´ ë ë§Œí•œ ëª‡ê°€ì§€ feature extraction ë°©ë²•ì„ ì†Œê°œí•˜ê² ìŠµë‹ˆë‹¤.


#### **1. Zero Crossing Rate (ZCR)**

â†ª íŠ¹ì • í”„ë ˆì„ì´ ì§€ì† ê¸°ê°„ ë™ì•ˆì˜ ì‹ í˜¸ì˜ ë¶€í˜¸(sign) ë³€í™”ìœ¨ i.e. ì‹ í˜¸ì˜ ë¶€í˜¸ê°€ ë°”ë€ŒëŠ” ë¹„ìœ¨

#### **2. Chroma_shift**

â†ª Waveform ë˜ëŠ” power spectrogramìœ¼ë¡œ ìƒì„±í•œ chromagram. 

#### **3. Mel spectrum**

â†ª ì˜¤ë””ì˜¤ ì‹ í˜¸(time domain)ì— Fast Fourier Transform (FFT) -> Spectrum (frequency domain)  

â†ª Spectrum + í•„í„°ë§ (Mel filter bank) -> Mel spectrum  

#### **4. MFCC (Mel-Frequency Cepstral Coefficient)**

â†ª Mel spectrumì—ì„œ Cepstral ë¶„ì„ì„ í†µí•´ ê³ ìœ í•œ íŠ¹ì„±ì„ ì¶”ì¶œí•¨

#### **5. RMS (Root Mean Square)**

â†ª ì˜¤ë””ì˜¤ í‰ê·  ìŒëŸ‰ ì¸¡ì •




```python
def extract_features(sample):
    # ZCR
    result = np.array([])
    zcr = np.mean(librosa.feature.zero_crossing_rate(y = sample).T, axis=0)
    result=np.hstack((result, zcr)) 

    # Chroma_stft
    stft = np.abs(librosa.stft(sample))
    chroma_stft = np.mean(librosa.feature.chroma_stft(S = stft, sr = sample_rate).T, axis=0)
    result = np.hstack((result, chroma_stft))

    # MelSpectogram
    mel = np.mean(librosa.feature.melspectrogram(y = sample, sr = sample_rate).T, axis=0)
    result = np.hstack((result, mel)) 

    # MFCC
    mfcc = np.mean(librosa.feature.mfcc(y = sample, sr = sample_rate).T, axis=0)
    result = np.hstack((result, mfcc)) 

    # Root Mean Square Value
    rms = np.mean(librosa.feature.rms(y = sample).T, axis=0)
    result = np.hstack((result, rms)) 

    return result
```

ğŸ“ Noise ì¶”ê°€, time stretching, pitching ë°©ë²•ë“¤ì„ í†µí•´ ìŒì„± ë°ì´í„° í•˜ë‚˜ ë‹¹ (1, 162) í¬ê¸°ì˜ featureë¥¼ (3, 162) ë¡œ ì¦ê°•í•©ë‹ˆë‹¤.



```python
def get_features(path):

    sample, sample_rate = librosa.load(path)
    
    # without augmentation
    res1 = extract_features(sample)
    result = np.array(res1)
    
    # sample with noise
    noise_sample = noise(sample)
    res2 = extract_features(noise_sample)
    result = np.vstack((result, res2)) 
    
    # sample with stretching and pitching
    str_sample = stretch(sample)
    sample_stretch_pitch = pitch(str_sample, sample_rate)
    res3 = extract_features(sample_stretch_pitch)
    result = np.vstack((result, res3)) 
    
    return result
```


```python
labels = train['label']
x, y = [], []
for f, label in zip(file_name, labels):
    feature = get_features(train_path + f)
    for fe in feature:
        x.append(fe)
        y.append(label)
```


```python
X = np.array(x)
Y = np.array(y)

print("Shape of X:", np.shape(X))
print("Shape of Y:", np.shape(Y))
```

<pre>
Shape of X: (1200, 162)
Shape of Y: (1200,)
</pre>

#### **Reference**
```
[1] Speech Emotion Recognition by SHIVAM BURNWAL, https://www.kaggle.com/code/shivamburnwal/speech-emotion-recognition
```
------------------
ì½ì–´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤ :)  
ë„ì›€ì´ ëê¸¸ ë°”ëë‹ˆë‹¤ ğŸ‘ğŸ‘

