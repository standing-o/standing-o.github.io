---
title: "ViT ì‚´íŽ´ë³´ê¸° | Vision Transformer"
date: 2023-02-09 13:00:00 +/-TTTT
categories: [ì¸ê³µì§€ëŠ¥ | AI, ì»´í“¨í„° ë¹„ì „ | Computer Vision]
tags: [vision-transformer, transformer, attention, positional-encoding]
math: true
author: seoyoung
img_path: /assets/img/for_post/
description: ðŸ“º Vision Transformer(ViT) ì˜ ê¸°ë³¸ ì›ë¦¬ì™€ êµ¬ì¡°, ìˆ˜ì‹ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.
---


------------------------

> **<u>KEYWORDS</u>**        
> Vision Transformer (ViT), ViTëž€, ViT Paper, ViT ëª¨ë¸, ViT ì„¤ëª…, ViT model, ViT êµ¬ì¡°, ViT Architecture, ViT ë…¼ë¬¸ë¦¬ë·°, BEIT, CCT, CVT, DeiT, MobileViT, PvT, Swin Transformer, T2T-VIT
{: .prompt-info }

------------------------

&nbsp;
&nbsp;
&nbsp;

`Original Paper Review` 
| [An image is worth 16x16 words: Transformers for image recognition at scale](https://github.com/standing-o/Machine_Learning_Paper_Review/issues/15)

&nbsp;
&nbsp;
&nbsp;

## **Overview**
![VIT](20230209-1.png)
- Vision Transformer(ViT)ëŠ” Transformer êµ¬ì¡°ë¥¼ ì´ë¯¸ì§€ ë°ì´í„° ì²˜ë¦¬ì— ì§ì ‘ ì ìš©í•œ ëª¨ë¸ìž…ë‹ˆë‹¤.
  - Image Patch Sequenceë¥¼ Inputìœ¼ë¡œ í•˜ì—¬ ê¸°ì¡´ì˜ Transformer êµ¬ì¡°ë¥¼ ê±°ì˜ ê·¸ëŒ€ë¡œ Vision Taskì— í™œìš©í•©ë‹ˆë‹¤.
- CNNì— ë¹„í•´ Inductive BiasëŠ” ë¶€ì¡±í•©ë‹ˆë‹¤.     
  âž” CNNì´ ê°€ì§€ëŠ” Translation Equivariance ë° Locality íŠ¹ì„±ì„ ViTëŠ” ê°–ì¶”ê³  ìžˆì§„ ì•ŠìŠµë‹ˆë‹¤.
    - ì´ëŠ” ViTê°€ ë°ì´í„° ì „ì²´ë¥¼ ê³ ë ¤í•˜ì—¬ Attentioní•  ìœ„ì¹˜ë¥¼ ì •í•˜ê¸° ë–„ë¬¸ìž…ë‹ˆë‹¤. 
- ì ì€ ë°ì´í„°ë¡œ í•™ìŠµí•  ê²½ìš°, Resnet50ë³´ë‹¤ ì„±ëŠ¥ì´ ë” ë–¨ì–´ì§ˆ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
- Imagenet 21K (1400ë§Œ ìž¥), JFT-300M (3ì–µ ìž¥) ìœ¼ë¡œ Pre-training, CIFAR-10 (6ë§Œ ìž¥) ìœ¼ë¡œ Transfer Learning
- Pretrained modelì˜ í¬ê¸°ëŠ” ì•½ 300MB

&nbsp;
&nbsp;
&nbsp;

## **Formulation**
### **1. Input**
#### (1) Image
- (C, H, W) = (Channel, Height, Width)
    
$$x\in\mathbb{R}^{C\times{H}\times{W}}$$
    

#### (2) Flatten
- $$P$$: íŒ¨ì¹˜ ì‚¬ì´ì¦ˆ,  $$N$$: ì´ë¯¸ì§€ ë‹¹ íŒ¨ì¹˜ì˜ ê°œìˆ˜
     
$$x_p \in \mathbb{R}^{N \times (P^2 C)} \quad \text{where} \quad N=\frac{HW}{P^2}$$

#### (3) Linear Projection
- $$D$$: Latent Vectorì˜ í¬ê¸°
     
$$[x^1_p E; x^2_p E; \cdots ; x^N_p] \in \mathbb{R}^{N \times D} \quad \text{where} \quad E \in \mathbb{R}^{(P^2 C)\times D}$$

#### (4) Insert the Class Token
- Class tokenì€ BERTì˜ class tokenê³¼ ìœ ì‚¬í•˜ë©°, Embedding patchë“¤ ì•žì— í•™ìŠµ ê°€ëŠ¥í•œ tokenì„ ì¶”ê°€í•©ë‹ˆë‹¤.   
  âž” Class token $$x_{cls}$$ ì€ Transformerì˜ ì—¬ëŸ¬ Encoder ì¸µì„ ê±°ì³ $$z^0 _L$$ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.    
  âž” ìµœì¢…ì ìœ¼ë¡œ ì´ ê°’ì€ ì´ë¯¸ì§€ì— ëŒ€í•œ Representation Vectorë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.    

$$z_0= [x_{cls}; x^1_p E; x^2_p E; \cdots ; x^N_p] \in \mathbb{R}^{(N+1)\times D}$$

#### (5) Add the Positional Encoding
> [Positional Encoding ì´ëž€?](https://standing-o.github.io/posts/positional-encoding/)
{: .prompt-tip }
    
$$z_0= [x_{cls}; x^1_p E; x^2_p E; \cdots ; x^N_p] + E_{pos} \in \mathbb{R}^{(N+1)\times D} \quad \text{where} \quad E_{pos} \in \mathbb{R}^{(N+1)\times D}$$

&nbsp;
&nbsp;
&nbsp;

### **2. Transformer Encoder**
#### (1) Multi-Head Self Attention (MSA)
- Query($$q$$), Key($$k$$), Value($$v$$) ê°„ì˜ ê´€ê³„ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
$$q = z \cdot w_q, \,\, k=z\cdot w_k, \,\, v = z \cdot w_v \quad \text{where} \quad w_q, w_k, w_v \in \mathbb{R}^{D \times D_h}$$
    
- **Self-Attention** 
  - $$A$$: Attention score matrix
  - Attention scoreê°€ value($$v$$)ì™€ ê³±í•´ì§€ë©´ì„œ query($$q$$)ì™€ key($$k$$)ì˜ ì—°ê´€ì„±ì´ value($$v$$)ì— ë°˜ì˜ë˜ì–´ ê·¸ ì¤‘ìš”ë„ë¥¼ ë°˜ì˜í•˜ê²Œ ë©ë‹ˆë‹¤.
  - $$D^{\frac{1}{2}}_h$$ ë¡œ ë‚˜ëˆ„ëŠ” ì´ìœ ëŠ” Softmax ê°’ì´ ìž‘ì€ Gradientë¥¼ ê°€ì§€ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•¨ìž…ë‹ˆë‹¤.
    
  $$SA(z) = A \cdot v \in \mathbb{R}^{N \times D_h} \quad \text{where} \quad A=\text{softmax}(\frac{q \cdot k^T}{\sqrt{D_h}}) \in \mathbb{R}^{N \times N}$$

- **Multi-head Self Attention**
    
$$\text{MSA}(z) = [SA_1(z); SA_2(z); \cdots ; SA_k(z)]U_{msa} \quad \text{where} \quad U_{msa} \in (k, D_h, D)$$

#### (2) Multi-Layer Perceptron (MLP)
- Pre-training ã…£ 1ê°œì˜ Hidden layer ì‚¬ìš©
- Fine-tuning ã…£ 1ê°œì˜ Linear Layer ì‚¬ìš©

#### (3) Encoder with L layers
- $$L$$: Layer ê°œìˆ˜

- `Layer Normalization âž” Multi-head Self Attention âž” Skip Connection`
    
$$z'_l = MSA(LN(z_{l-1})) + z_{l-1},$$
    
- `Layer Normalization âž” Multi-layer Perceptron âž” Skip Connection`
    
$$z_l = MLP(LN(z'_l)) + z'_l \quad \text{for} \quad l=1,2,\ldots, L$$
    

$$\text{where} \quad LN(z^j_i) = \gamma \frac{z^j_i - \mu_i}{\sqrt{\sigma^2_i + \epsilon}} + \beta$$
    
- $$\gamma, \beta$$: í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°

### **3. Output**
- Class token $$z^0 _L$$ ì„ í†µí•´ ìµœì¢… ì˜ˆì¸¡ì´ ìƒì„±ë©ë‹ˆë‹¤.
- $$C$$: í´ëž˜ìŠ¤ ê°œìˆ˜
    
$$\hat{y} = LN(z^0_L) \in \mathbb{R}^C \quad \text{where} \quad z^0_L \in \mathbb{R}^D \quad (z_L \in \mathbb{R}^{(N+1) \times D})$$
    

&nbsp;
&nbsp;
&nbsp;

## **ViT Variants**
### **BEIT** (Bidirectional Encoder representation from Image Transformers)
> Bao, Hangbo, et al. "Beit: Bert pre-training of image transformers." arXiv preprint arXiv:2106.08254 (2021).

### **CCT** (Compact Convolutional Transformers)
> Hassani, Ali, et al. "Escaping the big data paradigm with compact transformers." arXiv preprint arXiv:2104.05704 (2021).

### **CvT** (Convolutional vision Transformer)
> Wu, Haiping, et al. "Cvt: Introducing convolutions to vision transformers." Proceedings of the IEEE/CVF international conference on computer vision. 2021.

### **DeiT** (Data-efficient image Transformer)
> Touvron, Hugo, et al. "Training data-efficient image transformers & distillation through attention." International conference on machine learning. PMLR, 2021.

### **MobileViT**
> Mehta, Sachin, and Mohammad Rastegari. "Mobilevit: light-weight, general-purpose, and mobile-friendly vision transformer." arXiv preprint arXiv:2110.02178 (2021).

### **PvT** (Pyramid vision Transformer)
> Wang, Wenhai, et al. "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions." Proceedings of the IEEE/CVF international conference on computer vision. 2021.

### **Swin Transformer**
> Liu, Ze, et al. "Swin transformer: Hierarchical vision transformer using shifted windows." Proceedings of the IEEE/CVF international conference on computer vision. 2021.

### **T2T-VIT** (Tokens-to-Token VIT)
> Yuan, Li, et al. "Tokens-to-token vit: Training vision transformers from scratch on imagenet." Proceedings of the IEEE/CVF international conference on computer vision. 2021.

### **ViT with Deformable Attention**
> Xia, Zhuofan, et al. "Vision transformer with deformable attention." Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.

&nbsp;
&nbsp;
&nbsp;

---------------
## Reference
1. Dosovitskiy, Alexey. "An image is worth 16x16 words: Transformers for image recognition at scale." arXiv preprint arXiv:2010.11929 (2020).
