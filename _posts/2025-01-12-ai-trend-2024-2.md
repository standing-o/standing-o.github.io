---
title: "2024년 AI 기술 트렌드 둘러보기 #2 | AI Trends"
date: 2025-01-12 00:00:00 +/-TTTT
categories: [AI 트렌드]
tags: [ai-trend, survey]
math: true
toc: true
author: seoyoung
img_path: /assets/img/for_post/
pin: false
image:
  path: 20240710-t.jpg
  alt: ""
description: 🤖 2024년 하반기 AI 기술 동향과 주요 뉴스, 논문, 글, 사례를 소개합니다.
---

--------------------

> **<u>KEYWORDS</u>**        
> 인공지능 기술 트렌드, 2024 AI, Recent AI Trend, AI Issue, AI News, Top AI Paper
{: .prompt-info }

--------------------


&nbsp;
&nbsp;
&nbsp;

## 💡 **Issues**
### AI와 탄소배출량 [^ref-iss-1]
- Google의 탄소 배출량은 2021년에서 2022년 사이에 16.7%, 2022년에서 2023년 사이에 13.5% 증가하여 총 48% 증가했습니다. 
- Google의 연례 환경 보고서에 따르면, AI 기술이 제품에 점점 통합되면서 AI 컴퓨팅의 강도가 높아져 에너지 수요가 증가하고 탄소 배출량을 줄이는 것이 어려워지고 있다고 합니다.

### Google의 Neural GCM [^ref-iss-2]
- Google은 머신러닝과 기존의 수치해석 기반 순환 모델을 결합한 날씨 시뮬레이터 모델인 Neural GCM을 발표했습니다.
- Neural GCM은 유럽중기예보센터의 기상 모델과 비교했을 때 거의 유사한 성능을 보이면서도 컴퓨터 계산 효율이 좋습니다.

### EU의 Meta 멀티모달 모델 금지 [^ref-iss-3]
- Meta가 페이스북이나 인스타그램의 데이터로 모델을 학습시킴으로써 EU 개인정보 보호법을 위반할 수 있다고 밝혔습니다. 
- 따라서 EU 회사들은 향후에도 Meta의 멀티모달 모델을 활용한 어플을 만들 수 없게 되었습니다.

### OpenAI의 SearchGPT 발표 [^ref-iss-4]
- SearchGPT는 OpenAI의 모델이 생성하는 답변과 웹 크롤러가 수집한 정보를 결합하여 빠르고 직접적인 답변을 제공하는 새 검색 기능입니다.

![fig1](20250112-1.png){: width="600"}

### 군사와 AI [^ref-iss-5]
- 60여 개국이 군사에 AI를 사용하는 것에 대한 비구속적 지침인 행동 청사진에 서명했습니다.
- 이 청사진은 살상 무기 개발에 AI를 사용하는 것을 금지하고, 평화 유지와 인권 보호를 위한 국제 협력을 촉구합니다.

### Meta의 MovieGen [^ref-iss-6]
- Meta는 MovieGen이라고 하는 고화질 AI 텍스트-비디오 모델을 출시했습니다.

### 과소 평가된 임베딩 [^ref-iss-7]
- 최근 몇 년 간 임베딩(Embedding) 기술이 크게 발전하여 텍스트 간 연결을 발견하는 데 유용하게 사용될 수 있습니다.
- 해당 글은 문서 크기와 관계없이 동일한 크기의 배열을 반환하여 다양한 텍스트를 수학적으로 비교하는 방법을 보여줍니다.

### 개발자를 대체하는 AI [^ref-iss-8]
- Google의 CEO는 회사의 신규 코드 25% 이상이 AI에 의해 생성되고 있으며, Goose라는 Google의 AI 모델을 활용하여 개발 과정을 최적화하고 있다고 밝혔습니다.

### Maximum Likelihood Estimation [^ref-iss-9]
- 특정 손실 함수를 사용하고 다른 것들은 사용하지 않는지에 대한 의문을 해결하기 위해 Maximum Likelihood Estimation(MLE) 개념을 탐구합니다. 

### DeepSeek-V3 [^ref-iss-10]
- 토큰당 37B 매개변수를 활성화하는 671B 매개변수 MoE 언어 모델로, MLA 및 DeepSeekMoE 구조를 활용했습니다.


&nbsp;
&nbsp;
&nbsp;

-------------------

## 📰 **Papers**

### Detecting hallucinations in large language models using semantic entropy [^ref-pa-1]
- 옥스포드 대학의 연구원들은 LLM의 Hallucination 문제를 해결하기위해 통계 이론 기반의 불확실성 추정 방법으로 임의 오류(Confabulation)를 감지하는 방법을 발표했습니다.
- 특정 입력이 주어졌을 때 모델이 다양한 의미의 출력을 생성하여 응답할 가능성이 높을수록 해당 입력에 대한 응답이 Hallucination일 가능성이 높습니다. 

### SpreadsheetLLM: Encoding Spreadsheets for Large Language Models [^ref-pa-2]
- 스프레드시트를 잘 이해하고 추론하는 LLM을 최적화하기 위한 인코딩 방법과 Structural Anchor-based Compression, Inverse Index Translation, Data-Format-Aware Aggregation 모듈을 개발했습니다.
- GPT-4의 In-context Learning에서 스프레드시트 테이블 감지 성능을 25.6% 정도 향상시켰습니다.

### A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks [^ref-pa-3]
- 다양한 NLP Task에 대한 Prompt Engineering을 신속하게 할 수 있도록 작성된 Survey입니다.

### Beyond Euclid: An Illustrated Guide to Modern Machine Learning with Geometric, Topological, and Algebraic Structures [^ref-pa-4]
- 기존 머신러닝 방법론들은 주로 유클리드 공간 데이터를 다뤘지만, 현대에는 비유클리드적 구조의 복잡한 데이터를 점점 더 마주하고 있습니다.
- 비유클리드 구조를 활용하여 현대 머신러닝을 수학적으로 재정의하고 다양한 데이터 유형에 일반화하는 연구들을 소개합니다.

![fig2](20250112-2.png){: width="600"}
_Beyond Euclid: Algebraic Transformations_

### Agentic Retrieval-Augmented Generation for Time Series Analysis [^ref-pa-5]
- 시계열 데이터를 분석하기 위한 Agent RAG 프레임워크를 제안합니다.
- 다중 에이전트 구조와 맞춤형 SLM을 활용해 새로운 데이터 예측 성능을 향상시켰습니다.

### Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2 [^ref-pa-6]
- Google은 SAE를 사용하여 신경망을 해석했던 이전의 연구들을 활용하여 Gemma 2 2B와 Gemma 2 9B에 대한 SAE를 구축했습니다.

### De novo design of high-affinity protein binders with AlphaProteo [^ref-pa-7]
- Google은 단백질 설계를 위해 다양한 머신러닝 모델을 결합하여 학습했습니다.

### Can Large Language Models Unlock Novel Scientific Research Ideas? [^ref-pa-8]
- 연구 논문 정보를 기반으로 LLM이 새 연구 아이디어를 생성하는 능력을 탐구합니다. 
- Claude-2가 GPT-4보다 더 다양한 아이디어를 생성하며 GPT-3.5와 Gemini보다 저자의 관점에서는 더 부합하는 결과를 보였습니다.

### The Curse of Recursion: Training on Generated Data Makes Models Forget [^ref-pa-9]
- 모델이 합성 데이터로 학습될수록 성능 저하가 발생하며, 특히 이미 학습된 모델의 출력을 입력으로 학습할 경우 점점 더 정확도가 떨어질 수 있다고 말합니다.

### Monolith: Real Time Recommendation System With Collisionless Embedding Table [^ref-pa-10]
- Tiktok은 Monolith라는 추천 시스템 모델 구조를 공개했으며, Cuckoo 해싱을 기반으로 하는 충돌 없는 임베딩 테이블에 대한 통찰력을 함께 제공합니다.


&nbsp;
&nbsp;
&nbsp;

----------------

## 🧠 **Deep Learning, LLM**
### Hugging Face의 Open LLM Leaderboard [^ref-llm-1]
- Hugging Face는 Open LLM Leaderboard를 개편했으며, Qwen2에서 최근 출시된 720억개의 파라미터를 가진 Instruction-tuned 버전이 43.02/100점으로 1위를 차지했습니다.
- 작년에는 200만 명이 Open LLM Leaderboard를 조회하였으며, 30만 명 이상의 Hugging Face 커뮤니티 구성원이 매달 이를 잘 활용하고있다고 합니다.

![fig3](20250112-3.png){: width="800"}

### OpenAI o1 [^ref-llm-2]
- OpenAI의 o1-preview와 o1-mini는 Chain of Thought를 활용해 수학, 과학, 코딩 등에서 뛰어난 성능을 보이는 새로운 언어 모델입니다. 
- 강화 학습을 통해 학습된 이 모델들은 기존보다 높은 정확도를 제공하지만, 사용자가 추론 과정을 볼 수 없다는 한계를 갖습니다.

### Llama 3.2 [^ref-llm-3]
- Llama 3.2는 소형/중형 Vision LLM(11B, 90B)과 모바일 장치에 적합한 경량 텍스트 모델(1B, 3B)을 출시해 높은 성능을 보였습니다.
- 손쉬운 배포가 가능한 Llama Stack을 공개하여 RAG 및 툴링 기반 애플리케이션 구현을 지원합니다.

### Google의 Gemini 2.0 출시  [^ref-llm-4]
- Google의 Gemini 2.0은 텍스트, 이미지, 비디오, 오디오, 코드를 처리할 수 있는 고급 멀티모달 기능을 도입했습니다.

&nbsp;
&nbsp;
&nbsp;

----------------

## ⚙️ **MLOps & Data**
### Google의 TPU [^ref-data-1]
- Google의 TPU는 10년 전부터 개발되어왔으며, 대규모 컴퓨팅에서 성능과 효율성을 크게 향상시켰습니다. 그에 대한 여정을 글로 정리했습니다.

### Microsoft의 BitNet [^ref-data-2]
- Microsoft의 Bitnet.cpp는 GPU 기반 LLM을 CPU에서 효율적으로 실행하기 위한 공식 추론 프레임워크입니다.

### Google's Globally-Distributed Database [^ref-data-3]
- Spanner는 Google의 확장 가능한, 전 세계적으로 분산되고 동기화된 데이터베이스 시스템으로, 세계 규모에서 데이터를 분배하고 외부 일관성 분산 트랜잭션을 지원하는 최초의 시스템입니다. 

### AMD GPU 추론 [^ref-data-4]
- MLC-LLM는 AMD GPU에서 LLM을 컴파일하고 배포할 수 있는 기술로, ROCm을 사용하여 NVIDIA GPU와 경쟁할 수 있는 성능을 제공합니다. 

&nbsp;
&nbsp;
&nbsp;

----------------
## References
[^ref-iss-1]: [Our 2024 Environmental Report](https://blog.google/outreach-initiatives/sustainability/2024-environmental-report)

[^ref-iss-2]: [Fast, accurate climate modeling with NeuralGCM](https://research.google/blog/fast-accurate-climate-modeling-with-neuralgcm/)

[^ref-iss-3]: [Scoop: Meta won't offer future multimodal AI models in EU](https://www.axios.com/2024/07/17/meta-future-multimodal-ai-models-eu)

[^ref-iss-4]: [SearchGPT Prototype](https://openai.com/index/searchgpt-prototype/)

[^ref-iss-5]: [Sixty countries endorse 'blueprint' for AI use in military; China opts out](https://www.reuters.com/technology/artificial-intelligence/south-korea-summit-announces-blueprint-using-ai-military-2024-09-10)

[^ref-iss-6]: [Meta Movie Gen](https://ai.meta.com/research/movie-gen/)

[^ref-iss-7]: [Embeddings are underrated](https://technicalwriting.dev/embeddings/overview.html)

[^ref-iss-8]: [Over 25% of Google’s code is now written by AI—and CEO Sundar Pichai says it’s just the start](https://timesofindia.indiatimes.com/technology/tech-news/google-ceo-sundar-pichai-says-ai-generates-more-than-25-codes-at-the-company-this-helps-our-engineers-do-/articleshow/114755202.cms)

[^ref-iss-9]: [Maximum Likelihood Estimation and Loss Functions](https://rish-01.github.io/blog/posts/ml_estimation/)

[^ref-iss-10]: [DeepSeek-V3 Technical Report](https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf)

[^ref-pa-1]: [Detecting hallucinations in large language models using semantic entropy](https://www.nature.com/articles/s41586-024-07421-0)

[^ref-pa-2]: [SpreadsheetLLM: Encoding Spreadsheets for Large Language Models](https://arxiv.org/abs/2407.09025)

[^ref-pa-3]: [A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks](https://arxiv.org/abs/2407.12994)

[^ref-pa-4]: [Beyond Euclid: An Illustrated Guide to Modern Machine Learning with Geometric, Topological, and Algebraic Structures](https://www.arxiv.org/abs/2407.09468) 

[^ref-pa-5]: [Agentic Retrieval-Augmented Generation for Time Series Analysis](https://arxiv.org/abs/2408.14484)

[^ref-pa-6]: [Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2](https://storage.googleapis.com/gemma-scope/gemma-scope-report.pdf)

[^ref-pa-7]: [De novo design of high-affinity protein binders with AlphaProteo](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaproteo-generates-novel-proteins-for-biology-and-health-research/AlphaProteo2024.pdf)

[^ref-pa-8]: [Can Large Language Models Unlock Novel Scientific Research Ideas?](https://arxiv.org/abs/2409.06185)

[^ref-pa-9]: [The Curse of Recursion: Training on Generated Data Makes Models Forget](https://arxiv.org/abs/2305.17493)

[^ref-pa-10]: [Monolith: Real Time Recommendation System With Collisionless Embedding Table](https://arxiv.org/abs/2209.07663)

[^ref-llm-1]: [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)

[^ref-llm-2]: [Learning to reason with LLMs](https://openai.com/index/learning-to-reason-with-llms)

[^ref-llm-3]: [Llama 3.2: Revolutionizing edge AI and vision with open, customizable models](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices)

[^ref-llm-4]: [Introducing Gemini 2.0: our new AI model for the agentic era](https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/)

[^ref-data-1]: [TPU transformation: A look back at 10 years of our AI-specialized chips](https://cloud.google.com/blog/transform/ai-specialized-chips-tpu-history-gen-ai?hl=en)

[^ref-data-2]: [BitNet](https://github.com/microsoft/BitNet)

[^ref-data-3]: [Spanner: Google's Globally-Distributed Database](https://research.google/pubs/spanner-googles-globally-distributed-database-2/)

[^ref-data-4]: [Making AMD GPUs competitive for LLM inference](https://blog.mlc.ai/2023/08/09/Making-AMD-GPUs-competitive-for-LLM-inference)
