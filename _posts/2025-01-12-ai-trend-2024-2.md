---
title: "2024ë…„ AI ê¸°ìˆ  íŠ¸ë Œë“œ ë‘˜ëŸ¬ë³´ê¸° #2 | AI Trends"
date: 2025-01-12 00:00:00 +/-TTTT
categories: [AI íŠ¸ë Œë“œ]
tags: [ai-trend, survey]
math: true
toc: true
author: seoyoung
img_path: /assets/img/for_post/
pin: false
image:
  path: 20240710-t.jpg
  alt: ""
description: ğŸ¤– 2024ë…„ í•˜ë°˜ê¸° AI ê¸°ìˆ  ë™í–¥ê³¼ ì£¼ìš” ë‰´ìŠ¤, ë…¼ë¬¸, ê¸€, ì‚¬ë¡€ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤.
---

--------------------

> **<u>KEYWORDS</u>**        
> ì¸ê³µì§€ëŠ¥ ê¸°ìˆ  íŠ¸ë Œë“œ, 2024 AI, Recent AI Trend, AI Issue, AI News, Top AI Paper
{: .prompt-info }

--------------------


&nbsp;
&nbsp;
&nbsp;

## ğŸ’¡ **Issues**
### AIì™€ íƒ„ì†Œë°°ì¶œëŸ‰ [^ref-iss-1]
- Googleì˜ íƒ„ì†Œ ë°°ì¶œëŸ‰ì€ 2021ë…„ì—ì„œ 2022ë…„ ì‚¬ì´ì— 16.7%, 2022ë…„ì—ì„œ 2023ë…„ ì‚¬ì´ì— 13.5% ì¦ê°€í•˜ì—¬ ì´ 48% ì¦ê°€í–ˆìŠµë‹ˆë‹¤. 
- Googleì˜ ì—°ë¡€ í™˜ê²½ ë³´ê³ ì„œì— ë”°ë¥´ë©´, AI ê¸°ìˆ ì´ ì œí’ˆì— ì ì  í†µí•©ë˜ë©´ì„œ AI ì»´í“¨íŒ…ì˜ ê°•ë„ê°€ ë†’ì•„ì ¸ ì—ë„ˆì§€ ìˆ˜ìš”ê°€ ì¦ê°€í•˜ê³  íƒ„ì†Œ ë°°ì¶œëŸ‰ì„ ì¤„ì´ëŠ” ê²ƒì´ ì–´ë ¤ì›Œì§€ê³  ìˆë‹¤ê³  í•©ë‹ˆë‹¤.

### Googleì˜ Neural GCM [^ref-iss-2]
- Googleì€ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ê¸°ì¡´ì˜ ìˆ˜ì¹˜í•´ì„ ê¸°ë°˜ ìˆœí™˜ ëª¨ë¸ì„ ê²°í•©í•œ ë‚ ì”¨ ì‹œë®¬ë ˆì´í„° ëª¨ë¸ì¸ Neural GCMì„ ë°œí‘œí–ˆìŠµë‹ˆë‹¤.
- Neural GCMì€ ìœ ëŸ½ì¤‘ê¸°ì˜ˆë³´ì„¼í„°ì˜ ê¸°ìƒ ëª¨ë¸ê³¼ ë¹„êµí–ˆì„ ë•Œ ê±°ì˜ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë³´ì´ë©´ì„œë„ ì»´í“¨í„° ê³„ì‚° íš¨ìœ¨ì´ ì¢‹ìŠµë‹ˆë‹¤.

### EUì˜ Meta ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ê¸ˆì§€ [^ref-iss-3]
- Metaê°€ í˜ì´ìŠ¤ë¶ì´ë‚˜ ì¸ìŠ¤íƒ€ê·¸ë¨ì˜ ë°ì´í„°ë¡œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚´ìœ¼ë¡œì¨ EU ê°œì¸ì •ë³´ ë³´í˜¸ë²•ì„ ìœ„ë°˜í•  ìˆ˜ ìˆë‹¤ê³  ë°í˜”ìŠµë‹ˆë‹¤. 
- ë”°ë¼ì„œ EU íšŒì‚¬ë“¤ì€ í–¥í›„ì—ë„ Metaì˜ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì„ í™œìš©í•œ ì–´í”Œì„ ë§Œë“¤ ìˆ˜ ì—†ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.

### OpenAIì˜ SearchGPT ë°œí‘œ [^ref-iss-4]
- SearchGPTëŠ” OpenAIì˜ ëª¨ë¸ì´ ìƒì„±í•˜ëŠ” ë‹µë³€ê³¼ ì›¹ í¬ë¡¤ëŸ¬ê°€ ìˆ˜ì§‘í•œ ì •ë³´ë¥¼ ê²°í•©í•˜ì—¬ ë¹ ë¥´ê³  ì§ì ‘ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ìƒˆ ê²€ìƒ‰ ê¸°ëŠ¥ì…ë‹ˆë‹¤.

![fig1](20250112-1.png){: width="600"}

### êµ°ì‚¬ì™€ AI [^ref-iss-5]
- 60ì—¬ ê°œêµ­ì´ êµ°ì‚¬ì— AIë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì— ëŒ€í•œ ë¹„êµ¬ì†ì  ì§€ì¹¨ì¸ í–‰ë™ ì²­ì‚¬ì§„ì— ì„œëª…í–ˆìŠµë‹ˆë‹¤.
- ì´ ì²­ì‚¬ì§„ì€ ì‚´ìƒ ë¬´ê¸° ê°œë°œì— AIë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¸ˆì§€í•˜ê³ , í‰í™” ìœ ì§€ì™€ ì¸ê¶Œ ë³´í˜¸ë¥¼ ìœ„í•œ êµ­ì œ í˜‘ë ¥ì„ ì´‰êµ¬í•©ë‹ˆë‹¤.

### Metaì˜ MovieGen [^ref-iss-6]
- MetaëŠ” MovieGenì´ë¼ê³  í•˜ëŠ” ê³ í™”ì§ˆ AI í…ìŠ¤íŠ¸-ë¹„ë””ì˜¤ ëª¨ë¸ì„ ì¶œì‹œí–ˆìŠµë‹ˆë‹¤.

### ê³¼ì†Œ í‰ê°€ëœ ì„ë² ë”© [^ref-iss-7]
- ìµœê·¼ ëª‡ ë…„ ê°„ ì„ë² ë”©(Embedding) ê¸°ìˆ ì´ í¬ê²Œ ë°œì „í•˜ì—¬ í…ìŠ¤íŠ¸ ê°„ ì—°ê²°ì„ ë°œê²¬í•˜ëŠ” ë° ìœ ìš©í•˜ê²Œ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- í•´ë‹¹ ê¸€ì€ ë¬¸ì„œ í¬ê¸°ì™€ ê´€ê³„ì—†ì´ ë™ì¼í•œ í¬ê¸°ì˜ ë°°ì—´ì„ ë°˜í™˜í•˜ì—¬ ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ë¹„êµí•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

### ê°œë°œìë¥¼ ëŒ€ì²´í•˜ëŠ” AI [^ref-iss-8]
- Googleì˜ CEOëŠ” íšŒì‚¬ì˜ ì‹ ê·œ ì½”ë“œ 25% ì´ìƒì´ AIì— ì˜í•´ ìƒì„±ë˜ê³  ìˆìœ¼ë©°, Gooseë¼ëŠ” Googleì˜ AI ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ê°œë°œ ê³¼ì •ì„ ìµœì í™”í•˜ê³  ìˆë‹¤ê³  ë°í˜”ìŠµë‹ˆë‹¤.

### Maximum Likelihood Estimation [^ref-iss-9]
- íŠ¹ì • ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê³  ë‹¤ë¥¸ ê²ƒë“¤ì€ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ì§€ì— ëŒ€í•œ ì˜ë¬¸ì„ í•´ê²°í•˜ê¸° ìœ„í•´ Maximum Likelihood Estimation(MLE) ê°œë…ì„ íƒêµ¬í•©ë‹ˆë‹¤. 

### DeepSeek-V3 [^ref-iss-10]
- í† í°ë‹¹ 37B ë§¤ê°œë³€ìˆ˜ë¥¼ í™œì„±í™”í•˜ëŠ” 671B ë§¤ê°œë³€ìˆ˜ MoE ì–¸ì–´ ëª¨ë¸ë¡œ, MLA ë° DeepSeekMoE êµ¬ì¡°ë¥¼ í™œìš©í–ˆìŠµë‹ˆë‹¤.


&nbsp;
&nbsp;
&nbsp;

-------------------

## ğŸ“° **Papers**

### Detecting hallucinations in large language models using semantic entropy [^ref-pa-1]
- ì˜¥ìŠ¤í¬ë“œ ëŒ€í•™ì˜ ì—°êµ¬ì›ë“¤ì€ LLMì˜ Hallucination ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸°ìœ„í•´ í†µê³„ ì´ë¡  ê¸°ë°˜ì˜ ë¶ˆí™•ì‹¤ì„± ì¶”ì • ë°©ë²•ìœ¼ë¡œ ì„ì˜ ì˜¤ë¥˜(Confabulation)ë¥¼ ê°ì§€í•˜ëŠ” ë°©ë²•ì„ ë°œí‘œí–ˆìŠµë‹ˆë‹¤.
- íŠ¹ì • ì…ë ¥ì´ ì£¼ì–´ì¡Œì„ ë•Œ ëª¨ë¸ì´ ë‹¤ì–‘í•œ ì˜ë¯¸ì˜ ì¶œë ¥ì„ ìƒì„±í•˜ì—¬ ì‘ë‹µí•  ê°€ëŠ¥ì„±ì´ ë†’ì„ìˆ˜ë¡ í•´ë‹¹ ì…ë ¥ì— ëŒ€í•œ ì‘ë‹µì´ Hallucinationì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. 

### SpreadsheetLLM: Encoding Spreadsheets for Large Language Models [^ref-pa-2]
- ìŠ¤í”„ë ˆë“œì‹œíŠ¸ë¥¼ ì˜ ì´í•´í•˜ê³  ì¶”ë¡ í•˜ëŠ” LLMì„ ìµœì í™”í•˜ê¸° ìœ„í•œ ì¸ì½”ë”© ë°©ë²•ê³¼ Structural Anchor-based Compression, Inverse Index Translation, Data-Format-Aware Aggregation ëª¨ë“ˆì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤.
- GPT-4ì˜ In-context Learningì—ì„œ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ í…Œì´ë¸” ê°ì§€ ì„±ëŠ¥ì„ 25.6% ì •ë„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.

### A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks [^ref-pa-3]
- ë‹¤ì–‘í•œ NLP Taskì— ëŒ€í•œ Prompt Engineeringì„ ì‹ ì†í•˜ê²Œ í•  ìˆ˜ ìˆë„ë¡ ì‘ì„±ëœ Surveyì…ë‹ˆë‹¤.

### Beyond Euclid: An Illustrated Guide to Modern Machine Learning with Geometric, Topological, and Algebraic Structures [^ref-pa-4]
- ê¸°ì¡´ ë¨¸ì‹ ëŸ¬ë‹ ë°©ë²•ë¡ ë“¤ì€ ì£¼ë¡œ ìœ í´ë¦¬ë“œ ê³µê°„ ë°ì´í„°ë¥¼ ë‹¤ë¤˜ì§€ë§Œ, í˜„ëŒ€ì—ëŠ” ë¹„ìœ í´ë¦¬ë“œì  êµ¬ì¡°ì˜ ë³µì¡í•œ ë°ì´í„°ë¥¼ ì ì  ë” ë§ˆì£¼í•˜ê³  ìˆìŠµë‹ˆë‹¤.
- ë¹„ìœ í´ë¦¬ë“œ êµ¬ì¡°ë¥¼ í™œìš©í•˜ì—¬ í˜„ëŒ€ ë¨¸ì‹ ëŸ¬ë‹ì„ ìˆ˜í•™ì ìœ¼ë¡œ ì¬ì •ì˜í•˜ê³  ë‹¤ì–‘í•œ ë°ì´í„° ìœ í˜•ì— ì¼ë°˜í™”í•˜ëŠ” ì—°êµ¬ë“¤ì„ ì†Œê°œí•©ë‹ˆë‹¤.

![fig2](20250112-2.png){: width="600"}
_Beyond Euclid: Algebraic Transformations_

### Agentic Retrieval-Augmented Generation for Time Series Analysis [^ref-pa-5]
- ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê¸° ìœ„í•œ Agent RAG í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
- ë‹¤ì¤‘ ì—ì´ì „íŠ¸ êµ¬ì¡°ì™€ ë§ì¶¤í˜• SLMì„ í™œìš©í•´ ìƒˆë¡œìš´ ë°ì´í„° ì˜ˆì¸¡ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.

### Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2 [^ref-pa-6]
- Googleì€ SAEë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹ ê²½ë§ì„ í•´ì„í–ˆë˜ ì´ì „ì˜ ì—°êµ¬ë“¤ì„ í™œìš©í•˜ì—¬ Gemma 2 2Bì™€ Gemma 2 9Bì— ëŒ€í•œ SAEë¥¼ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤.

### De novo design of high-affinity protein binders with AlphaProteo [^ref-pa-7]
- Googleì€ ë‹¨ë°±ì§ˆ ì„¤ê³„ë¥¼ ìœ„í•´ ë‹¤ì–‘í•œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ í•™ìŠµí–ˆìŠµë‹ˆë‹¤.

### Can Large Language Models Unlock Novel Scientific Research Ideas? [^ref-pa-8]
- ì—°êµ¬ ë…¼ë¬¸ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLMì´ ìƒˆ ì—°êµ¬ ì•„ì´ë””ì–´ë¥¼ ìƒì„±í•˜ëŠ” ëŠ¥ë ¥ì„ íƒêµ¬í•©ë‹ˆë‹¤. 
- Claude-2ê°€ GPT-4ë³´ë‹¤ ë” ë‹¤ì–‘í•œ ì•„ì´ë””ì–´ë¥¼ ìƒì„±í•˜ë©° GPT-3.5ì™€ Geminië³´ë‹¤ ì €ìì˜ ê´€ì ì—ì„œëŠ” ë” ë¶€í•©í•˜ëŠ” ê²°ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.

### The Curse of Recursion: Training on Generated Data Makes Models Forget [^ref-pa-9]
- ëª¨ë¸ì´ í•©ì„± ë°ì´í„°ë¡œ í•™ìŠµë ìˆ˜ë¡ ì„±ëŠ¥ ì €í•˜ê°€ ë°œìƒí•˜ë©°, íŠ¹íˆ ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ì˜ ì¶œë ¥ì„ ì…ë ¥ìœ¼ë¡œ í•™ìŠµí•  ê²½ìš° ì ì  ë” ì •í™•ë„ê°€ ë–¨ì–´ì§ˆ ìˆ˜ ìˆë‹¤ê³  ë§í•©ë‹ˆë‹¤.

### Monolith: Real Time Recommendation System With Collisionless Embedding Table [^ref-pa-10]
- Tiktokì€ Monolithë¼ëŠ” ì¶”ì²œ ì‹œìŠ¤í…œ ëª¨ë¸ êµ¬ì¡°ë¥¼ ê³µê°œí–ˆìœ¼ë©°, Cuckoo í•´ì‹±ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ì¶©ëŒ ì—†ëŠ” ì„ë² ë”© í…Œì´ë¸”ì— ëŒ€í•œ í†µì°°ë ¥ì„ í•¨ê»˜ ì œê³µí•©ë‹ˆë‹¤.


&nbsp;
&nbsp;
&nbsp;

----------------

## ğŸ§  **Deep Learning, LLM**
### Hugging Faceì˜ Open LLM Leaderboard [^ref-llm-1]
- Hugging FaceëŠ” Open LLM Leaderboardë¥¼ ê°œí¸í–ˆìœ¼ë©°, Qwen2ì—ì„œ ìµœê·¼ ì¶œì‹œëœ 720ì–µê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§„ Instruction-tuned ë²„ì „ì´ 43.02/100ì ìœ¼ë¡œ 1ìœ„ë¥¼ ì°¨ì§€í–ˆìŠµë‹ˆë‹¤.
- ì‘ë…„ì—ëŠ” 200ë§Œ ëª…ì´ Open LLM Leaderboardë¥¼ ì¡°íšŒí•˜ì˜€ìœ¼ë©°, 30ë§Œ ëª… ì´ìƒì˜ Hugging Face ì»¤ë®¤ë‹ˆí‹° êµ¬ì„±ì›ì´ ë§¤ë‹¬ ì´ë¥¼ ì˜ í™œìš©í•˜ê³ ìˆë‹¤ê³  í•©ë‹ˆë‹¤.

![fig3](20250112-3.png){: width="800"}

### OpenAI o1 [^ref-llm-2]
- OpenAIì˜ o1-previewì™€ o1-miniëŠ” Chain of Thoughtë¥¼ í™œìš©í•´ ìˆ˜í•™, ê³¼í•™, ì½”ë”© ë“±ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ìƒˆë¡œìš´ ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤. 
- ê°•í™” í•™ìŠµì„ í†µí•´ í•™ìŠµëœ ì´ ëª¨ë¸ë“¤ì€ ê¸°ì¡´ë³´ë‹¤ ë†’ì€ ì •í™•ë„ë¥¼ ì œê³µí•˜ì§€ë§Œ, ì‚¬ìš©ìê°€ ì¶”ë¡  ê³¼ì •ì„ ë³¼ ìˆ˜ ì—†ë‹¤ëŠ” í•œê³„ë¥¼ ê°–ìŠµë‹ˆë‹¤.

### Llama 3.2 [^ref-llm-3]
- Llama 3.2ëŠ” ì†Œí˜•/ì¤‘í˜• Vision LLM(11B, 90B)ê³¼ ëª¨ë°”ì¼ ì¥ì¹˜ì— ì í•©í•œ ê²½ëŸ‰ í…ìŠ¤íŠ¸ ëª¨ë¸(1B, 3B)ì„ ì¶œì‹œí•´ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
- ì†ì‰¬ìš´ ë°°í¬ê°€ ê°€ëŠ¥í•œ Llama Stackì„ ê³µê°œí•˜ì—¬ RAG ë° íˆ´ë§ ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬í˜„ì„ ì§€ì›í•©ë‹ˆë‹¤.

### Googleì˜ Gemini 2.0 ì¶œì‹œ  [^ref-llm-4]
- Googleì˜ Gemini 2.0ì€ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ë¹„ë””ì˜¤, ì˜¤ë””ì˜¤, ì½”ë“œë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ê³ ê¸‰ ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥ì„ ë„ì…í–ˆìŠµë‹ˆë‹¤.

&nbsp;
&nbsp;
&nbsp;

----------------

## âš™ï¸ **MLOps & Data**
### Googleì˜ TPU [^ref-data-1]
- Googleì˜ TPUëŠ” 10ë…„ ì „ë¶€í„° ê°œë°œë˜ì–´ì™”ìœ¼ë©°, ëŒ€ê·œëª¨ ì»´í“¨íŒ…ì—ì„œ ì„±ëŠ¥ê³¼ íš¨ìœ¨ì„±ì„ í¬ê²Œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ê·¸ì— ëŒ€í•œ ì—¬ì •ì„ ê¸€ë¡œ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.

### Microsoftì˜ BitNet [^ref-data-2]
- Microsoftì˜ Bitnet.cppëŠ” GPU ê¸°ë°˜ LLMì„ CPUì—ì„œ íš¨ìœ¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ê³µì‹ ì¶”ë¡  í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

### Google's Globally-Distributed Database [^ref-data-3]
- SpannerëŠ” Googleì˜ í™•ì¥ ê°€ëŠ¥í•œ, ì „ ì„¸ê³„ì ìœ¼ë¡œ ë¶„ì‚°ë˜ê³  ë™ê¸°í™”ëœ ë°ì´í„°ë² ì´ìŠ¤ ì‹œìŠ¤í…œìœ¼ë¡œ, ì„¸ê³„ ê·œëª¨ì—ì„œ ë°ì´í„°ë¥¼ ë¶„ë°°í•˜ê³  ì™¸ë¶€ ì¼ê´€ì„± ë¶„ì‚° íŠ¸ëœì­ì…˜ì„ ì§€ì›í•˜ëŠ” ìµœì´ˆì˜ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. 

### AMD GPU ì¶”ë¡  [^ref-data-4]
- MLC-LLMëŠ” AMD GPUì—ì„œ LLMì„ ì»´íŒŒì¼í•˜ê³  ë°°í¬í•  ìˆ˜ ìˆëŠ” ê¸°ìˆ ë¡œ, ROCmì„ ì‚¬ìš©í•˜ì—¬ NVIDIA GPUì™€ ê²½ìŸí•  ìˆ˜ ìˆëŠ” ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. 

&nbsp;
&nbsp;
&nbsp;

----------------
## References
[^ref-iss-1]: [Our 2024 Environmental Report](https://blog.google/outreach-initiatives/sustainability/2024-environmental-report)

[^ref-iss-2]: [Fast, accurate climate modeling with NeuralGCM](https://research.google/blog/fast-accurate-climate-modeling-with-neuralgcm/)

[^ref-iss-3]: [Scoop: Meta won't offer future multimodal AI models in EU](https://www.axios.com/2024/07/17/meta-future-multimodal-ai-models-eu)

[^ref-iss-4]: [SearchGPT Prototype](https://openai.com/index/searchgpt-prototype/)

[^ref-iss-5]: [Sixty countries endorse 'blueprint' for AI use in military; China opts out](https://www.reuters.com/technology/artificial-intelligence/south-korea-summit-announces-blueprint-using-ai-military-2024-09-10)

[^ref-iss-6]: [Meta Movie Gen](https://ai.meta.com/research/movie-gen/)

[^ref-iss-7]: [Embeddings are underrated](https://technicalwriting.dev/embeddings/overview.html)

[^ref-iss-8]: [Over 25% of Googleâ€™s code is now written by AIâ€”and CEO Sundar Pichai says itâ€™s just the start](https://timesofindia.indiatimes.com/technology/tech-news/google-ceo-sundar-pichai-says-ai-generates-more-than-25-codes-at-the-company-this-helps-our-engineers-do-/articleshow/114755202.cms)

[^ref-iss-9]: [Maximum Likelihood Estimation and Loss Functions](https://rish-01.github.io/blog/posts/ml_estimation/)

[^ref-iss-10]: [DeepSeek-V3 Technical Report](https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf)

[^ref-pa-1]: [Detecting hallucinations in large language models using semantic entropy](https://www.nature.com/articles/s41586-024-07421-0)

[^ref-pa-2]: [SpreadsheetLLM: Encoding Spreadsheets for Large Language Models](https://arxiv.org/abs/2407.09025)

[^ref-pa-3]: [A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks](https://arxiv.org/abs/2407.12994)

[^ref-pa-4]: [Beyond Euclid: An Illustrated Guide to Modern Machine Learning with Geometric, Topological, and Algebraic Structures](https://www.arxiv.org/abs/2407.09468) 

[^ref-pa-5]: [Agentic Retrieval-Augmented Generation for Time Series Analysis](https://arxiv.org/abs/2408.14484)

[^ref-pa-6]: [Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2](https://storage.googleapis.com/gemma-scope/gemma-scope-report.pdf)

[^ref-pa-7]: [De novo design of high-affinity protein binders with AlphaProteo](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaproteo-generates-novel-proteins-for-biology-and-health-research/AlphaProteo2024.pdf)

[^ref-pa-8]: [Can Large Language Models Unlock Novel Scientific Research Ideas?](https://arxiv.org/abs/2409.06185)

[^ref-pa-9]: [The Curse of Recursion: Training on Generated Data Makes Models Forget](https://arxiv.org/abs/2305.17493)

[^ref-pa-10]: [Monolith: Real Time Recommendation System With Collisionless Embedding Table](https://arxiv.org/abs/2209.07663)

[^ref-llm-1]: [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)

[^ref-llm-2]: [Learning to reason with LLMs](https://openai.com/index/learning-to-reason-with-llms)

[^ref-llm-3]: [Llama 3.2: Revolutionizing edge AI and vision with open, customizable models](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices)

[^ref-llm-4]: [Introducing Gemini 2.0: our new AI model for the agentic era](https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/)

[^ref-data-1]: [TPU transformation: A look back at 10 years of our AI-specialized chips](https://cloud.google.com/blog/transform/ai-specialized-chips-tpu-history-gen-ai?hl=en)

[^ref-data-2]: [BitNet](https://github.com/microsoft/BitNet)

[^ref-data-3]: [Spanner: Google's Globally-Distributed Database](https://research.google/pubs/spanner-googles-globally-distributed-database-2/)

[^ref-data-4]: [Making AMD GPUs competitive for LLM inference](https://blog.mlc.ai/2023/08/09/Making-AMD-GPUs-competitive-for-LLM-inference)
