---
title: "XAI, 설명 가능한 인공지능 이해해보기 | Explainable AI"
date: 2022-07-22 17:00:00 +/-TTTT
categories: [AI Theory, XAI]
tags: [lg-aimers, xai, deep-learning, supervised-learning, neural-network, weakly-supervised-learning, cam, lime, rise]
math: true
author: seoyoung
img_path: /assets/img/for_post/
description: 설명가능한 인공지능, 인공지능 해석, 딥러닝 해석, 딥러닝 블랙박스, Explainable AI, XAI, CAM, LIME, RISE
---



-------------------

> 설명가능한 인공지능(XAI)의 개념과 구분 방법을 소개합니다.
{: .prompt-info }

딥러닝 모델의 내부 동작을 이해하고 해석하는 방법부터 모델 해석의 신뢰성을 평가하기 위한 여러 지표를 다룹니다.

모델 해석을 검증하기 위한 각종 검증 방법과 취약점을 정리합니다.

&nbsp;
&nbsp;
&nbsp;

## **딥러닝 기반 지도학습 <sup>Supervised (Deep) Learning</sup>**
- 딥러닝 기반의 지도학습 기법들은 큰 발전을 이루어왔지만, 딥러닝 모델 자체는 매우 복잡하게 구성되어 있습니다.
  - 엔드 투 엔드(End-to-end) 학습은 블랙박스(Blackbox)가 될 수 있으며, 모델이 중요한 결정을 내릴 때 다양한 문제가 생길 수 있습니다.
- 모델의 **설명 가능성 <sup>Explainability</sup>, 해석 가능성 <sup>Interpretability</sup>** 이란?
  - **설명 가능성 <sup>Explainability</sup>** ㅣ 모델 예측에 대한 원인과 근거를 사람이 이해할 수 있는 정도
  - **해석 가능성 <sup>Interpretability</sup>** ㅣ 사람이 모델의 Output을 일관되게 예측할 수 있는 정도

&nbsp;
&nbsp;
&nbsp;

## **Taxonomy of XAI Methods**

- **Local** vs. **Global**
  - **Local** ㅣ 개별 예측을 설명
  - **Global** ㅣ 전체 모델의 동작을 설명
- **White-box** vs. **Black-box**
  - **White-box** ㅣ 설명자(Explainer)가 모델 내부를 평가 가능
  - **Black-box** ㅣ 설명자(Explainer)가 모델의 출력만 평가 가능
- **Intrinsic** vs. **Post-hoc**
  - **Intrinsic** ㅣ 학습 전에 모델 복잡도를 제한
  - **Post-hoc** ㅣ 학습된 머신러닝 모델에 적용
- **Model Specific** vs. **Model Agnostic**
  - **Model-specific** ㅣ 특정 모델 클래스에만 적용 가능한 방법
  - **Model-agnostic** ㅣ 모든 모델에 사용할 수 있는 방법

- **<u>예시</u>**
  - 선형 모델, 간단한 결정 트리
  ➔ Global, white-box, intrinsic, model-specific
  - Grad-CAM
  ➔ Local, white-box, post-hoc, model-agnostic

&nbsp;
&nbsp;
&nbsp;

## **Gradient Methods**
- 단순히 그레디언트(Gradient)를 설명(중요도)으로 사용하는 기법입니다.

  - Interpretation of f at x<sub>0</sub> (for the i-th input/feature/pixel)

  $$
  R_i=(\nabla{f(x)}|_{x_0})_i
  $$

  - 함수 값이 각 입력에 얼마나 민감한지를 보여줍니다.

- **예시** ㅣ 가장 높은 점수를 받은 클래스에 대하여, Gradient Map을 시각화할 수 있습니다.
- **장점** ㅣ Back Propagation을 통해 쉽게 계산할 수 있습니다.
- **단점** ㅣ Gradient Shattering 문제로 인해 노이즈가 생길 수 있습니다.


### **SmoothGrad**
- 노이즈가 섞인 Gradient 문제를 해결하기 위한 방법으로 사용될 수 있습니다.
  - 입력에 노이즈를 추가하고 평균 값을 계산합니다.

  $$
  \nabla_{\text{smooth}}f(x)=\mathbb{E}_{\epsilon\sim\mathcal{N}(0,\sigma^{2}I)}[\nabla{f(x+\epsilon)}]
  $$

  - 약간 변형시킨 입력 값에 대한 Gradient를 평균 계산하면 그 해석이 부드러워집니다.
  - 일반적인 휴리스틱(Heuristic) 방법으로는..
    - 기댓값(Expectation)은 몬테카를로(Monte-Carlo) 방식으로 근사하기 (약 50회 반복)
    - $\sigma$는 $X_{max} - X_{min}$의 10~20%로 설정하기

- **장점**
  - 단순 평균 계산을 통해 명확한 해석 제공합니다.
  - 민감한 여러 함수에 쉽게 적용할 수 있습니다.
- **약점** ㅣ 계산 비용이 큽니다.



### **CAM <sup>Class Activation Map</sup>**

- CAM을 입력 이미지 크기에 맞게 업샘플링(Upsampling)하는 방법입니다.
  - Softmax 층 이전에 Global Average Pooling(GAP)을 구현합니다.
- CAM의 대안적 관점
  - 클래스 c의 Logit 값은 (GAP-FC 모델로) 다음과 같이 표현할 수 있습니다.
  - CAM은 이미지 내에서 객체를 지역화 할 수 있습니다.
  $$
  Y^c=\sum_k{w_k^c}\frac{1}{Z}\sum_{ij}{A^k_{ij}}
  $$

- **장점** ㅣ 모델이 주목하고 있는 객체를 명확히 보여줍니다.
- **단점**
  - 제한된 아키텍처의 모델에만 적용 가능하므로, 모델 제한적인 단점을 가집니다.
  - 마지막 합성곱 층에서만 적용 가능하므로 해석 해상도가 낮습니다.



### **Grad-CAM <sup>Gradient-weighted Class Activation Mapping</sup>**

- 채널별 가중 합계를 계산하기 위하여, Grad-CAM은 가중치 값을 평균 연산으로 Pooling된 Gradient 값으로 대체합니다.
- Grad-CAM으로 모델의 학습 과정을 디버깅 할 수 있습니다.
- **장점** ㅣ 다양한 출력 모델에 적용 가능하므로, 모델에 비의존적입니다.
- **단점** ㅣ 평균 Gradient 값이 항상 정확하지 않을 수 있습니다.



### **LIME <sup>Local Interpretable Model-agnostic Explanations</sup>**
- 어떤 분류기라도 지역적으로 해석 가능한 모델로 근사하여 예측을 설명하는 기법입니다.
- 모델에 비의존적이며, 해석에 대한 개괄적 개요를 제공합니다.
- 슈퍼픽셀을 변형하고 주어진 예제에 대한 지역적 해석 모델 획득할 수 있습니다.
  - ex. Google의 Inception 신경망으로 만든 이미지 분류 예측 설명

- **장점** ㅣ 블랙박스인 딥러닝 모델에 대한 해석이 가능합니다.
- **단점**
  - 계산 비용이 높으며, 특정 유형의 모델에 적용하기 어렵습니다.
  - 모델 연산 후에도 지역적으로 여전히 비선형적일 경우 해석이 어려울 수 있습니다.



### **RISE <sup>Randomized Input Sampling for Explanation</sup>**

- 입력 이미지를 랜덤 마스크 형태로 서브 샘플링(Sub-sampling)하는 방법입니다.
- 각 마스크 처리된 이미지에 대한 모델의 예측을 기록합니다.
- LIME의 Saliency Map은 슈퍼픽셀에 의존하며, 이는 정확한 영역을 포착하지 못할 수 있습니다.

- **장점** ㅣ 훨씬 더 명확한 Saliency Map을 제공합니다.
- **단점**
  - 계산 복잡도가 높으며, 샘플링으로 인한 노이즈가 발생할 수 있습니다.
  - 샘플링 근사(Monte Carlo)로 인해 중요도 맵이 노이즈를 포함할 수 있습니다.
    - 특히 다양한 크기의 객체가 있을 경우 더 큰 영향을 받을 수 있음

&nbsp;
&nbsp;
&nbsp;

## **영향 함수 <sup>Influence Function</sup> 를 통해 블랙박스 예측 이해하기**

- 특정 예측에 대해 가장 영향을 미치는 학습 데이터 포인트를 식별하는 또 다른 XAI 기법입니다.
- **영향 함수 (Influence Function)**
  - 특정 학습 샘플을 제거했을 때 테스트 손실값에 미치는 영향을 측정합니다.
- 영향 함수를 활용한 설명 방식은 모델 간의 차이를 시각적으로 보여줄 수 있습니다.

&nbsp;
&nbsp;
&nbsp;

## **Metrics**
- XAI 기법들의 평가 방법론을 소개합니다.

### **Human-based Visual Assessment**
- **AMT (Amazon Mechanical Turk) 테스트**
  - 사람이 모델 예측을 해석하는 것으로 예측할 수 있는지 평가하는 방법이며, 인간 평가 데이터를 얻는 데 비용이 많이 든다는 단점을 가집니다.

### **Human Annotation**
- 사람의 주석 데이터(예: 객체 위치 정보와 의미적 분할)를 **정답 데이터(Ground Truth)**로 사용하여 해석과 비교합니다.
  - ex. Pointing game, Weakly supervised semantic segmentation
- **Pointing Game**

  - 인간의 주석(Annotation)인 경계 상자(Bounding Box) ${B^i}_{i=1,...,N}$와 그 해석$h^i_{I=1,...,N}$에 대하여, Pointing Game의 평균 정확도는 다음과 같이 정의됩니다.
  $$
  Acc=\frac{1}{N}\sum^N_{i=1}1_{[p^{(i)}\in{B^{(i)}}]},
  $$

  - where $p_i$ is a pixel s.t. $p_i=argmax_p(h_p^i)$
  - $1_{[p^i∈B^i]}$는 해석 점수가 가장 높은 픽셀 $p^i$가 경계 상자 $B^i$ 내에 위치하는 경우 값을 1로 가지는 지표 함수입니다.

- **Weakly Supervised Semantic Segmentation**

  - 학습 시 픽셀 레벨 레이블이 제공되지 않도록 설정합니다.
  - 이 지표는 해석과 의미적 분할(Semantic Segmentation) 정답 라벨 간의 평균 IoU(Intersection over Union)를 측정합니다.

- **단점**
  - 사람이 주석을 생성하는 작업은 노동 집약적입니다.
  - 이러한 경계 상자나 분할 라벨에 대한 해석은 절대적 정답이 아닙니다.



### **Pixel Perturbation**
- 이미지에서 중요한 영역을 제거하면 해당 클래스의 Logit 값이 감소할 것이라 가정합니다.
- **AOPC (Area over the MoRF Perturbation Curve)**
  - 가장 관련 있는 영역을 먼저 제거하는 순서(MoRF)로 입력 패치를 교체했을 때, Logit 감소량을 측정합니다.
  $$
  AOPC=\frac{1}{L+1}\mathbb{E}_{x\sim{p(x)}}[\sum^L_{k=0}f(x^{(0)}_{MoRF}-x^{(k)}_{MoRF})],
  $$
  - where $f$ is the logit for true label

- **Insertion and Deletion**
  - **Insertion** ㅣ 이미지를 회색으로 처음 설정하고 중요한 픽셀들을 복구하면서 클래스 확률 곡선의 면적을 측정합니다.
  - **Deletion** ㅣ 중요한 픽셀을 제거하며 클래스 확률 곡선의 면적을 측정합니다.
- **단점**
  - 머신러닝의 기본 가정(학습 및 평가 데이터는 동일 분포를 가짐)을 위반하는 기법입니다.
  - 변형된 입력 데이터는 테스트 시 배포되고 해석되는 모델과 다를 수 있습니다.
  - 변형으로 인해 모델이 입력을 다른 클래스(예: Balloon)로 예측할 가능성이 존재합니다.

  
### **ROAR <sup>RemOve And Retrain</sup>**
- 해석 결과에서 중요한 픽셀 순서대로 학습 데이터를 일부 제거하고 새 모델을 재학습하는 방법입니다.
- **단점** ㅣ 매번 재학습하게 되므로, 매우 높은 계산 비용을 소모합니다.

&nbsp;
&nbsp;
&nbsp;

## **Sanity Checks**

### **모델 랜덤화 (Model Randomization)**

- **Interpretation = Edge Detector?**
  - 일부 해석 방법은 에지 검출기(Edge Detector)로 생성된 Saliency Map과 크게 유사한 결과를 보여줍니다.
- **모델 랜덤화 테스트**
  - 이 실험은 파라미터를 계단식(Cascading) 또는 개별 레이어 방식으로 무작위 재초기화 합니다.
  - 일부 해석 방법은 이러한 랜덤화에 민감하지 않습니다. (ex. Guided-backprop, LRP, Pattern Attribution).


### **Adversarial Attack**
- **기하학적 문제가 원인?**
  - 해석에 대한 적대적 공격(Adversarial Attack)을 다음과 같이 제안할 수 있습니다. 
    $$
    \mathbb{L}=||h(x_{adv})-h^t||^2+\gamma||g(x_{adv})-g(x)||^2
    $$
  - 해석 공격을 해제(Undo)하는 스무딩(Smoothing) 방법을 제안할 수 있습니다.
    - Softplus 활성화 함수를 높은 $\beta$값으로 사용하는 방법을 통해 공격을 해제할 수 있습니다.
  - 이러한 공격들에 대한 이론적 경계(Bound)를 제공합니다.


### **Adversarial Model Manipulation**
- 두 모델이 유사한 정확도를 보이지만, 전혀 다른 해석을 생성할 수 있습니다.
- **입력에 대한 공격?**
  - 모델 정확도 하락은 미미할 수 있으며, 속이는(Fooling) 방식은 검증 데이터 전체에 일반화됩니다.
  - 다른 해석 방법에도 속임 효과가 전이(Transfer)될 수 있습니다.
  - AOPC 분석이 진정한 속임(Fooling)을 확인시켜줍니다.


&nbsp;
&nbsp;
&nbsp;

-------------
## Reference
> 본 포스팅은 LG Aimers 프로그램에서 학습한 내용을 기반으로 작성되었습니다. (전체 내용 X)
{: .prompt-warning }

1. LG Aimers AI Essential Course Module 4. 설명가능한 AI(Explainable AI), 서울대학교 문태섭 교수






